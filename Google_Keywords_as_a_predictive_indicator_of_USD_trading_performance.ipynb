{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv8O4ZnYLlInuxB15cnhMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackbudge98-cpu/gt-markets/blob/main/Google_Keywords_as_a_predictive_indicator_of_USD_trading_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Keywords as a predictive indicator of USD trading performance**"
      ],
      "metadata": {
        "id": "vO9JOC3kodYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Project is exploring how Google Trend KeyWord Data can be used in forward validation to determine the probability of a movement in a trading pair."
      ],
      "metadata": {
        "id": "X-FBJBFdn5Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Control Variable will be USD to determine's its performance on the following trading pairs:\n",
        "\n",
        "*   USD to Chinese Yuan\n",
        "*   USD to BTC\n",
        "*   USD to Oil\n",
        "*   USD to Gold\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nCz8ZH7qoKyT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khfIbZv6n1Vh",
        "outputId": "52a0e391-752e-4031-b1f8-f4fecf372a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-924416535.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(tickers, period=\"10y\", interval=\"1d\")[\"Close\"]\n",
            "[*********************100%***********************]  5 of 5 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ticker      USD to BTC  USD to Oil  USD to Gold       USD  USD to Chinese Yuan\n",
            "Date                                                                          \n",
            "2015-08-29  229.779999         NaN          NaN       NaN                  NaN\n",
            "2015-08-30  228.761002         NaN          NaN       NaN                  NaN\n",
            "2015-08-31  230.056000   49.200001  1131.599976  1.390876               6.3785\n",
            "2015-09-01  228.121002   45.410000  1138.699951  1.281003               6.3664\n",
            "2015-09-02  229.283997   46.250000  1132.500000  1.341353               6.3545\n",
            "2015-09-03  227.182999   46.750000  1123.699951  1.383659               6.3459\n",
            "2015-09-04  230.298004   46.049999  1120.599976  1.345162               6.3459\n",
            "2015-09-05  235.018997         NaN          NaN       NaN                  NaN\n",
            "2015-09-06  239.839996         NaN          NaN       NaN                  NaN\n",
            "2015-09-07  239.847000         NaN          NaN       NaN               6.3459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Use of the following libaries will assist in providing the project manager with the data\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "\n",
        "#The first data set we will want to see is USD over a 10 year period\n",
        "\n",
        "tickers = [\"USD\", \"USDCNY=X\",\"BTC-USD\",\"CL=F\", \"GC=F\"]\n",
        "\n",
        "#The definition of tickers will assist in a batch query rather than a singular batch query\n",
        "\n",
        "df = yf.download(tickers, period=\"10y\", interval=\"1d\")[\"Close\"]\n",
        "\n",
        "#Rename the columns to be more user friendly, and align with our assumptions\n",
        "\n",
        "df.rename(columns={\"CL=F\":\"USD to Oil\",\"GC=F\":\"USD to Gold\",\"BTC-USD\": \"USD to BTC\",\"USDCNY=X\": \"USD to Chinese Yuan\"},inplace=True)\n",
        "\n",
        "#print headers for 10 rows to see what the data looks like\n",
        "\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b06eac4f",
        "outputId": "03abc9c2-f4e8-4428-8c6f-07fb61566eba"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import os\n",
        "from datetime import date\n",
        "\n",
        "# Authenticate and mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Define the folder ID and filename\n",
        "folder_id = '1tqNeIkQM2IawFLS-NHBzaDl8fsJAo0t_'\n",
        "today = date.today()\n",
        "filename = f\"financial_data_raw_data_from_yf{today.strftime('%Y-%m-%d')}.csv\"\n",
        "filepath = f\"/content/{filename}\" # Save locally first\n",
        "\n",
        "# Save the DataFrame to a temporary local CSV file\n",
        "df.to_csv(filepath)\n",
        "\n",
        "# Create a file in the shared drive\n",
        "file_metadata = {\n",
        "    'name': filename,\n",
        "    'parents': [folder_id]\n",
        "}\n",
        "\n",
        "media = MediaFileUpload(filepath, mimetype='text/csv')\n",
        "\n",
        "gfile = drive_service.files().create(\n",
        "    body=file_metadata,\n",
        "    media_body=media,\n",
        "    fields='id'\n",
        ").execute()\n",
        "\n",
        "print(f\"File ID: {gfile.get('id')}\")\n",
        "print(f\"Data exported to shared drive folder: {folder_id}\")\n",
        "\n",
        "# Clean up the local file\n",
        "os.remove(filepath)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File ID: 1_-jK0nuGdsQOsZ03lKsePRL_d24gic21\n",
            "Data exported to shared drive folder: 1tqNeIkQM2IawFLS-NHBzaDl8fsJAo0t_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Next step is to obtain information on the datatable, and apply pre-processing steps\n",
        "\n",
        "df.info()\n",
        "\n",
        "#understand how many numbers of rows are in the dataset\n",
        "print('\\n')\n",
        "num_rows = len(df)\n",
        "print(f\"Number of rows: {num_rows}\")\n",
        "\n",
        "#the next step is to export the dataset as a csv file to enable a view of the data\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "today = date.today()\n",
        "filename = f\"financial_data_raw_data_from_yf{today.strftime('%Y-%m-%d')}.csv\"\n",
        "directory = \"/content/drive/Shared drives/1tqNeIkQM2IawFLS-NHBzaDl8fsJAo0t_\"\n",
        "filepath = f\"{directory}/{filename}\" # Construct the full path\n",
        "\n",
        "# Create the directory if it doesn't exist - Note: This may not work for shared drives. Ensure the folder exists manually.\n",
        "# os.makedirs(directory, exist_ok=True) # Commenting out as it might not work for shared drives.\n",
        "\n",
        "df.to_csv(filepath)\n",
        "\n",
        "print (f\"Data exported to {filepath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "L0oKAs8Oqthm",
        "outputId": "2e3f1b3b-0126-4068-a30d-f4f3f4762022"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 3654 entries, 2015-08-29 to 2025-08-29\n",
            "Freq: D\n",
            "Data columns (total 5 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   USD to BTC           3654 non-null   float64\n",
            " 1   USD to Oil           2515 non-null   float64\n",
            " 2   USD to Gold          2514 non-null   float64\n",
            " 3   USD                  2515 non-null   float64\n",
            " 4   USD to Chinese Yuan  2603 non-null   float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 171.3 KB\n",
            "\n",
            "\n",
            "Number of rows: 3654\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '/content/drive/Shared drives/1tqNeIkQM2IawFLS-NHBzaDl8fsJAo0t_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-566761475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# os.makedirs(directory, exist_ok=True) # Commenting out as it might not work for shared drives.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data exported to {filepath}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/content/drive/Shared drives/1tqNeIkQM2IawFLS-NHBzaDl8fsJAo0t_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Blank values in the raw database\")\n",
        "print('\\n')\n",
        "\n",
        "#Identify the number of rows are blank\n",
        "print(df.isna().sum())\n",
        "\n",
        "print('\\n')\n",
        "blank_rate = (df.isna().sum() / num_rows) * 100\n",
        "print(\"Blank Rate (%):\")\n",
        "print(blank_rate.round(2))"
      ],
      "metadata": {
        "id": "TfDNpH8CUyBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After Identifying the blank rate in the original dataframe pre-processing needs to be applied\n",
        "df_for_pre_processing = df.copy()\n",
        "df_for_pre_processing['Day of Week'] = df_for_pre_processing.index.day_name()\n",
        "print (df_for_pre_processing.head(10))\n",
        "\n",
        "#We know Bitcoin trades all the time, but we want to see if there is any other blanks in the dataset\n",
        "print('\\n')\n",
        "print(\"Blank values in the pre-processed database\")\n",
        "print('\\n')\n",
        "missing_values_per_day_of_week = df_for_pre_processing.groupby('Day of Week').apply(lambda g: g.isna().sum().sum())\n",
        "print(missing_values_per_day_of_week)"
      ],
      "metadata": {
        "id": "XVDl4dl7U5FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Therefore in the pre-processing dataset the decision will be made to drop both Saturday and Sunday\n",
        "df_weekday = df_for_pre_processing.drop(df_for_pre_processing[(df_for_pre_processing['Day of Week'] == 'Saturday') | (df_for_pre_processing['Day of Week'] == 'Sunday')].index)\n",
        "df_weekday_reordered = df_weekday[['Day of Week', 'USD', 'USD to Chinese Yuan', 'USD to BTC', 'USD to Oil', 'USD to Gold']]\n",
        "df_weekday_reordered.head(10)"
      ],
      "metadata": {
        "id": "QhycQ2y1WoRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Where data is missing the assumption will be to replace in the df_weekday dataframe with the previous value in the dataset\n",
        "df_weekday_usd = df_weekday.drop(columns=['USD to BTC','USD to Oil','USD to Gold','USD to Chinese Yuan'])\n",
        "df_weekday_usd_reordered = df_weekday_usd[['Day of Week', 'USD']]\n",
        "df_weekday_usd_reordered.head(10)"
      ],
      "metadata": {
        "id": "8dEWzeiIfAw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next step is for any NaN is to show the dates\n",
        "df_weekday_usd_reordered.isna().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "QCabwnrD1pa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06b01074"
      },
      "source": [
        "# Show dates where 'USD' is NaN\n",
        "dates_with_missing_usd = df_weekday_usd_reordered[df_weekday_usd_reordered['USD'].isna()].index\n",
        "print(\"Dates with missing 'USD' values:\")\n",
        "print(dates_with_missing_usd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are holidays in the United States which are not a set date but rather a day near of. For simplicity of the dataset it will be easier to use the previous close value."
      ],
      "metadata": {
        "id": "xYWN00xp6BT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Where there is a NaaN going to use the previous day close to populate the value\n",
        "df_weekday_usd_reordered['USD'] = df_weekday_usd_reordered['USD'].ffill()\n",
        "df_weekday_usd_reordered.head(10)"
      ],
      "metadata": {
        "id": "1e0kDv171_ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we add a daily change amount, and a percentage daily change to the dataset\n",
        "df_weekday_usd_reordered['Daily Change'] = df_weekday_usd_reordered['USD'].diff()\n",
        "df_weekday_usd_reordered['% Daily Change'] = df_weekday_usd_reordered['Daily Change'] / df_weekday_usd_reordered['USD']\n",
        "\n",
        "# Fill the initial NaN values with 0\n",
        "df_weekday_usd_reordered['Daily Change'].fillna(0, inplace=True)\n",
        "df_weekday_usd_reordered['% Daily Change'].fillna(0, inplace=True)\n",
        "\n",
        "df_weekday_usd_reordered.head(10)"
      ],
      "metadata": {
        "id": "OLCzGqE43JVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Export the DF Weekday USD Performance as a CSV File with the First Date - Last Date as the file name\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import os\n",
        "from datetime import date\n",
        "\n",
        "# Authenticate and mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Define the folder ID and filename\n",
        "folder_id = '1wOqTixtRA5n5uHN1ptfIbQm6suTrLDxA'\n",
        "today = date.today()\n",
        "filename = f\"financial_data_pre_processed_data_from_yf_{today.strftime('%Y-%m-%d')}.csv\" # Changed filename slightly to distinguish\n",
        "filepath = f\"/content/{filename}\" # Save locally first\n",
        "\n",
        "# Save the DataFrame to a temporary local CSV file\n",
        "df_weekday_usd_reordered.to_csv(filepath)\n",
        "\n",
        "# Create a file in the shared drive\n",
        "file_metadata = {\n",
        "    'name': filename,\n",
        "    'parents': [folder_id]\n",
        "}\n",
        "\n",
        "media = MediaFileUpload(filepath, mimetype='text/csv')\n",
        "\n",
        "gfile = drive_service.files().create(\n",
        "    body=file_metadata,\n",
        "    media_body=media,\n",
        "    fields='id'\n",
        ").execute()\n",
        "\n",
        "print(f\"File ID: {gfile.get('id')}\")\n",
        "print(f\"Data exported to shared drive folder: {folder_id}\")\n",
        "\n",
        "# Clean up the local file\n",
        "os.remove(filepath)"
      ],
      "metadata": {
        "id": "Zap8nwFZ3ovg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de12aab5-b807-4d36-c0c6-de626e0fd53a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File ID: 1aEAPWO1KSiQvzmt6rB_beotoO2JJtiVH\n",
            "Data exported to shared drive folder: 1wOqTixtRA5n5uHN1ptfIbQm6suTrLDxA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLe67T2R56bQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}