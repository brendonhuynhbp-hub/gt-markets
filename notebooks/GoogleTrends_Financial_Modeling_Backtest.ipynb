{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyPW4UX6NCIFimeYVBcpdgZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/main/notebooks/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "af2404b2-405d-4f05-bc8e-4231e2264211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "PROJECT_DIR: /content/drive/MyDrive/gt-markets\n",
            "DATA_DIR:    /content/drive/MyDrive/gt-markets/data/processed\n",
            "KW_DIR:      /content/drive/MyDrive/gt-markets/data/Keyword Selection\n",
            "OUT_DIR:     /content/drive/MyDrive/gt-markets/outputs\n",
            "TF on CPU\n",
            "TF version: 2.19.0\n",
            "Trend columns with missing values (auto-filled to 0): ['entrepreneurial_trend', 'cryptocurrency_trend'] \n",
            "DEBUG: using last 1000 rows only.\n",
            "Output run folder: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527\n",
            "        data: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/00_data\n",
            "        logs: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/10_logs\n",
            "   preds_val: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/20_preds/val\n",
            "  preds_test: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/20_preds/test\n",
            "   backtests: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/30_backtests\n",
            "        figs: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs\n",
            " leaderboard: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/50_leaderboards\n",
            "Setup complete → base frame: 2021-11-08 → 2025-09-05 | rows: 1000\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# SETUP\n",
        "# - Mount Drive, resolve project paths, seed everything\n",
        "# - Load merged dataset & keyword list (correct file)\n",
        "# - Auto-detect/fix missing trend columns (fill with 0)\n",
        "# - Optional weekly aggregation\n",
        "# - Utilities (target, keyword mapping, lags, logging)\n",
        "# - Structured output folders for this run\n",
        "# =========================================================\n",
        "\n",
        "# ---- 0) Global switches ---------------------------------\n",
        "DEBUG = False        # True = faster dev loop; False = full run\n",
        "FREQ  = \"D\"         # \"D\" daily, \"W\" weekly aggregation\n",
        "\n",
        "# ---- 1) Drive + paths -----------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import os, sys, warnings, random, json, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "CANDIDATE_PROJECT_DIRS = [\n",
        "    Path(\"/content/drive/MyDrive/gt-markets\"),\n",
        "    Path(\"/content/drive/Shareddrives/gt-markets\"),\n",
        "]\n",
        "PROJECT_DIR = next((p for p in CANDIDATE_PROJECT_DIRS if p.exists()), None)\n",
        "assert PROJECT_DIR is not None, \"Project directory not found in Drive.\"\n",
        "\n",
        "DATA_DIR = PROJECT_DIR / \"data\" / \"processed\"\n",
        "KW_DIR   = PROJECT_DIR / \"data\" / \"Keyword Selection\"   # ← exact folder name (with space)\n",
        "OUT_DIR  = PROJECT_DIR / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Correct files from older code\n",
        "RAW_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"\n",
        "ENG_FILE = DATA_DIR / \"merged_financial_trends_engineered_2025-09-07.csv\"\n",
        "KW_CSV   = KW_DIR  / \"combined_significant_lagged_correlations.csv\"\n",
        "\n",
        "assert RAW_FILE.exists(), f\"Missing dataset: {RAW_FILE}\"\n",
        "assert KW_CSV.exists(),   f\"Missing keyword file: {KW_CSV}\"\n",
        "\n",
        "print(f\"PROJECT_DIR: {PROJECT_DIR}\")\n",
        "print(f\"DATA_DIR:    {DATA_DIR}\")\n",
        "print(f\"KW_DIR:      {KW_DIR}\")\n",
        "print(f\"OUT_DIR:     {OUT_DIR}\")\n",
        "\n",
        "# ---- 2) Reproducibility + warnings ----------------------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---- 3) ML/DL stack ------------------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# Keep TF from grabbing all GPU memory (prevents OOM)\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    print(f\"TF GPU devices: {len(gpus)} (mem growth on)\" if gpus else \"TF on CPU\")\n",
        "except Exception as e:\n",
        "    print(\"TF GPU setup note:\", e)\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# ---- 4) Asset registry ---------------------------------\n",
        "ASSETS = [\n",
        "    {\"PAIR_ID\": \"GC=F\",      \"price_col\": \"GC=F Close\",      \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\",   \"price_col\": \"BTC-USD Close\",   \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",      \"price_col\": \"CL=F Close\",      \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"USDCNY=X\",  \"price_col\": \"USDCNY=X Close\",  \"label\": \"USDCNY\"},\n",
        "]\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in ASSETS}\n",
        "\n",
        "# ---- 5) Load merged table + fix trend NaNs --------------\n",
        "# Start with RAW by default; ENG will be selectable in Runner\n",
        "df0 = pd.read_csv(RAW_FILE, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "\n",
        "# Auto-detect trend columns and fill any missing values with zero.\n",
        "trend_cols_all = [c for c in df0.columns if \"trend\" in c.lower()]\n",
        "bad_trend_cols = [c for c in trend_cols_all if df0[c].isna().any()]\n",
        "if bad_trend_cols:\n",
        "    print(\"Trend columns with missing values (auto-filled to 0):\",\n",
        "          bad_trend_cols[:10], \"...\" if len(bad_trend_cols)>10 else \"\")\n",
        "    df0[bad_trend_cols] = df0[bad_trend_cols].fillna(0.0)\n",
        "\n",
        "# Optional: debug slice AFTER filling so lags won’t reintroduce NaNs\n",
        "if DEBUG:\n",
        "    df0 = df0.tail(1000)\n",
        "    print(\"DEBUG: using last 1000 rows only.\")\n",
        "\n",
        "# ---- 6) Optional weekly aggregation ---------------------\n",
        "def to_frequency(df_in: pd.DataFrame, freq: str = \"D\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Resample to end-of-week for prices (last), and mean for non-price columns.\n",
        "    \"\"\"\n",
        "    if freq.upper() == \"D\":\n",
        "        return df_in\n",
        "    assert freq.upper() == \"W\", \"Supported frequencies: 'D' or 'W'.\"\n",
        "    out = pd.DataFrame(index=df_in.resample(\"W\").last().index)\n",
        "    # prices: last of week\n",
        "    for a in ASSETS:\n",
        "        out[a[\"price_col\"]] = df_in[a[\"price_col\"]].resample(\"W\").last()\n",
        "    # other columns: mean of week\n",
        "    price_cols = {a[\"price_col\"] for a in ASSETS}\n",
        "    other_cols = [c for c in df_in.columns if c not in price_cols]\n",
        "    for c in other_cols:\n",
        "        out[c] = df_in[c].resample(\"W\").mean()\n",
        "    return out\n",
        "\n",
        "df_basefreq = to_frequency(df0, FREQ)\n",
        "\n",
        "# ---- 7) Utilities ---------------------------------------\n",
        "from functools import lru_cache\n",
        "from datetime import datetime\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list:\n",
        "    d = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\",\"Keyword\"}.issubset(d.columns), \"Keyword CSV must have: Pair, Keyword\"\n",
        "    # tolerate USDCNY aliasing\n",
        "    aliases = {pair_id}\n",
        "    if pair_id == \"USDCNY=X\": aliases |= {\"CNY=X\"}\n",
        "    if pair_id == \"CNY=X\":    aliases |= {\"USDCNY=X\"}\n",
        "    kws = (d.loc[d[\"Pair\"].isin(aliases), \"Keyword\"]\n",
        "             .dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "    return kws\n",
        "\n",
        "def map_keywords_to_trend_cols(all_cols: pd.Index, keywords: list) -> list:\n",
        "    # keyword \"gold price\" -> \"gold_price_trend\"\n",
        "    norm = lambda s: str(s).lower().strip().replace(\" \", \"_\")\n",
        "    desired = {f\"{norm(k)}_trend\" for k in keywords}\n",
        "    return [c for c in all_cols if str(c).lower() in desired]\n",
        "\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"ret1\"] = out[price_col].pct_change()\n",
        "    out[\"y_up\"] = (out[price_col].shift(-1) > out[price_col]).astype(int)\n",
        "    return out.dropna(subset=[price_col]).dropna()\n",
        "\n",
        "def build_trend_lag_features(df_in: pd.DataFrame, sel_cols: list, lag_steps=(7, 14, 21)) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build percentage-change features over given lags, then shift(+1) for causality.\n",
        "    Handles cases where previous value is 0 to avoid +/-inf; sanitizes NaN/inf and clips outliers.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for c in sel_cols:\n",
        "        s = df_in[c].astype(float)\n",
        "        for L in lag_steps:\n",
        "            chg = s.pct_change(L).shift(1)\n",
        "            chg = chg.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            # clip absurd pct changes to stabilize scaling (optional but recommended)\n",
        "            chg = chg.clip(lower=-10.0, upper=10.0)\n",
        "            out[f\"{c}__chg{L}\"] = chg\n",
        "    return pd.DataFrame(out, index=df_in.index)\n",
        "\n",
        "def pick_start_index(n: int, frac: float = 0.6) -> int:\n",
        "    return max(50 if DEBUG else 200, int(n * (0.8 if DEBUG else frac)))\n",
        "\n",
        "# ---- 8) Structured output folders for this run ----------\n",
        "RUN_MODE  = \"debug\" if DEBUG else \"prod\"\n",
        "RUN_STAMP = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_ID    = f\"{FREQ.lower()}_{RUN_MODE}_{RUN_STAMP}\"\n",
        "\n",
        "RUN_ROOT = OUT_DIR / \"runs\" / RUN_ID\n",
        "STAGES = {\n",
        "    \"data\":        RUN_ROOT / \"00_data\",\n",
        "    \"logs\":        RUN_ROOT / \"10_logs\",\n",
        "    \"preds_val\":   RUN_ROOT / \"20_preds\" / \"val\",\n",
        "    \"preds_test\":  RUN_ROOT / \"20_preds\" / \"test\",\n",
        "    \"backtests\":   RUN_ROOT / \"30_backtests\",\n",
        "    \"figs\":        RUN_ROOT / \"40_figs\",\n",
        "    \"leaderboard\": RUN_ROOT / \"50_leaderboards\",\n",
        "}\n",
        "for p in STAGES.values():\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Snapshot (for report reproducibility)\n",
        "(df0.head(1)\n",
        " .assign(_start=df_basefreq.index.min(), _end=df_basefreq.index.max())\n",
        " .to_csv(STAGES[\"data\"] / f\"dataset_snapshot_{RAW_FILE.name}.head1.csv\"))\n",
        "\n",
        "def _slug(s: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"_\", s.lower()).strip(\"_\")\n",
        "\n",
        "def _pred_name(asset_label: str, dataset: str, model: str, *, window: int | None = None, split=\"test\") -> str:\n",
        "    bits = [_slug(asset_label), dataset, \"dl\", model.lower()]\n",
        "    if window: bits.append(f\"w{window}\")\n",
        "    bits.append(\"extended\")\n",
        "    return \".\".join([\"_\".join(bits), split, \"csv\"])\n",
        "\n",
        "def _log_name(asset_label: str, dataset: str, run_name: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{dataset}_{_slug(run_name)}.{RUN_STAMP}.txt\"\n",
        "\n",
        "def _fig_name(asset_label: str, tail: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{tail}.png\"\n",
        "\n",
        "def save_txt_log(asset_label: str, dataset: str, run_name: str, text_lines: list[str]) -> Path:\n",
        "    path = STAGES[\"logs\"] / _log_name(asset_label, dataset, run_name)\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(text_lines))\n",
        "    return path\n",
        "\n",
        "def save_preds_df(df_pred: pd.DataFrame, asset_label: str, dataset: str, model: str, *, window: int | None, split: str):\n",
        "    stage = STAGES[\"preds_val\"] if split == \"val\" else STAGES[\"preds_test\"]\n",
        "    outp = stage / _pred_name(asset_label, dataset, model, window=window, split=split)\n",
        "    df_pred.to_csv(outp)\n",
        "    return outp\n",
        "\n",
        "def save_leaderboard(df_leader: pd.DataFrame, tag: str = \"metrics\") -> Path:\n",
        "    path = STAGES[\"leaderboard\"] / f\"leaderboard_{tag}.csv\"\n",
        "    df_leader.to_csv(path, index=False)\n",
        "    return path\n",
        "\n",
        "def save_backtest_table(df_bt: pd.DataFrame, asset_label: str, model_tag: str) -> Path:\n",
        "    path = STAGES[\"backtests\"] / f\"{_slug(asset_label)}_{_slug(model_tag)}_backtest.csv\"\n",
        "    df_bt.to_csv(path)\n",
        "    return path\n",
        "\n",
        "def save_figure(fig, asset_label: str, tail: str) -> Path:\n",
        "    path = STAGES[\"figs\"] / _fig_name(asset_label, tail)\n",
        "    fig.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "    return path\n",
        "\n",
        "print(f\"Output run folder: {RUN_ROOT}\")\n",
        "for name, path in STAGES.items():\n",
        "    print(f\"{name:>12}: {path}\")\n",
        "print(\"Setup complete → base frame:\", df_basefreq.index.min().date(), \"→\", df_basefreq.index.max().date(), \"| rows:\", len(df_basefreq))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# RUNNER\n",
        "# - Build extended features (no blanket dropna)\n",
        "# - Purged/embargoed splits\n",
        "# - LSTM (primary) + MLP (ablation)\n",
        "# - Save VAL + TEST predictions to structured folders\n",
        "# - TXT logs to /10_logs\n",
        "# =========================================================\n",
        "\n",
        "# ---- 1) Split helpers -----------------------------------\n",
        "def make_purged_splits(n: int, train=0.70, valid=0.15, embargo: int = 5):\n",
        "    \"\"\"\n",
        "    Purged + embargoed indices to reduce leakage:\n",
        "      train: [0 : i_tr)\n",
        "      gap1:  [i_tr : i_tr+embargo)\n",
        "      val:   [i_tr+embargo : i_va)\n",
        "      gap2:  [i_va : i_va+embargo)\n",
        "      test:  [i_va+embargo : n)\n",
        "    \"\"\"\n",
        "    i_tr = int(n * train)\n",
        "    i_va = int(n * (train + valid))\n",
        "    tr = slice(0, i_tr)\n",
        "    va = slice(min(i_tr + embargo, i_va), i_va)\n",
        "    te = slice(min(i_va + embargo, n), n)\n",
        "    return tr, va, te\n",
        "\n",
        "# ---- 2) DL builders -------------------------------------\n",
        "def make_mlp(input_dim: int):\n",
        "    tf.keras.utils.set_random_seed(SEED)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(input_dim,)),\n",
        "        keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(64, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[keras.metrics.AUC(name=\"auc\"), keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def make_lstm(input_shape, units=64, dropout=0.2):\n",
        "    tf.keras.utils.set_random_seed(SEED)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),          # (window, n_features)\n",
        "        keras.layers.LSTM(units, return_sequences=True),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.LSTM(units//2),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[keras.metrics.AUC(name=\"auc\"), keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_sequences_from_extended(df_ext: pd.DataFrame, feature_cols: list, y_col=\"y_up\", window=30):\n",
        "    X = df_ext[feature_cols].values\n",
        "    y = df_ext[y_col].astype(int).values\n",
        "    idx = df_ext.index\n",
        "    xs, ys, dates = [], [], []\n",
        "    for t in range(window, len(df_ext)):\n",
        "        xs.append(X[t-window:t, :]); ys.append(y[t]); dates.append(idx[t])\n",
        "    return np.asarray(xs), np.asarray(ys), pd.DatetimeIndex(dates)\n",
        "\n",
        "def split_scale_sequences(X_seq, y_seq, train=0.70, valid=0.15):\n",
        "    # time split first (70/15/15)\n",
        "    n = len(X_seq); i_tr = int(n*train); i_va = int(n*(train+valid))\n",
        "    X_tr, X_va, X_te = X_seq[:i_tr], X_seq[i_tr:i_va], X_seq[i_va:]\n",
        "    y_tr, y_va, y_te = y_seq[:i_tr], y_seq[i_tr:i_va], y_seq[i_va:]\n",
        "    # fit scaler on train only, apply to all splits\n",
        "    if len(X_tr)==0: return (X_tr,y_tr),(X_va,y_va),(X_te,y_te)\n",
        "    T,W,F = X_tr.shape\n",
        "    scaler = StandardScaler().fit(X_tr.reshape(T*W, F))\n",
        "    def _tf(x):\n",
        "        if len(x)==0: return x\n",
        "        TT,WW,FF = x.shape\n",
        "        return scaler.transform(x.reshape(TT*WW,FF)).reshape(TT,WW,FF)\n",
        "    return (_tf(X_tr),y_tr),(_tf(X_va),y_va),(_tf(X_te),y_te)\n",
        "\n",
        "# ---- 3) Main per-asset runner ---------------------------\n",
        "def run_asset(asset: dict, dataset_version: str = \"raw\", use_dl: bool = True, use_dl_mode: str = \"lstm\"):\n",
        "    \"\"\"\n",
        "    - dataset_version: \"raw\" or \"eng\" (tag only; switch df source here if needed)\n",
        "    - use_dl_mode: \"mlp\" | \"lstm\" | \"both\"\n",
        "    \"\"\"\n",
        "    label      = asset[\"label\"]\n",
        "    price_col  = asset[\"price_col\"]\n",
        "    label_tag  = f\"{label} [{dataset_version}]\"\n",
        "\n",
        "    # If wanting to run engineered file too, swap df_basefreq source here:\n",
        "    base_source = df_basefreq  # replace with engineered resample if using ENG_FILE externally\n",
        "\n",
        "    # Base target frame (on chosen frequency)\n",
        "    base = make_target(base_source[[price_col]], price_col).dropna(subset=[price_col, \"y_up\"])\n",
        "\n",
        "    # Keyword selection for this pair\n",
        "    kws = load_keywords_for_pair(KW_CSV, asset[\"PAIR_ID\"])\n",
        "    trend_cols = [c for c in map_keywords_to_trend_cols(base_source.columns, kws) if c in base_source.columns]\n",
        "\n",
        "    # Lag features\n",
        "    lag_df = build_trend_lag_features(base_source, trend_cols, lag_steps=(7,14,21))\n",
        "\n",
        "    # Extended frame: join without blanket dropna; fill safe defaults for trend/lag features\n",
        "    ext = base.join(base_source[trend_cols], how=\"left\").join(lag_df, how=\"left\")\n",
        "    if trend_cols: ext[trend_cols] = ext[trend_cols].fillna(0.0)\n",
        "    lag_cols = [c for c in ext.columns if \"__chg\" in c]\n",
        "    if lag_cols:  ext[lag_cols]  = ext[lag_cols].fillna(0.0)\n",
        "\n",
        "    # Final sanitization: no infinities; clip extreme outliers for stability\n",
        "    num_cols = [c for c in ext.columns if c not in {price_col, \"ret1\", \"y_up\"}]\n",
        "    if num_cols:\n",
        "        ext[num_cols] = ext[num_cols].replace([np.inf, -np.inf], 0.0)\n",
        "        ext[num_cols] = ext[num_cols].clip(lower=-10.0, upper=10.0)\n",
        "\n",
        "    ext = ext.dropna(subset=[price_col, \"y_up\"]).copy()\n",
        "\n",
        "    # Feature set for DL\n",
        "    exclude = {price_col, \"ret1\", \"y_up\"}\n",
        "    extended_cols = [c for c in ext.columns if c not in exclude]\n",
        "    rows_used = len(ext)\n",
        "\n",
        "    results = {}\n",
        "    if use_dl:\n",
        "        modes = [use_dl_mode] if use_dl_mode in {\"mlp\",\"lstm\"} else [\"mlp\",\"lstm\"]\n",
        "\n",
        "        for mode in modes:\n",
        "            if mode == \"mlp\":\n",
        "                # ------- MLP on tabular extended features -------\n",
        "                X = ext[extended_cols].values\n",
        "                y = ext[\"y_up\"].astype(int).values\n",
        "\n",
        "                tr, va, te = make_purged_splits(len(ext), train=0.70, valid=0.15, embargo=5)\n",
        "                X_tr, X_va, X_te = X[tr], X[va], X[te]\n",
        "                y_tr, y_va, y_te = y[tr], y[va], y[te]\n",
        "\n",
        "                scaler = StandardScaler().fit(X_tr)\n",
        "                X_tr, X_va, X_te = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "                MAX_EPOCHS = 3 if DEBUG else 30\n",
        "                BATCH      = 32 if DEBUG else 64\n",
        "                es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True)\n",
        "\n",
        "                mlp = make_mlp(X_tr.shape[1])\n",
        "                mlp.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=MAX_EPOCHS, batch_size=BATCH, callbacks=[es], verbose=0)\n",
        "\n",
        "                p_va = mlp.predict(X_va, verbose=0).ravel()\n",
        "                h_va = (p_va >= 0.5).astype(int)\n",
        "                p_te = mlp.predict(X_te, verbose=0).ravel()\n",
        "                h_te = (p_te >= 0.5).astype(int)\n",
        "\n",
        "                m = {\n",
        "                    \"acc\": accuracy_score(y_te, h_te) if len(y_te) else float(\"nan\"),\n",
        "                    \"f1\":  f1_score(y_te, h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                    \"auc\": roc_auc_score(y_te, p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                }\n",
        "                results[\"MLP\"] = (None, m)\n",
        "\n",
        "                # Save VAL + TEST predictions\n",
        "                df_val  = pd.DataFrame({\"date\": ext.index[va], \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\")\n",
        "                df_test = pd.DataFrame({\"date\": ext.index[te], \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\")\n",
        "                save_preds_df(df_val,  label, dataset_version, \"mlp\", window=None, split=\"val\")\n",
        "                save_preds_df(df_test, label, dataset_version, \"mlp\", window=None, split=\"test\")\n",
        "\n",
        "            elif mode == \"lstm\":\n",
        "                # ------- LSTM on sequences of extended features -------\n",
        "                WINDOW     = 30\n",
        "                MAX_EPOCHS = 3 if DEBUG else 50\n",
        "                BATCH      = 32 if DEBUG else 64\n",
        "\n",
        "                Xseq, yseq, idx = build_sequences_from_extended(ext, extended_cols, \"y_up\", window=WINDOW)\n",
        "                nseq = len(Xseq)\n",
        "                if nseq < 50:\n",
        "                    print(f\"[WARN] Not enough samples for {label_tag} (LSTM).\")\n",
        "                else:\n",
        "                    # Purged indices for sequences\n",
        "                    tr, va, te = make_purged_splits(nseq, train=0.70, valid=0.15, embargo=5)\n",
        "                    (X_tr_raw,y_tr_raw), (X_va_raw,y_va_raw), (X_te_raw,y_te_raw) = split_scale_sequences(Xseq, yseq, train=0.70, valid=0.15)\n",
        "\n",
        "                    # Re-slice after scaling to align with purged indices\n",
        "                    i_tr_end = int(nseq*0.70)\n",
        "                    i_va_end = int(nseq*0.85)\n",
        "                    X_tr, y_tr = X_tr_raw[tr], y_tr_raw[tr]\n",
        "                    X_va, y_va = X_va_raw[va.start - i_tr_end: va.stop - i_tr_end], y_va_raw[va.start - i_tr_end: va.stop - i_tr_end]\n",
        "                    X_te, y_te = X_te_raw[te.start - i_va_end:],                y_te_raw[te.start - i_va_end:]\n",
        "\n",
        "                    if len(X_va)==0 or len(X_te)==0:\n",
        "                        print(f\"[WARN] Not enough samples after purge for {label_tag} (LSTM).\")\n",
        "                    else:\n",
        "                        es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
        "                        lstm = make_lstm(input_shape=X_tr.shape[1:])\n",
        "                        lstm.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=MAX_EPOCHS, batch_size=BATCH, callbacks=[es], verbose=0)\n",
        "\n",
        "                        p_va = lstm.predict(X_va, verbose=0).ravel()\n",
        "                        h_va = (p_va >= 0.5).astype(int)\n",
        "                        p_te = lstm.predict(X_te, verbose=0).ravel()\n",
        "                        h_te = (p_te >= 0.5).astype(int)\n",
        "\n",
        "                        m = {\n",
        "                            \"acc\": accuracy_score(y_te, h_te) if len(y_te) else float(\"nan\"),\n",
        "                            \"f1\":  f1_score(y_te, h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                            \"auc\": roc_auc_score(y_te, p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                        }\n",
        "                        results[\"LSTM\"] = (None, m)\n",
        "\n",
        "                        # Map sequence indices back to dates\n",
        "                        val_idx  = idx[va]\n",
        "                        test_idx = idx[te]\n",
        "                        df_val  = pd.DataFrame({\"date\": val_idx,  \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\")\n",
        "                        df_test = pd.DataFrame({\"date\": test_idx, \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\")\n",
        "                        save_preds_df(df_val,  label, dataset_version, \"lstm\", window=WINDOW, split=\"val\")\n",
        "                        save_preds_df(df_test, label, dataset_version, \"lstm\", window=WINDOW, split=\"test\")\n",
        "\n",
        "    # ---- TXT log into /10_logs ---------------------------\n",
        "    if results:\n",
        "        lines = [\n",
        "            f\"{label} [{dataset_version}] — DL {','.join(results.keys())} (extended)\",\n",
        "            f\"[Run] FREQ={FREQ} | DEBUG={DEBUG} | RUN_ID={RUN_ID}\",\n",
        "            f\"[Data] Rows used: {rows_used} | Features: {len(extended_cols)}\",\n",
        "            f\"[Extended] Keywords used: {len(kws)}\",\n",
        "        ]\n",
        "        for name, (_x, met) in results.items():\n",
        "            lines.append(f\"{name}: ACC={met.get('acc', float('nan')):.3f} F1={met.get('f1', float('nan')):.3f} AUC={met.get('auc', float('nan')):.3f}\")\n",
        "        save_txt_log(label, dataset_version, f\"DL_{','.join(results.keys())}_extended\", lines)\n",
        "\n",
        "    print(f\"✅ Finished {label_tag} | rows: {rows_used} | features: {len(extended_cols)}\")\n"
      ],
      "metadata": {
        "id": "eaTi5-cs-B3-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# CONVENIENCE\n",
        "# - Batch runners for all assets\n",
        "# - Leaderboard builder (from logs)\n",
        "# - Backtest helpers (figures + summaries)\n",
        "# =========================================================\n",
        "\n",
        "def run_all_pairs(use_dl_mode=\"lstm\", dataset_version=\"raw\"):\n",
        "    for lbl, asset in asset_by_label.items():\n",
        "        print(\"=\"*70, f\"\\n{lbl.upper()} — {use_dl_mode.upper()} [{dataset_version}] FREQ={FREQ} DEBUG={DEBUG}\")\n",
        "        run_asset(asset, dataset_version=dataset_version, use_dl=True, use_dl_mode=use_dl_mode)\n",
        "\n",
        "def run_all_pairs_lstm(dataset_version=\"raw\"):\n",
        "    run_all_pairs(use_dl_mode=\"lstm\", dataset_version=dataset_version)\n",
        "\n",
        "def run_all_pairs_mlp(dataset_version=\"raw\"):\n",
        "    run_all_pairs(use_dl_mode=\"mlp\", dataset_version=dataset_version)\n",
        "\n",
        "# ---- leaderboard from /10_logs ---------------------------\n",
        "def build_leaderboard_from_logs(run_root: Path = RUN_ROOT):\n",
        "    pat_hdr   = re.compile(r\"^(?P<label>.+?) \\[(?P<dataset>.+?)\\] — (?P<run>.+)$\")\n",
        "    pat_model = re.compile(r\"^(?P<model>LR|RF|XGB|MLP|LSTM): ACC=(?P<acc>[\\d\\.]+) F1=(?P<f1>[\\d\\.NaN]+) AUC=(?P<auc>[\\d\\.NaN]+)\")\n",
        "    rows = []\n",
        "    for fp in sorted((run_root / \"10_logs\").glob(\"*.txt\")):\n",
        "        label = dataset = run = None\n",
        "        for ln in open(fp, \"r\"):\n",
        "            ln = ln.strip()\n",
        "            m1 = pat_hdr.match(ln)\n",
        "            if m1:\n",
        "                label, dataset, run = m1.group(\"label\"), m1.group(\"dataset\"), m1.group(\"run\"); continue\n",
        "            m2 = pat_model.match(ln)\n",
        "            if m2 and label and run:\n",
        "                rows.append({\n",
        "                    \"file\": fp.name,\n",
        "                    \"asset\": label,\n",
        "                    \"dataset\": dataset,\n",
        "                    \"run_type\": run,\n",
        "                    \"model\": m2.group(\"model\"),\n",
        "                    \"ACC\": float(m2.group(\"acc\")),\n",
        "                    \"F1\":  float(\"nan\") if m2.group(\"f1\")==\"NaN\" else float(m2.group(\"f1\")),\n",
        "                    \"AUC\": float(\"nan\") if m2.group(\"auc\")==\"NaN\" else float(m2.group(\"auc\")),\n",
        "                })\n",
        "    df_leader = (pd.DataFrame(rows)\n",
        "                 .sort_values([\"asset\",\"dataset\",\"AUC\",\"ACC\"], ascending=[True,True,False,False]))\n",
        "    path = save_leaderboard(df_leader, tag=\"metrics\")\n",
        "    print(\"Leaderboard saved:\", path)\n",
        "    return df_leader\n",
        "\n",
        "# ---- backtest helpers -----------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def backtest_pred_file(pred_csv_path: Path, price_csv_path: Path, asset_label: str,\n",
        "                       up_thr=0.55, down_thr=0.45, fee_bps=0.0005):\n",
        "    PRICE_COL_MAP = {a[\"label\"]: a[\"price_col\"] for a in ASSETS}\n",
        "    px = pd.read_csv(price_csv_path, parse_dates=[\"Date\"]).set_index(\"Date\")\n",
        "    price_col = PRICE_COL_MAP[asset_label]\n",
        "    px = px[[price_col]].rename(columns={price_col:\"Close\"})\n",
        "    px[\"ret1\"] = px[\"Close\"].pct_change()\n",
        "\n",
        "    preds = pd.read_csv(pred_csv_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
        "    dfb = preds.join(px, how=\"inner\").dropna()\n",
        "    dfb[\"pos\"] = np.where(dfb[\"prob_up\"] >= up_thr, 1, np.where(dfb[\"prob_up\"] <= down_thr, -1, 0))\n",
        "    dfb[\"pos_shift\"] = dfb[\"pos\"].shift(1).fillna(0)\n",
        "    dfb[\"turnover\"] = (dfb[\"pos\"] != dfb[\"pos_shift\"]).astype(int)\n",
        "    dfb[\"pnl\"] = dfb[\"pos_shift\"] * dfb[\"ret1\"] - dfb[\"turnover\"] * fee_bps\n",
        "    dfb[\"cum_pnl\"] = (1 + dfb[\"pnl\"]).cumprod()\n",
        "    return dfb\n",
        "\n",
        "def plot_and_save_equity(df_bt: pd.DataFrame, asset_label: str, model_tag: str):\n",
        "    fig = plt.figure()\n",
        "    df_bt[\"cum_pnl\"].plot()\n",
        "    plt.title(f\"{asset_label} – {model_tag}\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Equity\")\n",
        "    path = save_figure(fig, asset_label, f\"{_slug(model_tag)}_equity\")\n",
        "    plt.close(fig)\n",
        "    print(\"Figure saved:\", path)\n",
        "    return path\n",
        "\n",
        "def select_best_models(df_leader):\n",
        "    # top by AUC then ACC per asset/dataset\n",
        "    return (df_leader.sort_values([\"asset\",\"dataset\",\"AUC\",\"ACC\"], ascending=[True,True,False,False])\n",
        "                     .groupby([\"asset\",\"dataset\"]).head(1).reset_index(drop=True))\n",
        "\n",
        "def backtest_best_from_leaderboard(df_leader: pd.DataFrame, *, only_model=(\"LSTM\",\"MLP\")):\n",
        "    best = (df_leader[df_leader[\"model\"].isin(only_model)]\n",
        "            .sort_values([\"asset\",\"dataset\",\"AUC\",\"ACC\"], ascending=[True,True,False,False])\n",
        "            .groupby([\"asset\",\"dataset\"]).head(1))\n",
        "\n",
        "    summaries = []\n",
        "    for _, row in best.iterrows():\n",
        "        asset, dataset, model = row[\"asset\"], row[\"dataset\"], row[\"model\"]\n",
        "        # Guess test filename based on convention\n",
        "        guess = _pred_name(asset, dataset, \"lstm\", window=30, split=\"test\") if model==\"LSTM\" \\\n",
        "                else _pred_name(asset, dataset, \"mlp\",  window=None, split=\"test\")\n",
        "        pred_path = STAGES[\"preds_test\"] / guess\n",
        "        if not pred_path.exists():\n",
        "            print(\"Skip (no preds):\", pred_path.name); continue\n",
        "\n",
        "        bt = backtest_pred_file(pred_path, RAW_FILE, asset, up_thr=0.55, down_thr=0.45, fee_bps=0.0005)\n",
        "        model_tag = f\"{model} ({dataset})\"\n",
        "        save_backtest_table(bt, asset, model_tag)\n",
        "        plot_and_save_equity(bt, asset, model_tag)\n",
        "\n",
        "        summaries.append({\n",
        "            \"asset\": asset,\n",
        "            \"dataset\": dataset,\n",
        "            \"model\": model,\n",
        "            \"final_equity\": float(bt[\"cum_pnl\"].iloc[-1]),\n",
        "            \"obs\": int(bt[\"cum_pnl\"].shape[0])\n",
        "        })\n",
        "\n",
        "    if summaries:\n",
        "        df_sum = pd.DataFrame(summaries).sort_values([\"asset\",\"final_equity\"], ascending=[True,False])\n",
        "        path = STAGES[\"leaderboard\"] / \"backtest_summary.csv\"\n",
        "        df_sum.to_csv(path, index=False)\n",
        "        print(\"Backtest summary saved:\", path)\n",
        "        print(df_sum)\n",
        "    else:\n",
        "        print(\"No summaries produced (did any TEST predictions save?).\")\n"
      ],
      "metadata": {
        "id": "wLZD8qyWKU3D"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# RUNS\n",
        "# - Print config\n",
        "# - Train/evaluate (LSTM primary)\n",
        "# - Build leaderboard\n",
        "# - Backtest best per asset\n",
        "# =========================================================\n",
        "\n",
        "print(\"=== RUN CONFIG ===\")\n",
        "print(\"FREQ:\", FREQ, \"| DEBUG:\", DEBUG, \"| RUN_ID:\", RUN_ID)\n",
        "print(\"OUTPUT ROOT:\", RUN_ROOT)\n",
        "\n",
        "# 1) Train/evaluate (LSTM; switch to 'both' or 'mlp' if needed)\n",
        "run_all_pairs_lstm(dataset_version=\"raw\")\n",
        "\n",
        "# 2) (Optional) also compare engineered dataset if available:\n",
        "# If using engineered features file externally, you can construct df_basefreq_eng and call:\n",
        "# run_all_pairs_lstm(dataset_version=\"eng\")\n",
        "\n",
        "# 3) Leaderboard (metrics)\n",
        "df_leader = build_leaderboard_from_logs(RUN_ROOT)\n",
        "\n",
        "# 4) Backtest top models (saves tables + equity charts)\n",
        "backtest_best_from_leaderboard(df_leader, only_model=(\"LSTM\",\"MLP\"))\n",
        "\n",
        "print(\"\\n=== ARTIFACTS ===\")\n",
        "for name, path in STAGES.items():\n",
        "    num = len(list(path.glob(\"*\")))\n",
        "    print(f\"{name:>12}: {path}  ({num} files)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbnCif16Khqm",
        "outputId": "35ceea07-8558-4d13-9e48-5c54e14f05ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RUN CONFIG ===\n",
            "FREQ: D | DEBUG: True | RUN_ID: d_debug_20250912-062527\n",
            "OUTPUT ROOT: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527\n",
            "====================================================================== \n",
            "GOLD — LSTM [raw] FREQ=D DEBUG=True\n",
            "✅ Finished Gold [raw] | rows: 999 | features: 60\n",
            "====================================================================== \n",
            "BTC — LSTM [raw] FREQ=D DEBUG=True\n",
            "✅ Finished BTC [raw] | rows: 999 | features: 88\n",
            "====================================================================== \n",
            "OIL — LSTM [raw] FREQ=D DEBUG=True\n",
            "✅ Finished Oil [raw] | rows: 999 | features: 96\n",
            "====================================================================== \n",
            "USDCNY — LSTM [raw] FREQ=D DEBUG=True\n",
            "✅ Finished USDCNY [raw] | rows: 999 | features: 112\n",
            "Leaderboard saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/50_leaderboards/leaderboard_metrics.csv\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs/btc_lstm_raw_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs/gold_lstm_raw_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs/oil_lstm_raw_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs/usdcny_lstm_raw_equity.png\n",
            "Backtest summary saved: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/50_leaderboards/backtest_summary.csv\n",
            "    asset dataset model  final_equity  obs\n",
            "0     BTC     raw  LSTM      0.928097  141\n",
            "1    Gold     raw  LSTM      0.985035  141\n",
            "2     Oil     raw  LSTM      0.989633  141\n",
            "3  USDCNY     raw  LSTM      1.005149  141\n",
            "\n",
            "=== ARTIFACTS ===\n",
            "        data: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/00_data  (1 files)\n",
            "        logs: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/10_logs  (4 files)\n",
            "   preds_val: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/20_preds/val  (4 files)\n",
            "  preds_test: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/20_preds/test  (4 files)\n",
            "   backtests: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/30_backtests  (4 files)\n",
            "        figs: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/40_figs  (4 files)\n",
            " leaderboard: /content/drive/MyDrive/gt-markets/outputs/runs/d_debug_20250912-062527/50_leaderboards  (2 files)\n"
          ]
        }
      ]
    }
  ]
}