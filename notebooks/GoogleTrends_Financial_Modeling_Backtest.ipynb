{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyP/M6xJ28pzH4i5EMZwtROs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/main/notebooks/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "e5b78fdf-dfee-459e-f418-6460641c77a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Using PROJECT_DIR: /content/drive/MyDrive/gt-markets\n",
            "üìÅ DATA_DIR: /content/drive/MyDrive/gt-markets/data/processed\n",
            "üìÅ KW_DIR: /content/drive/MyDrive/gt-markets/data/Keyword Selection\n",
            "üìÅ OUT_DIR: /content/drive/MyDrive/gt-markets/outputs\n",
            "‚úÖ Libraries loaded.\n",
            "‚úÖ Setup complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =====================================================\n",
        "# 1) SETUP  ‚Äî  run once per Colab session\n",
        "# =====================================================\n",
        "\n",
        "# --- 1.1 Mount Google Drive & project paths ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Try common locations: MyDrive and Shared drives\n",
        "CANDIDATE_PROJECT_DIRS = [\n",
        "    Path(\"/content/drive/MyDrive/gt-markets\"),          # personal MyDrive\n",
        "    Path(\"/content/drive/Shared drives/gt-markets\"),    # Google Shared drive (team drive)\n",
        "    Path(\"/content/drive/MyDrive/SharedWithMe/gt-markets\"),  # occasional layout\n",
        "]\n",
        "\n",
        "PROJECT_DIR = next((p for p in CANDIDATE_PROJECT_DIRS if p.exists()), None)\n",
        "if PROJECT_DIR is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"‚ùå Could not find 'gt-markets' in MyDrive or Shared drives.\\n\"\n",
        "        f\"Tried:\\n- \" + \"\\n- \".join(str(p) for p in CANDIDATE_PROJECT_DIRS)\n",
        "    )\n",
        "\n",
        "DATA_DIR = PROJECT_DIR / \"data\" / \"processed\"\n",
        "KW_DIR   = PROJECT_DIR / \"data\" / \"Keyword Selection\"\n",
        "OUT_DIR  = PROJECT_DIR / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Using PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"üìÅ DATA_DIR:\", DATA_DIR)\n",
        "print(\"üìÅ KW_DIR:\", KW_DIR)\n",
        "print(\"üìÅ OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "\n",
        "# --- 1.2 Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "!pip -q install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"‚úÖ Libraries loaded.\")\n",
        "\n",
        "\n",
        "# --- 1.3 Data files (RAW vs ENGINEERED) ---\n",
        "# (Update filenames here if your dates change)\n",
        "RAW_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"\n",
        "ENG_FILE = DATA_DIR / \"merged_financial_trends_engineered_2025-09-07.csv\"\n",
        "KW_CSV   = KW_DIR   / \"combined_significant_lagged_correlations.csv\"\n",
        "\n",
        "assert RAW_FILE.exists(), f\"Missing RAW dataset: {RAW_FILE}\"\n",
        "assert ENG_FILE.exists(), f\"Missing ENGINEERED dataset: {ENG_FILE}\"\n",
        "assert KW_CSV.exists(),  f\"Missing keyword file: {KW_CSV}\"\n",
        "\n",
        "DATASETS = {\"raw\": RAW_FILE, \"engineered\": ENG_FILE}\n",
        "\n",
        "def get_dataset_path(dataset_version: str) -> Path:\n",
        "    \"\"\"Resolve dataset path by version: 'raw' or 'engineered'.\"\"\"\n",
        "    key = (dataset_version or \"raw\").strip().lower()\n",
        "    if key not in DATASETS:\n",
        "        raise ValueError(f\"Unknown dataset_version '{dataset_version}'. Use one of {list(DATASETS.keys())}.\")\n",
        "    return DATASETS[key]\n",
        "\n",
        "def tag_label(label: str, dataset_version: str) -> str:\n",
        "    \"\"\"Produce human label with dataset tag, e.g. 'Gold [raw]'.\"\"\"\n",
        "    return f\"{label} [{dataset_version}]\"\n",
        "\n",
        "\n",
        "# --- 1.4 Asset registry (no magic numbers) ---\n",
        "# Pair IDs must match the \"Pair\" column in James's keyword CSV.\n",
        "assets = [\n",
        "    {\"PAIR_ID\": \"GC=F\",    \"price_col\": \"GC=F Close\",    \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\", \"price_col\": \"BTC-USD Close\", \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",    \"price_col\": \"CL=F Close\",    \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"USDCNY=X\",   \"price_col\": \"USDCNY=X Close\",   \"label\": \"USDCNY\"},\n",
        "]\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in assets}\n",
        "asset_alias = {\n",
        "    \"gold\":\"gold\",\"gc=f\":\"gold\",\"xau\":\"gold\",\"xauusd\":\"gold\",\n",
        "    \"btc\":\"btc\",\"bitcoin\":\"btc\",\"btc-usd\":\"btc\",\n",
        "    \"oil\":\"oil\",\"cl=f\":\"oil\",\"wti\":\"oil\",\n",
        "    \"usdcny\":\"usdcny\",\"cny=x\":\"usdcny\",\"cny\":\"usdcny\",\n",
        "}\n",
        "\n",
        "def resolve_label(name: str) -> str:\n",
        "    key = (name or \"\").strip().lower()\n",
        "    return asset_alias.get(key, key)  # normalize alias ‚Üí canonical\n",
        "\n",
        "# --- 1.5 Utilities: target, features, eval, saving ---\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add 1-day return and classification target y_up (whether next day is up).\n",
        "    Drops rows with NaN so downstream code is clean.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"ret1\"] = df[price_col].pct_change()\n",
        "    df[\"y_up\"] = (df[\"ret1\"].shift(-1) > 0).astype(int)\n",
        "    return df.dropna().copy()\n",
        "\n",
        "def pick_start_index(n_rows: int, floor: int = 60, frac: float = 0.30, cap: int = 500) -> int:\n",
        "    \"\"\"\n",
        "    Choose a walk-forward starting index:\n",
        "    - at least 'floor' rows,\n",
        "    - about 'frac' of data as initial train window,\n",
        "    - but never exceed 'cap'.\n",
        "    \"\"\"\n",
        "    return max(floor, min(cap, int(n_rows * frac)))\n",
        "\n",
        "def walk_forward_eval(df_in: pd.DataFrame, feature_cols, model, start_index: int = 500):\n",
        "    \"\"\"\n",
        "    Expanding-window walk-forward:\n",
        "      train on [0:i) ‚Üí predict on [i]\n",
        "    IMPORTANT: Scale using train only ‚Üí transform test (no leakage).\n",
        "    Returns (pred_df, metrics_dict).\n",
        "    \"\"\"\n",
        "    df_in = df_in.copy()\n",
        "    X_all = df_in[feature_cols].values\n",
        "    y_all = df_in[\"y_up\"].values\n",
        "    idxs  = df_in.index\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    preds, probs, trues, dates = [], [], [], []\n",
        "\n",
        "    for i in range(start_index, len(df_in)):\n",
        "        X_train, y_train = X_all[:i], y_all[:i]\n",
        "        X_test,  y_test  = X_all[i:i+1], y_all[i]\n",
        "\n",
        "        X_train_s = scaler.fit_transform(X_train)\n",
        "        X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_s, y_train)\n",
        "        p    = model.predict_proba(X_test_s)[0, 1]\n",
        "        yhat = int(p >= 0.5)\n",
        "\n",
        "        preds.append(yhat); probs.append(p); trues.append(int(y_test)); dates.append(idxs[i])\n",
        "\n",
        "    out = pd.DataFrame({\"date\": dates, \"y_true\": trues, \"y_pred\": preds, \"prob_up\": probs}).set_index(\"date\")\n",
        "    acc = accuracy_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    f1  = f1_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    try:\n",
        "        auc = roc_auc_score(out[\"y_true\"], out[\"prob_up\"])\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "    return out, {\"acc\": acc, \"f1\": f1, \"auc\": auc}\n",
        "\n",
        "def save_run_group_txt(pair_label: str, run_label: str, results_dict: dict,\n",
        "                       keywords: list, features_count: int, rows_used: int, note: str = \"\"):\n",
        "    \"\"\"\n",
        "    Write ONE TXT per run containing:\n",
        "      - asset label + run label (+ timestamp)\n",
        "      - features/rows info\n",
        "      - keyword list (or 'None')\n",
        "      - metrics for ALL models in this run (LR/RF/XGB) or DL summary\n",
        "    \"\"\"\n",
        "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    safe_pair = pair_label.replace(\" \", \"_\").lower()\n",
        "    safe_run  = run_label.replace(\" \", \"_\").replace(\"+\", \"plus\").lower()\n",
        "    out_path  = OUT_DIR / f\"{safe_pair}_{safe_run}_{ts}.txt\"\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        f.write(f\"{pair_label} ‚Äî {run_label}\\n\")\n",
        "        f.write(f\"Timestamp: {ts}\\n\")\n",
        "        f.write(f\"Features used: {features_count}\\n\")\n",
        "        f.write(f\"Rows used: {rows_used}\\n\")\n",
        "        if note:\n",
        "            f.write(f\"Note: {note}\\n\")\n",
        "\n",
        "        f.write(\"\\nKeywords used:\\n\")\n",
        "        if keywords:\n",
        "            for k in keywords: f.write(f\"- {k}\\n\")\n",
        "        else:\n",
        "            f.write(\"- None\\n\")\n",
        "\n",
        "        f.write(\"\\nResults (all models):\\n\")\n",
        "        for model_name, (_out, met) in results_dict.items():\n",
        "            acc = met.get(\"acc\", float(\"nan\"))\n",
        "            f1  = met.get(\"f1\",  float(\"nan\"))\n",
        "            auc = met.get(\"auc\", float(\"nan\"))\n",
        "            f.write(f\"{model_name}: ACC={acc:.3f}, F1={f1:.3f}, AUC={auc:.3f}\\n\")\n",
        "\n",
        "    print(\"üìù Saved:\", out_path)\n",
        "\n",
        "def sanitize_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Defensive feature cleaning:\n",
        "    - replace ¬±inf with NaN\n",
        "    - fill NaN with 0 (only for derived columns we add)\n",
        "    - clip extreme outliers to 0.1%/99.9% quantiles\n",
        "    \"\"\"\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    lo = X.quantile(0.001)\n",
        "    hi = X.quantile(0.999)\n",
        "    return X.clip(lower=lo, upper=hi, axis=1)\n",
        "\n",
        "def build_trend_lag_features(frame: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    From *_trend level series, create leak-safe daily change features:\n",
        "      - 1-day % change, lagged by 1 day\n",
        "      - 7-day % change, lagged by 1 day\n",
        "    \"\"\"\n",
        "    feats = {}\n",
        "    for c in cols:\n",
        "        chg1 = frame[c].pct_change()\n",
        "        chg7 = frame[c].pct_change(7)\n",
        "        feats[c+\"_chg1_lag1\"] = chg1.shift(1)\n",
        "        feats[c+\"_chg7_lag1\"] = chg7.shift(1)\n",
        "    return pd.DataFrame(feats, index=frame.index)\n",
        "\n",
        "def map_keywords_to_trend_cols(df_cols: list, keywords: list) -> list:\n",
        "    \"\"\"\n",
        "    Map keyword strings (from James's CSV) to *_trend columns present in our dataset.\n",
        "    Matching is normalized to tolerate underscores/symbols.\n",
        "    \"\"\"\n",
        "    trend_cols_all = [c for c in df_cols if str(c).endswith(\"_trend\")]\n",
        "    def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
        "    kw_to_col = []\n",
        "    for kw in keywords:\n",
        "        n = _norm(kw)\n",
        "        hits = [c for c in trend_cols_all if n in _norm(c.replace(\"_trend\",\"\"))]\n",
        "        if hits:\n",
        "            kw_to_col.append(hits[0])  # first reasonable match\n",
        "    # de-dup while preserving order\n",
        "    seen, ordered = set(), []\n",
        "    for c in kw_to_col:\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list:\n",
        "    \"\"\"Load keywords from James's CSV for a given Pair ID (e.g., 'GC=F').\"\"\"\n",
        "    dfk = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\",\"Keyword\"}.issubset(dfk.columns), f\"Unexpected keyword CSV columns: {dfk.columns}\"\n",
        "    kw = (dfk.loc[dfk[\"Pair\"] == pair_id, \"Keyword\"]\n",
        "            .dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "    return kw\n",
        "\n",
        "\n",
        "# --- 1.6 Models (re-used across assets) ---\n",
        "models = {\n",
        "    \"LR\":  LogisticRegression(max_iter=500),\n",
        "    \"RF\":  RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "    \"XGB\": XGBClassifier(\n",
        "        n_estimators=500, max_depth=4, learning_rate=0.05,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        tree_method=\"hist\", random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 2) RUNNER ‚Äî Baseline ‚Üí Extended ‚Üí (DL) for one asset\n",
        "# =====================================================\n",
        "\n",
        "def run_asset(asset: dict, dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Full pipeline for a single asset + chosen dataset:\n",
        "      dataset_version ‚àà {'raw','engineered'}\n",
        "      1) Baseline: technical features only (no *_trend)\n",
        "      2) Extended: baseline + lagged trend-change features for this asset's keywords\n",
        "      3) Deep Learning: MLP on extended features (optional)\n",
        "    Produces one TXT per run with results for ALL models.\n",
        "    \"\"\"\n",
        "    label       = asset[\"label\"]\n",
        "    pair_id     = asset[\"PAIR_ID\"]\n",
        "    price_col   = asset[\"price_col\"]\n",
        "\n",
        "    ds_path     = get_dataset_path(dataset_version)\n",
        "    label_tag   = tag_label(label, dataset_version)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"‚ñ∂Ô∏è  Running {label_tag}  |  Pair: {pair_id}\")\n",
        "    print(f\"    Dataset: {ds_path.name}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # --- Load & target ---\n",
        "    df = pd.read_csv(ds_path, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    df_mod = make_target(df, price_col)\n",
        "    print(f\"[Data] {df_mod.index.min().date()} ‚Üí {df_mod.index.max().date()} | Rows: {len(df_mod)}\")\n",
        "\n",
        "    # --- BASELINE: technical-only (exclude *_trend) ---\n",
        "    all_trends = [c for c in df_mod.columns if str(c).endswith(\"_trend\")]\n",
        "    exclude    = {price_col, \"ret1\", \"y_up\"} | set(all_trends)\n",
        "    numeric    = [c for c in df_mod.columns if df_mod[c].dtype != \"O\"]\n",
        "    baseline_cols = [c for c in numeric if c not in exclude]\n",
        "    print(f\"[Baseline] Features: {len(baseline_cols)}\")\n",
        "\n",
        "    start_idx = pick_start_index(len(df_mod))\n",
        "    results_baseline = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_b, m_b = walk_forward_eval(df_mod, baseline_cols, mdl, start_index=start_idx)\n",
        "        results_baseline[name] = (out_b, m_b)\n",
        "        print(f\"  BASE {name}: ACC={m_b['acc']:.3f} F1={m_b['f1']:.3f} AUC={m_b['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label_tag, \"baseline\", results_baseline,\n",
        "                       keywords=[], features_count=len(baseline_cols), rows_used=len(df_mod))\n",
        "\n",
        "    # --- EXTENDED: baseline + (asset) keywords as lagged changes ---\n",
        "    keep_keywords = load_keywords_for_pair(KW_CSV, pair_id)\n",
        "    print(f\"[Extended] Keywords for {pair_id}: {len(keep_keywords)}\")\n",
        "    sel_trend_cols = map_keywords_to_trend_cols(df_mod.columns, keep_keywords)\n",
        "\n",
        "    if not sel_trend_cols:\n",
        "        print(\"[WARN] No *_trend columns matched these keywords. Skipping extended & DL.\")\n",
        "        return\n",
        "\n",
        "    lag_feats = build_trend_lag_features(df_mod, sel_trend_cols)\n",
        "    # Drop sparse derived cols (>50% NaN), then warmup-trim (pct_change(7)+shift)\n",
        "    nan_ratio = lag_feats.isna().mean()\n",
        "    kept_cols = nan_ratio[nan_ratio < 0.50].index.tolist()\n",
        "    lag_feats = lag_feats[kept_cols]\n",
        "    WARMUP = 8\n",
        "    df_ext = df_mod.join(lag_feats).iloc[WARMUP:].copy()\n",
        "    df_ext[kept_cols] = df_ext[kept_cols].fillna(0.0)\n",
        "\n",
        "    extended_cols = sorted(set(baseline_cols).union(kept_cols))\n",
        "    df_ext[extended_cols] = sanitize_features(df_ext[extended_cols])\n",
        "    print(f\"[Extended] Features: {len(extended_cols)} | Rows: {len(df_ext)}\")\n",
        "\n",
        "    start_idx_ext = pick_start_index(len(df_ext))\n",
        "    results_extended = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_e, m_e = walk_forward_eval(df_ext, extended_cols, mdl, start_index=start_idx_ext)\n",
        "        results_extended[name] = (out_e, m_e)\n",
        "        print(f\"  EXT  {name}: ACC={m_e['acc']:.3f} F1={m_e['f1']:.3f} AUC={m_e['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label_tag, \"baseline + keywords\", results_extended,\n",
        "                       keywords=keep_keywords, features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    # --- DL (optional): MLP on extended features ---\n",
        "    if use_dl:\n",
        "        print(\"[DL] MLP on extended features‚Ä¶\")\n",
        "        X = df_ext[extended_cols].values\n",
        "        y = df_ext[\"y_up\"].astype(int).values\n",
        "\n",
        "        n      = len(df_ext)\n",
        "        i_tr   = int(n * 0.70)\n",
        "        i_va   = int(n * 0.85)\n",
        "        X_tr, X_va, X_te = X[:i_tr], X[i_tr:i_va], X[i_va:]\n",
        "        y_tr, y_va, y_te = y[:i_tr], y[i_tr:i_va], y[i_va:]\n",
        "\n",
        "        scaler = StandardScaler().fit(X_tr)\n",
        "        X_tr, X_va, X_te = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "        tf.keras.utils.set_random_seed(42)\n",
        "        mlp = keras.Sequential([\n",
        "            keras.layers.Input(shape=(X_tr.shape[1],)),\n",
        "            keras.layers.Dense(128, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.30),\n",
        "            keras.layers.Dense(64, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.30),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ])\n",
        "        mlp.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[keras.metrics.AUC(name=\"auc\"),\n",
        "                             keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
        "\n",
        "        es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
        "                                           patience=5, restore_best_weights=True)\n",
        "        mlp.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
        "                epochs=30, batch_size=64, callbacks=[es], verbose=0)\n",
        "\n",
        "        p_te = mlp.predict(X_te, verbose=0).ravel()\n",
        "        yhat = (p_te >= 0.5).astype(int)\n",
        "        dl_metrics = {\n",
        "            \"acc\": accuracy_score(y_te, yhat),\n",
        "            \"f1\":  f1_score(y_te, yhat),\n",
        "            \"auc\": roc_auc_score(y_te, p_te) if len(set(y_te)) > 1 else np.nan\n",
        "        }\n",
        "        print(f\"  DL MLP: ACC={dl_metrics['acc']:.3f} F1={dl_metrics['f1']:.3f} AUC={dl_metrics['auc']:.3f}\")\n",
        "\n",
        "        save_run_group_txt(label_tag, \"DL MLP (extended)\",\n",
        "                           {\"MLP\": (None, dl_metrics)},\n",
        "                           keywords=keep_keywords, features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    print(f\"‚úÖ Finished {label_tag}.\")"
      ],
      "metadata": {
        "id": "eaTi5-cs-B3-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 3) CONVENIENCE  ‚Äî run by readable label\n",
        "# =====================================================\n",
        "\n",
        "def run_asset_by_label(name: str, dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Call like:\n",
        "      run_asset_by_label(\"Gold\", dataset_version=\"raw\")\n",
        "      run_asset_by_label(\"BTC\",  dataset_version=\"engineered\")\n",
        "    Aliases like 'gc=f', 'wti', 'cny=x' also work.\n",
        "    \"\"\"\n",
        "    canon = resolve_label(name)\n",
        "    asset = asset_by_label.get(canon)\n",
        "    if asset is None:\n",
        "        print(f\"[!] Unknown asset '{name}'. Available: {list(asset_by_label.keys())}\")\n",
        "        return\n",
        "    run_asset(asset, dataset_version=dataset_version, use_dl=use_dl)\n",
        "\n",
        "def run_pair_raw_and_engineered(name: str, use_dl: bool = True):\n",
        "    \"\"\"Run the same asset twice: RAW then ENGINEERED.\"\"\"\n",
        "    run_asset_by_label(name, dataset_version=\"raw\",         use_dl=use_dl)\n",
        "    run_asset_by_label(name, dataset_version=\"engineered\",  use_dl=use_dl)\n",
        "\n",
        "def run_all_pairs(dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"Loop all assets for a chosen dataset_version ('raw' or 'engineered').\"\"\"\n",
        "    for lbl in asset_by_label.keys():\n",
        "        run_asset_by_label(lbl, dataset_version=dataset_version, use_dl=use_dl)\n",
        "\n",
        "print(\"‚úÖ Ready: use run_asset_by_label('Gold', dataset_version='raw') etc.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZD8qyWKU3D",
        "outputId": "af5661be-f677-463d-9d0c-c8e21b2fa8e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ready: use run_asset_by_label('Gold', dataset_version='raw') etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 4) RUNS  ‚Äî choose what to execute\n",
        "# =====================================================\n",
        "\n",
        "# Examples (uncomment what you need):\n",
        "\n",
        "# 4.1 Gold only (RAW)\n",
        "run_asset_by_label(\"Gold\", dataset_version=\"raw\", use_dl=True)\n",
        "\n",
        "# 4.2 Gold only (ENGINEERED)\n",
        "# run_asset_by_label(\"Gold\", dataset_version=\"engineered\", use_dl=True)\n",
        "\n",
        "# 4.3 Gold on BOTH datasets\n",
        "# run_pair_raw_and_engineered(\"Gold\", use_dl=True)\n",
        "\n",
        "# 4.4 All pairs on RAW (7 runs per pair)\n",
        "# run_all_pairs(dataset_version=\"raw\", use_dl=True)\n",
        "\n",
        "# 4.5 All pairs on ENGINEERED (another 7 runs per pair)\n",
        "# run_all_pairs(dataset_version=\"engineered\", use_dl=True)\n",
        "\n",
        "# TXT outputs appear in:\n",
        "#   /content/drive/MyDrive/gt-markets/outputs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nbnCif16Khqm",
        "outputId": "7db60cac-c27d-476c-adfb-a2308465cb9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running Gold [raw]  |  Pair: GC=F\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.503 F1=0.480 AUC=0.492\n",
            "  BASE RF: ACC=0.468 F1=0.445 AUC=0.473\n",
            "  BASE XGB: ACC=0.475 F1=0.468 AUC=0.467\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_baseline_20250910-052006.txt\n",
            "[Extended] Keywords for GC=F: 15\n",
            "[Extended] Features: 66 | Rows: 977\n",
            "  EXT  LR: ACC=0.484 F1=0.471 AUC=0.476\n",
            "  EXT  RF: ACC=0.497 F1=0.472 AUC=0.501\n",
            "  EXT  XGB: ACC=0.472 F1=0.475 AUC=0.475\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_baseline_plus_keywords_20250910-062135.txt\n",
            "[DL] MLP on extended features‚Ä¶\n",
            "  DL MLP: ACC=0.571 F1=0.609 AUC=0.571\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_dl_mlp_(extended)_20250910-062141.txt\n",
            "‚úÖ Finished Gold [raw].\n",
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running Gold [raw]  |  Pair: GC=F\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.503 F1=0.480 AUC=0.492\n",
            "  BASE RF: ACC=0.468 F1=0.445 AUC=0.473\n",
            "  BASE XGB: ACC=0.475 F1=0.468 AUC=0.467\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_baseline_20250910-070644.txt\n",
            "[Extended] Keywords for GC=F: 15\n",
            "[Extended] Features: 66 | Rows: 977\n",
            "  EXT  LR: ACC=0.484 F1=0.471 AUC=0.476\n",
            "  EXT  RF: ACC=0.497 F1=0.472 AUC=0.501\n",
            "  EXT  XGB: ACC=0.472 F1=0.475 AUC=0.475\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_baseline_plus_keywords_20250910-080830.txt\n",
            "[DL] MLP on extended features‚Ä¶\n",
            "  DL MLP: ACC=0.571 F1=0.609 AUC=0.571\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/gold_[raw]_dl_mlp_(extended)_20250910-080835.txt\n",
            "‚úÖ Finished Gold [raw].\n",
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running BTC [raw]  |  Pair: BTC-USD\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.504 F1=0.605 AUC=0.487\n",
            "  BASE RF: ACC=0.513 F1=0.584 AUC=0.493\n",
            "  BASE XGB: ACC=0.497 F1=0.573 AUC=0.457\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/btc_[raw]_baseline_20250910-085248.txt\n",
            "[Extended] Keywords for BTC-USD: 22\n",
            "[Extended] Features: 82 | Rows: 977\n",
            "  EXT  LR: ACC=0.500 F1=0.582 AUC=0.512\n",
            "  EXT  RF: ACC=0.520 F1=0.608 AUC=0.480\n",
            "  EXT  XGB: ACC=0.509 F1=0.596 AUC=0.471\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/btc_[raw]_baseline_plus_keywords_20250910-100324.txt\n",
            "[DL] MLP on extended features‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f386fe12840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f386fe12840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  DL MLP: ACC=0.422 F1=0.370 AUC=0.369\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/btc_[raw]_dl_mlp_(extended)_20250910-100328.txt\n",
            "‚úÖ Finished BTC [raw].\n",
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running Oil [raw]  |  Pair: CL=F\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.514 F1=0.539 AUC=0.509\n",
            "  BASE RF: ACC=0.501 F1=0.526 AUC=0.508\n",
            "  BASE XGB: ACC=0.530 F1=0.552 AUC=0.520\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/oil_[raw]_baseline_20250910-104820.txt\n",
            "[Extended] Keywords for CL=F: 24\n",
            "[Extended] Features: 84 | Rows: 977\n",
            "  EXT  LR: ACC=0.499 F1=0.523 AUC=0.489\n",
            "  EXT  RF: ACC=0.501 F1=0.546 AUC=0.495\n",
            "  EXT  XGB: ACC=0.488 F1=0.514 AUC=0.472\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/oil_[raw]_baseline_plus_keywords_20250910-115953.txt\n",
            "[DL] MLP on extended features‚Ä¶\n",
            "  DL MLP: ACC=0.476 F1=0.584 AUC=0.513\n",
            "üìù Saved: /content/drive/MyDrive/gt-markets/outputs/oil_[raw]_dl_mlp_(extended)_20250910-115957.txt\n",
            "‚úÖ Finished Oil [raw].\n",
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running USDCNY [raw]  |  Pair: CNY=X\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'CNY=X Close'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CNY=X Close'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2537713271.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 4.4 All pairs on RAW (7 runs per pair)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mrun_all_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 4.5 All pairs on ENGINEERED (another 7 runs per pair)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3153775188.py\u001b[0m in \u001b[0;36mrun_all_pairs\u001b[0;34m(dataset_version, use_dl)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"\"\"Loop all assets for a chosen dataset_version ('raw' or 'engineered').\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masset_by_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mrun_asset_by_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Ready: use run_asset_by_label('Gold', dataset_version='raw') etc.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3153775188.py\u001b[0m in \u001b[0;36mrun_asset_by_label\u001b[0;34m(name, dataset_version, use_dl)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[!] Unknown asset '{name}'. Available: {list(asset_by_label.keys())}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrun_asset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_pair_raw_and_engineered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-416264276.py\u001b[0m in \u001b[0;36mrun_asset\u001b[0;34m(asset, dataset_version, use_dl)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# --- Load & target ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdf_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Data] {df_mod.index.min().date()} ‚Üí {df_mod.index.max().date()} | Rows: {len(df_mod)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-374914394.py\u001b[0m in \u001b[0;36mmake_target\u001b[0;34m(df, price_col)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m    105\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ret1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprice_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpct_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_up\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ret1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'CNY=X Close'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_asset_by_label(\"cny\", dataset_version=\"raw\", use_dl=True)"
      ],
      "metadata": {
        "id": "jPnyTu1M4pm6",
        "outputId": "4e727f5d-76e5-4340-dc4d-70c4b2b85502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running USDCNY [raw]  |  Pair: USDCNY=X\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.632 F1=0.598 AUC=0.698\n"
          ]
        }
      ]
    }
  ]
}