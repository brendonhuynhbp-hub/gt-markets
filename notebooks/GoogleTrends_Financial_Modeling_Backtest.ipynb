{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyMCf700bDinr6vJNyMnCVti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/main/notebooks/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "25816f70-05eb-4930-a700-ffaa00b18012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Setup: libs loaded.\n",
            "‚úÖ Setup complete: config, helpers, models ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =====================================================\n",
        "# 1) SETUP  ‚Äî  one-time per Colab session\n",
        "# =====================================================\n",
        "\n",
        "# --- 1.1 Mount Google Drive & set project paths ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "PROJECT_DIR = Path(\"/content/drive/MyDrive/gt-markets\")\n",
        "DATA_DIR    = PROJECT_DIR / \"data\" / \"processed\"\n",
        "OUT_DIR     = PROJECT_DIR / \"outputs\"\n",
        "KW_DIR      = PROJECT_DIR / \"data\" / \"Keyword Selection\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 1.2 Libraries (ML/DL + utils) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "!pip -q install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"‚úÖ Setup: libs loaded.\")\n",
        "\n",
        "# --- 1.3 Config (filenames & safe defaults) ---\n",
        "MERGED_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"  # raw merged (prices + trends)\n",
        "KW_CSV      = KW_DIR / \"combined_significant_lagged_correlations.csv\"   # James's keyword selection\n",
        "assert MERGED_FILE.exists(), f\"Missing merged dataset: {MERGED_FILE}\"\n",
        "assert KW_CSV.exists(), f\"Missing keyword file: {KW_CSV}\"\n",
        "\n",
        "# --- 1.4 Asset registry (no magic indices anywhere) ---\n",
        "# PAIR_ID must match exactly what's in the Keyword CSV \"Pair\" column.\n",
        "assets = [\n",
        "    {\"PAIR_ID\": \"GC=F\",    \"price_col\": \"GC=F Close\",    \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\", \"price_col\": \"BTC-USD Close\", \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",    \"price_col\": \"CL=F Close\",    \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"CNY=X\",   \"price_col\": \"CNY=X Close\",   \"label\": \"USDCNY\"},\n",
        "]\n",
        "\n",
        "# Fast lookup by label\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in assets}\n",
        "\n",
        "# Optional aliases so you can call run_asset_by_label(\"gc=f\") etc.\n",
        "asset_alias = {\n",
        "    \"gold\": \"gold\", \"gc=f\": \"gold\", \"xau\": \"gold\", \"xauusd\": \"gold\",\n",
        "    \"btc\": \"btc\", \"bitcoin\": \"btc\", \"btc-usd\": \"btc\",\n",
        "    \"oil\": \"oil\", \"cl=f\": \"oil\", \"wti\": \"oil\",\n",
        "    \"usdcny\": \"usdcny\", \"cny=x\": \"usdcny\", \"cny\": \"usdcny\",\n",
        "}\n",
        "\n",
        "def resolve_label(name: str) -> str:\n",
        "    key = (name or \"\").strip().lower()\n",
        "    return asset_alias.get(key, key)  # normalize known aliases ‚Üí canonical\n",
        "\n",
        "# --- 1.5 Reusable helpers (target, features, evaluation, saving) ---\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add returns & classification target for 'next-day up' based on the given price column.\n",
        "    We drop rows with NaNs afterward so downstream code sees a clean panel.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"ret1\"] = df[price_col].pct_change()\n",
        "    df[\"y_up\"] = (df[\"ret1\"].shift(-1) > 0).astype(int)\n",
        "    return df.dropna().copy()\n",
        "\n",
        "def pick_start_index(n_rows: int, floor: int = 60, frac: float = 0.30, cap: int = 500) -> int:\n",
        "    \"\"\"\n",
        "    Choose a walk-forward starting point:\n",
        "    - enough warm-up for the scaler and model to stabilize (floor),\n",
        "    - a fraction of data to train first (frac),\n",
        "    - capped so we don't waste tons of history (cap).\n",
        "    \"\"\"\n",
        "    return max(floor, min(cap, int(n_rows * frac)))\n",
        "\n",
        "def walk_forward_eval(df_in: pd.DataFrame, feature_cols, model, start_index: int = 500):\n",
        "    \"\"\"\n",
        "    Expanding-window walk-forward:\n",
        "      train on [0:i) ‚Üí predict on [i]\n",
        "      IMPORTANT: scale using train only ‚Üí transform test (no leakage).\n",
        "    Returns (pred_df, metrics_dict).\n",
        "    \"\"\"\n",
        "    df_in = df_in.copy()\n",
        "    X_all = df_in[feature_cols].values\n",
        "    y_all = df_in[\"y_up\"].values\n",
        "    idxs  = df_in.index\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    preds, probs, trues, dates = [], [], [], []\n",
        "\n",
        "    for i in range(start_index, len(df_in)):\n",
        "        X_train, y_train = X_all[:i], y_all[:i]\n",
        "        X_test,  y_test  = X_all[i:i+1], y_all[i]\n",
        "\n",
        "        X_train_s = scaler.fit_transform(X_train)\n",
        "        X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_s, y_train)\n",
        "        p = model.predict_proba(X_test_s)[0, 1]\n",
        "        yhat = int(p >= 0.5)\n",
        "\n",
        "        preds.append(yhat); probs.append(p); trues.append(int(y_test)); dates.append(idxs[i])\n",
        "\n",
        "    out = pd.DataFrame({\"date\": dates, \"y_true\": trues, \"y_pred\": preds, \"prob_up\": probs}).set_index(\"date\")\n",
        "    acc = accuracy_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    f1  = f1_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    try:\n",
        "        auc = roc_auc_score(out[\"y_true\"], out[\"prob_up\"])\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "    return out, {\"acc\": acc, \"f1\": f1, \"auc\": auc}\n",
        "\n",
        "def save_run_group_txt(pair_label: str, run_label: str, results_dict: dict,\n",
        "                       keywords: list, features_count: int, rows_used: int, note: str = \"\"):\n",
        "    \"\"\"\n",
        "    Write one TXT per run containing:\n",
        "      - asset label + run label\n",
        "      - features/rows info\n",
        "      - keyword list (or 'None')\n",
        "      - metrics for ALL models in this run\n",
        "    \"\"\"\n",
        "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    safe_pair = pair_label.replace(\" \", \"_\").lower()\n",
        "    safe_run  = run_label.replace(\" \", \"_\").replace(\"+\", \"plus\").lower()\n",
        "    out_path  = OUT_DIR / f\"{safe_pair}_{safe_run}_{ts}.txt\"\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        f.write(f\"{pair_label} ‚Äî {run_label}\\n\")\n",
        "        f.write(f\"Timestamp: {ts}\\n\")\n",
        "        f.write(f\"Features used: {features_count}\\n\")\n",
        "        f.write(f\"Rows used: {rows_used}\\n\")\n",
        "        if note:\n",
        "            f.write(f\"Note: {note}\\n\")\n",
        "\n",
        "        f.write(\"\\nKeywords used:\\n\")\n",
        "        if keywords:\n",
        "            for k in keywords:\n",
        "                f.write(f\"- {k}\\n\")\n",
        "        else:\n",
        "            f.write(\"- None\\n\")\n",
        "\n",
        "        f.write(\"\\nResults (all models):\\n\")\n",
        "        for model_name, (_out, met) in results_dict.items():\n",
        "            f.write(f\"{model_name}: ACC={met['acc']:.3f}, F1={met['f1']:.3f}, AUC={met['auc']:.3f}\\n\")\n",
        "\n",
        "    print(\"üìù Saved:\", out_path)\n",
        "\n",
        "def sanitize_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Be defensive: replace inf, fill NaN, clip outliers (winsorize 0.1%‚Äì99.9%).\n",
        "    \"\"\"\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    lo = X.quantile(0.001)\n",
        "    hi = X.quantile(0.999)\n",
        "    return X.clip(lower=lo, upper=hi, axis=1)\n",
        "\n",
        "def build_trend_lag_features(frame: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    From *_trend columns, compute leak-safe change features:\n",
        "      - 1-day % change, lagged 1 day\n",
        "      - 7-day % change, lagged 1 day\n",
        "    \"\"\"\n",
        "    feats = {}\n",
        "    for c in cols:\n",
        "        chg1 = frame[c].pct_change()\n",
        "        chg7 = frame[c].pct_change(7)\n",
        "        feats[c+\"_chg1_lag1\"] = chg1.shift(1)  # only info available at t-1\n",
        "        feats[c+\"_chg7_lag1\"] = chg7.shift(1)\n",
        "    return pd.DataFrame(feats, index=frame.index)\n",
        "\n",
        "def map_keywords_to_trend_cols(df_cols: list, keywords: list) -> list:\n",
        "    \"\"\"\n",
        "    Match keyword strings (from James's CSV) to *_trend columns present in our dataset.\n",
        "    We normalize both sides to be resilient to underscores/spacing.\n",
        "    \"\"\"\n",
        "    trend_cols_all = [c for c in df_cols if str(c).endswith(\"_trend\")]\n",
        "    def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
        "    kw_to_col, unmatched = {}, []\n",
        "    for kw in keywords:\n",
        "        n = _norm(kw)\n",
        "        hits = [c for c in trend_cols_all if n in _norm(c.replace(\"_trend\",\"\"))]\n",
        "        if hits:\n",
        "            kw_to_col[kw] = hits[0]  # first match is fine\n",
        "        else:\n",
        "            unmatched.append(kw)\n",
        "    # (We could print unmatched here if you want to inspect)\n",
        "    return list(dict.fromkeys(kw_to_col.values()))  # de-dup, keep order\n",
        "\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list:\n",
        "    \"\"\"\n",
        "    Pull all keywords from James's file for the given Pair (e.g., 'GC=F').\n",
        "    \"\"\"\n",
        "    dfk = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\", \"Keyword\"}.issubset(dfk.columns), f\"Unexpected keyword CSV columns: {dfk.columns}\"\n",
        "    subset = dfk[dfk[\"Pair\"] == pair_id]\n",
        "    kw = (subset[\"Keyword\"].dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "    return kw\n",
        "\n",
        "# --- 1.6 Models (define once; re-used across assets) ---\n",
        "models = {\n",
        "    \"LR\":  LogisticRegression(max_iter=500),\n",
        "    \"RF\":  RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "    \"XGB\": XGBClassifier(\n",
        "        n_estimators=500, max_depth=4, learning_rate=0.05,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        tree_method=\"hist\", random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Setup complete: config, helpers, models ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 2) RUNNER ‚Äî Baseline ‚Üí Extended ‚Üí (DL) for one asset\n",
        "# =====================================================\n",
        "\n",
        "def run_asset(asset: dict, keyword_csv: Path, use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Run the full pipeline for a single asset:\n",
        "      1) Baseline (no trend features)\n",
        "      2) Extended (add GOLD/BTC/OIL/USDCNY keyword trends as lagged features)\n",
        "      3) Deep Learning (MLP on extended features) ‚Äî optional\n",
        "    Saves one TXT per run with metrics for ALL models.\n",
        "    \"\"\"\n",
        "    label     = asset[\"label\"]\n",
        "    pair_id   = asset[\"PAIR_ID\"]\n",
        "    price_col = asset[\"price_col\"]\n",
        "\n",
        "    print(f\"\\n==============================\\n‚ñ∂Ô∏è  Running {label} (Pair={pair_id})\\n==============================\")\n",
        "\n",
        "    # --- Load & prep dataset ---\n",
        "    df = pd.read_csv(MERGED_FILE, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    # Build target for THIS asset\n",
        "    df_mod = make_target(df, price_col)\n",
        "    print(f\"[Data] Range: {df_mod.index.min().date()} ‚Üí {df_mod.index.max().date()} | Rows: {len(df_mod)}\")\n",
        "\n",
        "    # --- BASELINE (technical-only) ---\n",
        "    # Exclude raw price, direct return/target, and ALL *_trend columns to keep baseline clean\n",
        "    all_trend_cols = [c for c in df_mod.columns if c.endswith(\"_trend\")]\n",
        "    exclude = {price_col, \"ret1\", \"y_up\"} | set(all_trend_cols)\n",
        "    numeric = [c for c in df_mod.columns if df_mod[c].dtype != \"O\"]\n",
        "    baseline_cols = [c for c in numeric if c not in exclude]\n",
        "    print(f\"[Baseline] Features: {len(baseline_cols)}\")\n",
        "\n",
        "    start_idx = pick_start_index(len(df_mod))\n",
        "    results_baseline = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_b, m_b = walk_forward_eval(df_mod, baseline_cols, mdl, start_index=start_idx)\n",
        "        results_baseline[name] = (out_b, m_b)\n",
        "        print(f\"  BASE {name}: ACC={m_b['acc']:.3f} F1={m_b['f1']:.3f} AUC={m_b['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label, \"baseline\", results_baseline, keywords=[],\n",
        "                       features_count=len(baseline_cols), rows_used=len(df_mod))\n",
        "\n",
        "    # --- EXTENDED (baseline + keyword trend features) ---\n",
        "    # 1) Load pair-specific keywords\n",
        "    keep_keywords = load_keywords_for_pair(keyword_csv, pair_id)\n",
        "    print(f\"[Extended] Keywords found for {pair_id}: {len(keep_keywords)}\")\n",
        "\n",
        "    # 2) Map keywords ‚Üí columns actually present in our dataset\n",
        "    selected_trend_cols = map_keywords_to_trend_cols(df_mod.columns, keep_keywords)\n",
        "    if not selected_trend_cols:\n",
        "        print(\"[WARN] No trend columns matched for these keywords. Skipping extended run.\")\n",
        "        return\n",
        "\n",
        "    # 3) Turn trend level into lagged changes (leak-safe), and drop sparse ones\n",
        "    lag_feats = build_trend_lag_features(df_mod, selected_trend_cols)\n",
        "    nan_ratio = lag_feats.isna().mean()\n",
        "    kept = nan_ratio[nan_ratio < 0.50].index.tolist()\n",
        "    lag_feats = lag_feats[kept]\n",
        "    print(f\"[Extended] New lagged trend features kept: {len(kept)}\")\n",
        "\n",
        "    # 4) Assemble extended dataset and sanitize feature matrix\n",
        "    WARMUP = 8  # due to pct_change(7) then shift(1)\n",
        "    df_ext = df_mod.join(lag_feats).iloc[WARMUP:].copy()\n",
        "    df_ext[kept] = df_ext[kept].fillna(0.0)\n",
        "\n",
        "    extended_cols = sorted(set(baseline_cols).union(kept))\n",
        "    df_ext[extended_cols] = sanitize_features(df_ext[extended_cols])\n",
        "    print(f\"[Extended] Total features (baseline + trends): {len(extended_cols)} | Rows: {len(df_ext)}\")\n",
        "\n",
        "    start_idx_ext = pick_start_index(len(df_ext))\n",
        "    results_extended = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_e, m_e = walk_forward_eval(df_ext, extended_cols, mdl, start_index=start_idx_ext)\n",
        "        results_extended[name] = (out_e, m_e)\n",
        "        print(f\"  EXT  {name}: ACC={m_e['acc']:.3f} F1={m_e['f1']:.3f} AUC={m_e['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label, \"baseline + keywords\", results_extended, keywords=keep_keywords,\n",
        "                       features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    # --- DEEP LEARNING (optional): simple MLP on extended features ---\n",
        "    if use_dl:\n",
        "        print(\"[DL] Training MLP on extended features‚Ä¶\")\n",
        "        X = df_ext[extended_cols].values\n",
        "        y = df_ext[\"y_up\"].astype(int).values\n",
        "\n",
        "        n = len(df_ext)\n",
        "        i_tr, i_va = int(n * 0.70), int(n * 0.85)\n",
        "\n",
        "        X_tr, X_va, X_te = X[:i_tr], X[i_tr:i_va], X[i_va:]\n",
        "        y_tr, y_va, y_te = y[:i_tr], y[i_tr:i_va], y[i_va:]\n",
        "\n",
        "        scaler = StandardScaler().fit(X_tr)\n",
        "        X_tr_s, X_va_s, X_te_s = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "        tf.keras.utils.set_random_seed(42)\n",
        "        mlp = keras.Sequential([\n",
        "            keras.layers.Input(shape=(X_tr_s.shape[1],)),\n",
        "            keras.layers.Dense(128, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Dense(64, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ])\n",
        "        mlp.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[keras.metrics.AUC(name=\"auc\"),\n",
        "                             keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
        "\n",
        "        es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True)\n",
        "        mlp.fit(X_tr_s, y_tr, validation_data=(X_va_s, y_va), epochs=30, batch_size=64, callbacks=[es], verbose=0)\n",
        "\n",
        "        p_te = mlp.predict(X_te_s, verbose=0).ravel()\n",
        "        yhat = (p_te >= 0.5).astype(int)\n",
        "        dl_metrics = {\n",
        "            \"acc\": accuracy_score(y_te, yhat),\n",
        "            \"f1\":  f1_score(y_te, yhat),\n",
        "            \"auc\": roc_auc_score(y_te, p_te) if len(set(y_te)) > 1 else np.nan\n",
        "        }\n",
        "        print(f\"  DL MLP: ACC={dl_metrics['acc']:.3f} F1={dl_metrics['f1']:.3f} AUC={dl_metrics['auc']:.3f}\")\n",
        "\n",
        "        # Save as a TXT run with a single \"MLP\" entry\n",
        "        save_run_group_txt(label, \"DL MLP (extended)\", {\"MLP\": (None, dl_metrics)},\n",
        "                           keywords=keep_keywords, features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    print(f\"‚úÖ Finished {label}.\")"
      ],
      "metadata": {
        "id": "eaTi5-cs-B3-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 3) CONVENIENCE ‚Äî run by human-readable label\n",
        "# =====================================================\n",
        "\n",
        "def run_asset_by_label(name: str, use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Call like:\n",
        "      run_asset_by_label(\"Gold\")\n",
        "      run_asset_by_label(\"BTC\")\n",
        "      run_asset_by_label(\"Oil\")\n",
        "      run_asset_by_label(\"USDCNY\")\n",
        "    Aliases like 'gc=f', 'wti', 'cny=x' also work.\n",
        "    \"\"\"\n",
        "    canon = resolve_label(name)\n",
        "    asset = asset_by_label.get(canon)\n",
        "    if asset is None:\n",
        "        print(f\"[!] Unknown asset '{name}'. Available: {list(asset_by_label.keys())}\")\n",
        "        return\n",
        "    run_asset(asset, KW_CSV, use_dl=use_dl)\n",
        "\n",
        "print(\"‚úÖ You can now call run_asset_by_label('Gold') etc.\")"
      ],
      "metadata": {
        "id": "wLZD8qyWKU3D",
        "outputId": "4d71b4d6-47ba-4b56-9b46-e258790535f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ You can now call run_asset_by_label('Gold') etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 4) RUNS ‚Äî you control what to run today\n",
        "# =====================================================\n",
        "\n",
        "# 4.1 Run Gold end-to-end (Baseline ‚Üí Extended ‚Üí DL)\n",
        "run_asset_by_label(\"Gold\", use_dl=True)\n",
        "\n",
        "# 4.2 Run BTC\n",
        "# run_asset_by_label(\"BTC\", use_dl=True)\n",
        "\n",
        "# 4.3 Run Oil\n",
        "# run_asset_by_label(\"Oil\", use_dl=True)\n",
        "\n",
        "# 4.4 Run USD/CNY\n",
        "# run_asset_by_label(\"USDCNY\", use_dl=True)\n",
        "\n",
        "# TIP: Uncomment only what you want to run this session.\n",
        "# All TXT outputs will appear under:\n",
        "#   /content/drive/MyDrive/gt-markets/outputs/"
      ],
      "metadata": {
        "id": "nbnCif16Khqm",
        "outputId": "c194dae3-eace-4b07-baed-72c95dedda38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "‚ñ∂Ô∏è  Running Gold (Pair=GC=F)\n",
            "==============================\n",
            "[Data] Range: 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.503 F1=0.480 AUC=0.492\n"
          ]
        }
      ]
    }
  ]
}