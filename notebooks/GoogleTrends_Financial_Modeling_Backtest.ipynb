{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyN2omRLxEtUqxJiOHRo0IoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/main/notebooks/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "d65884ce-510a-4b83-e3d0-7730db82531a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Libraries loaded.\n",
            "‚úÖ Setup complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =====================================================\n",
        "# 1) SETUP  ‚Äî  run once per Colab session\n",
        "# =====================================================\n",
        "\n",
        "# --- 1.1 Mount Google Drive & project paths ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "PROJECT_DIR = Path(\"/content/drive/MyDrive/gt-markets\")\n",
        "DATA_DIR    = PROJECT_DIR / \"data\" / \"processed\"\n",
        "KW_DIR      = PROJECT_DIR / \"data\" / \"Keyword Selection\"\n",
        "OUT_DIR     = PROJECT_DIR / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 1.2 Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "!pip -q install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"‚úÖ Libraries loaded.\")\n",
        "\n",
        "\n",
        "# --- 1.3 Data files (RAW vs ENGINEERED) ---\n",
        "# (Update filenames here if your dates change)\n",
        "RAW_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"\n",
        "ENG_FILE = DATA_DIR / \"merged_financial_trends_engineered_2025-09-07.csv\"\n",
        "KW_CSV   = KW_DIR   / \"combined_significant_lagged_correlations.csv\"\n",
        "\n",
        "assert RAW_FILE.exists(), f\"Missing RAW dataset: {RAW_FILE}\"\n",
        "assert ENG_FILE.exists(), f\"Missing ENGINEERED dataset: {ENG_FILE}\"\n",
        "assert KW_CSV.exists(),  f\"Missing keyword file: {KW_CSV}\"\n",
        "\n",
        "DATASETS = {\"raw\": RAW_FILE, \"engineered\": ENG_FILE}\n",
        "\n",
        "def get_dataset_path(dataset_version: str) -> Path:\n",
        "    \"\"\"Resolve dataset path by version: 'raw' or 'engineered'.\"\"\"\n",
        "    key = (dataset_version or \"raw\").strip().lower()\n",
        "    if key not in DATASETS:\n",
        "        raise ValueError(f\"Unknown dataset_version '{dataset_version}'. Use one of {list(DATASETS.keys())}.\")\n",
        "    return DATASETS[key]\n",
        "\n",
        "def tag_label(label: str, dataset_version: str) -> str:\n",
        "    \"\"\"Produce human label with dataset tag, e.g. 'Gold [raw]'.\"\"\"\n",
        "    return f\"{label} [{dataset_version}]\"\n",
        "\n",
        "\n",
        "# --- 1.4 Asset registry (no magic numbers) ---\n",
        "# Pair IDs must match the \"Pair\" column in James's keyword CSV.\n",
        "assets = [\n",
        "    {\"PAIR_ID\": \"GC=F\",    \"price_col\": \"GC=F Close\",    \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\", \"price_col\": \"BTC-USD Close\", \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",    \"price_col\": \"CL=F Close\",    \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"CNY=X\",   \"price_col\": \"CNY=X Close\",   \"label\": \"USDCNY\"},\n",
        "]\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in assets}\n",
        "asset_alias = {\n",
        "    \"gold\":\"gold\",\"gc=f\":\"gold\",\"xau\":\"gold\",\"xauusd\":\"gold\",\n",
        "    \"btc\":\"btc\",\"bitcoin\":\"btc\",\"btc-usd\":\"btc\",\n",
        "    \"oil\":\"oil\",\"cl=f\":\"oil\",\"wti\":\"oil\",\n",
        "    \"usdcny\":\"usdcny\",\"cny=x\":\"usdcny\",\"cny\":\"usdcny\",\n",
        "}\n",
        "\n",
        "def resolve_label(name: str) -> str:\n",
        "    key = (name or \"\").strip().lower()\n",
        "    return asset_alias.get(key, key)  # normalize alias ‚Üí canonical\n",
        "\n",
        "# --- 1.5 Utilities: target, features, eval, saving ---\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add 1-day return and classification target y_up (whether next day is up).\n",
        "    Drops rows with NaN so downstream code is clean.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"ret1\"] = df[price_col].pct_change()\n",
        "    df[\"y_up\"] = (df[\"ret1\"].shift(-1) > 0).astype(int)\n",
        "    return df.dropna().copy()\n",
        "\n",
        "def pick_start_index(n_rows: int, floor: int = 60, frac: float = 0.30, cap: int = 500) -> int:\n",
        "    \"\"\"\n",
        "    Choose a walk-forward starting index:\n",
        "    - at least 'floor' rows,\n",
        "    - about 'frac' of data as initial train window,\n",
        "    - but never exceed 'cap'.\n",
        "    \"\"\"\n",
        "    return max(floor, min(cap, int(n_rows * frac)))\n",
        "\n",
        "def walk_forward_eval(df_in: pd.DataFrame, feature_cols, model, start_index: int = 500):\n",
        "    \"\"\"\n",
        "    Expanding-window walk-forward:\n",
        "      train on [0:i) ‚Üí predict on [i]\n",
        "    IMPORTANT: Scale using train only ‚Üí transform test (no leakage).\n",
        "    Returns (pred_df, metrics_dict).\n",
        "    \"\"\"\n",
        "    df_in = df_in.copy()\n",
        "    X_all = df_in[feature_cols].values\n",
        "    y_all = df_in[\"y_up\"].values\n",
        "    idxs  = df_in.index\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "    preds, probs, trues, dates = [], [], [], []\n",
        "\n",
        "    for i in range(start_index, len(df_in)):\n",
        "        X_train, y_train = X_all[:i], y_all[:i]\n",
        "        X_test,  y_test  = X_all[i:i+1], y_all[i]\n",
        "\n",
        "        X_train_s = scaler.fit_transform(X_train)\n",
        "        X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_s, y_train)\n",
        "        p    = model.predict_proba(X_test_s)[0, 1]\n",
        "        yhat = int(p >= 0.5)\n",
        "\n",
        "        preds.append(yhat); probs.append(p); trues.append(int(y_test)); dates.append(idxs[i])\n",
        "\n",
        "    out = pd.DataFrame({\"date\": dates, \"y_true\": trues, \"y_pred\": preds, \"prob_up\": probs}).set_index(\"date\")\n",
        "    acc = accuracy_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    f1  = f1_score(out[\"y_true\"], out[\"y_pred\"])\n",
        "    try:\n",
        "        auc = roc_auc_score(out[\"y_true\"], out[\"prob_up\"])\n",
        "    except Exception:\n",
        "        auc = np.nan\n",
        "    return out, {\"acc\": acc, \"f1\": f1, \"auc\": auc}\n",
        "\n",
        "def save_run_group_txt(pair_label: str, run_label: str, results_dict: dict,\n",
        "                       keywords: list, features_count: int, rows_used: int, note: str = \"\"):\n",
        "    \"\"\"\n",
        "    Write ONE TXT per run containing:\n",
        "      - asset label + run label (+ timestamp)\n",
        "      - features/rows info\n",
        "      - keyword list (or 'None')\n",
        "      - metrics for ALL models in this run (LR/RF/XGB) or DL summary\n",
        "    \"\"\"\n",
        "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    safe_pair = pair_label.replace(\" \", \"_\").lower()\n",
        "    safe_run  = run_label.replace(\" \", \"_\").replace(\"+\", \"plus\").lower()\n",
        "    out_path  = OUT_DIR / f\"{safe_pair}_{safe_run}_{ts}.txt\"\n",
        "\n",
        "    with open(out_path, \"w\") as f:\n",
        "        f.write(f\"{pair_label} ‚Äî {run_label}\\n\")\n",
        "        f.write(f\"Timestamp: {ts}\\n\")\n",
        "        f.write(f\"Features used: {features_count}\\n\")\n",
        "        f.write(f\"Rows used: {rows_used}\\n\")\n",
        "        if note:\n",
        "            f.write(f\"Note: {note}\\n\")\n",
        "\n",
        "        f.write(\"\\nKeywords used:\\n\")\n",
        "        if keywords:\n",
        "            for k in keywords: f.write(f\"- {k}\\n\")\n",
        "        else:\n",
        "            f.write(\"- None\\n\")\n",
        "\n",
        "        f.write(\"\\nResults (all models):\\n\")\n",
        "        for model_name, (_out, met) in results_dict.items():\n",
        "            acc = met.get(\"acc\", float(\"nan\"))\n",
        "            f1  = met.get(\"f1\",  float(\"nan\"))\n",
        "            auc = met.get(\"auc\", float(\"nan\"))\n",
        "            f.write(f\"{model_name}: ACC={acc:.3f}, F1={f1:.3f}, AUC={auc:.3f}\\n\")\n",
        "\n",
        "    print(\"üìù Saved:\", out_path)\n",
        "\n",
        "def sanitize_features(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Defensive feature cleaning:\n",
        "    - replace ¬±inf with NaN\n",
        "    - fill NaN with 0 (only for derived columns we add)\n",
        "    - clip extreme outliers to 0.1%/99.9% quantiles\n",
        "    \"\"\"\n",
        "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    lo = X.quantile(0.001)\n",
        "    hi = X.quantile(0.999)\n",
        "    return X.clip(lower=lo, upper=hi, axis=1)\n",
        "\n",
        "def build_trend_lag_features(frame: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    From *_trend level series, create leak-safe daily change features:\n",
        "      - 1-day % change, lagged by 1 day\n",
        "      - 7-day % change, lagged by 1 day\n",
        "    \"\"\"\n",
        "    feats = {}\n",
        "    for c in cols:\n",
        "        chg1 = frame[c].pct_change()\n",
        "        chg7 = frame[c].pct_change(7)\n",
        "        feats[c+\"_chg1_lag1\"] = chg1.shift(1)\n",
        "        feats[c+\"_chg7_lag1\"] = chg7.shift(1)\n",
        "    return pd.DataFrame(feats, index=frame.index)\n",
        "\n",
        "def map_keywords_to_trend_cols(df_cols: list, keywords: list) -> list:\n",
        "    \"\"\"\n",
        "    Map keyword strings (from James's CSV) to *_trend columns present in our dataset.\n",
        "    Matching is normalized to tolerate underscores/symbols.\n",
        "    \"\"\"\n",
        "    trend_cols_all = [c for c in df_cols if str(c).endswith(\"_trend\")]\n",
        "    def _norm(s): return re.sub(r\"[^a-z0-9]+\", \"\", str(s).lower())\n",
        "    kw_to_col = []\n",
        "    for kw in keywords:\n",
        "        n = _norm(kw)\n",
        "        hits = [c for c in trend_cols_all if n in _norm(c.replace(\"_trend\",\"\"))]\n",
        "        if hits:\n",
        "            kw_to_col.append(hits[0])  # first reasonable match\n",
        "    # de-dup while preserving order\n",
        "    seen, ordered = set(), []\n",
        "    for c in kw_to_col:\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list:\n",
        "    \"\"\"Load keywords from James's CSV for a given Pair ID (e.g., 'GC=F').\"\"\"\n",
        "    dfk = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\",\"Keyword\"}.issubset(dfk.columns), f\"Unexpected keyword CSV columns: {dfk.columns}\"\n",
        "    kw = (dfk.loc[dfk[\"Pair\"] == pair_id, \"Keyword\"]\n",
        "            .dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "    return kw\n",
        "\n",
        "\n",
        "# --- 1.6 Models (re-used across assets) ---\n",
        "models = {\n",
        "    \"LR\":  LogisticRegression(max_iter=500),\n",
        "    \"RF\":  RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "    \"XGB\": XGBClassifier(\n",
        "        n_estimators=500, max_depth=4, learning_rate=0.05,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        tree_method=\"hist\", random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 2) RUNNER ‚Äî Baseline ‚Üí Extended ‚Üí (DL) for one asset\n",
        "# =====================================================\n",
        "\n",
        "def run_asset(asset: dict, dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Full pipeline for a single asset + chosen dataset:\n",
        "      dataset_version ‚àà {'raw','engineered'}\n",
        "      1) Baseline: technical features only (no *_trend)\n",
        "      2) Extended: baseline + lagged trend-change features for this asset's keywords\n",
        "      3) Deep Learning: MLP on extended features (optional)\n",
        "    Produces one TXT per run with results for ALL models.\n",
        "    \"\"\"\n",
        "    label       = asset[\"label\"]\n",
        "    pair_id     = asset[\"PAIR_ID\"]\n",
        "    price_col   = asset[\"price_col\"]\n",
        "\n",
        "    ds_path     = get_dataset_path(dataset_version)\n",
        "    label_tag   = tag_label(label, dataset_version)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"‚ñ∂Ô∏è  Running {label_tag}  |  Pair: {pair_id}\")\n",
        "    print(f\"    Dataset: {ds_path.name}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # --- Load & target ---\n",
        "    df = pd.read_csv(ds_path, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    df_mod = make_target(df, price_col)\n",
        "    print(f\"[Data] {df_mod.index.min().date()} ‚Üí {df_mod.index.max().date()} | Rows: {len(df_mod)}\")\n",
        "\n",
        "    # --- BASELINE: technical-only (exclude *_trend) ---\n",
        "    all_trends = [c for c in df_mod.columns if str(c).endswith(\"_trend\")]\n",
        "    exclude    = {price_col, \"ret1\", \"y_up\"} | set(all_trends)\n",
        "    numeric    = [c for c in df_mod.columns if df_mod[c].dtype != \"O\"]\n",
        "    baseline_cols = [c for c in numeric if c not in exclude]\n",
        "    print(f\"[Baseline] Features: {len(baseline_cols)}\")\n",
        "\n",
        "    start_idx = pick_start_index(len(df_mod))\n",
        "    results_baseline = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_b, m_b = walk_forward_eval(df_mod, baseline_cols, mdl, start_index=start_idx)\n",
        "        results_baseline[name] = (out_b, m_b)\n",
        "        print(f\"  BASE {name}: ACC={m_b['acc']:.3f} F1={m_b['f1']:.3f} AUC={m_b['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label_tag, \"baseline\", results_baseline,\n",
        "                       keywords=[], features_count=len(baseline_cols), rows_used=len(df_mod))\n",
        "\n",
        "    # --- EXTENDED: baseline + (asset) keywords as lagged changes ---\n",
        "    keep_keywords = load_keywords_for_pair(KW_CSV, pair_id)\n",
        "    print(f\"[Extended] Keywords for {pair_id}: {len(keep_keywords)}\")\n",
        "    sel_trend_cols = map_keywords_to_trend_cols(df_mod.columns, keep_keywords)\n",
        "\n",
        "    if not sel_trend_cols:\n",
        "        print(\"[WARN] No *_trend columns matched these keywords. Skipping extended & DL.\")\n",
        "        return\n",
        "\n",
        "    lag_feats = build_trend_lag_features(df_mod, sel_trend_cols)\n",
        "    # Drop sparse derived cols (>50% NaN), then warmup-trim (pct_change(7)+shift)\n",
        "    nan_ratio = lag_feats.isna().mean()\n",
        "    kept_cols = nan_ratio[nan_ratio < 0.50].index.tolist()\n",
        "    lag_feats = lag_feats[kept_cols]\n",
        "    WARMUP = 8\n",
        "    df_ext = df_mod.join(lag_feats).iloc[WARMUP:].copy()\n",
        "    df_ext[kept_cols] = df_ext[kept_cols].fillna(0.0)\n",
        "\n",
        "    extended_cols = sorted(set(baseline_cols).union(kept_cols))\n",
        "    df_ext[extended_cols] = sanitize_features(df_ext[extended_cols])\n",
        "    print(f\"[Extended] Features: {len(extended_cols)} | Rows: {len(df_ext)}\")\n",
        "\n",
        "    start_idx_ext = pick_start_index(len(df_ext))\n",
        "    results_extended = {}\n",
        "    for name, mdl in models.items():\n",
        "        out_e, m_e = walk_forward_eval(df_ext, extended_cols, mdl, start_index=start_idx_ext)\n",
        "        results_extended[name] = (out_e, m_e)\n",
        "        print(f\"  EXT  {name}: ACC={m_e['acc']:.3f} F1={m_e['f1']:.3f} AUC={m_e['auc']:.3f}\")\n",
        "\n",
        "    save_run_group_txt(label_tag, \"baseline + keywords\", results_extended,\n",
        "                       keywords=keep_keywords, features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    # --- DL (optional): MLP on extended features ---\n",
        "    if use_dl:\n",
        "        print(\"[DL] MLP on extended features‚Ä¶\")\n",
        "        X = df_ext[extended_cols].values\n",
        "        y = df_ext[\"y_up\"].astype(int).values\n",
        "\n",
        "        n      = len(df_ext)\n",
        "        i_tr   = int(n * 0.70)\n",
        "        i_va   = int(n * 0.85)\n",
        "        X_tr, X_va, X_te = X[:i_tr], X[i_tr:i_va], X[i_va:]\n",
        "        y_tr, y_va, y_te = y[:i_tr], y[i_tr:i_va], y[i_va:]\n",
        "\n",
        "        scaler = StandardScaler().fit(X_tr)\n",
        "        X_tr, X_va, X_te = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "        tf.keras.utils.set_random_seed(42)\n",
        "        mlp = keras.Sequential([\n",
        "            keras.layers.Input(shape=(X_tr.shape[1],)),\n",
        "            keras.layers.Dense(128, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.30),\n",
        "            keras.layers.Dense(64, activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dropout(0.30),\n",
        "            keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "        ])\n",
        "        mlp.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                    loss=\"binary_crossentropy\",\n",
        "                    metrics=[keras.metrics.AUC(name=\"auc\"),\n",
        "                             keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
        "\n",
        "        es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\",\n",
        "                                           patience=5, restore_best_weights=True)\n",
        "        mlp.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
        "                epochs=30, batch_size=64, callbacks=[es], verbose=0)\n",
        "\n",
        "        p_te = mlp.predict(X_te, verbose=0).ravel()\n",
        "        yhat = (p_te >= 0.5).astype(int)\n",
        "        dl_metrics = {\n",
        "            \"acc\": accuracy_score(y_te, yhat),\n",
        "            \"f1\":  f1_score(y_te, yhat),\n",
        "            \"auc\": roc_auc_score(y_te, p_te) if len(set(y_te)) > 1 else np.nan\n",
        "        }\n",
        "        print(f\"  DL MLP: ACC={dl_metrics['acc']:.3f} F1={dl_metrics['f1']:.3f} AUC={dl_metrics['auc']:.3f}\")\n",
        "\n",
        "        save_run_group_txt(label_tag, \"DL MLP (extended)\",\n",
        "                           {\"MLP\": (None, dl_metrics)},\n",
        "                           keywords=keep_keywords, features_count=len(extended_cols), rows_used=len(df_ext))\n",
        "\n",
        "    print(f\"‚úÖ Finished {label_tag}.\")"
      ],
      "metadata": {
        "id": "eaTi5-cs-B3-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 3) CONVENIENCE  ‚Äî run by readable label\n",
        "# =====================================================\n",
        "\n",
        "def run_asset_by_label(name: str, dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"\n",
        "    Call like:\n",
        "      run_asset_by_label(\"Gold\", dataset_version=\"raw\")\n",
        "      run_asset_by_label(\"BTC\",  dataset_version=\"engineered\")\n",
        "    Aliases like 'gc=f', 'wti', 'cny=x' also work.\n",
        "    \"\"\"\n",
        "    canon = resolve_label(name)\n",
        "    asset = asset_by_label.get(canon)\n",
        "    if asset is None:\n",
        "        print(f\"[!] Unknown asset '{name}'. Available: {list(asset_by_label.keys())}\")\n",
        "        return\n",
        "    run_asset(asset, dataset_version=dataset_version, use_dl=use_dl)\n",
        "\n",
        "def run_pair_raw_and_engineered(name: str, use_dl: bool = True):\n",
        "    \"\"\"Run the same asset twice: RAW then ENGINEERED.\"\"\"\n",
        "    run_asset_by_label(name, dataset_version=\"raw\",         use_dl=use_dl)\n",
        "    run_asset_by_label(name, dataset_version=\"engineered\",  use_dl=use_dl)\n",
        "\n",
        "def run_all_pairs(dataset_version: str = \"raw\", use_dl: bool = True):\n",
        "    \"\"\"Loop all assets for a chosen dataset_version ('raw' or 'engineered').\"\"\"\n",
        "    for lbl in asset_by_label.keys():\n",
        "        run_asset_by_label(lbl, dataset_version=dataset_version, use_dl=use_dl)\n",
        "\n",
        "print(\"‚úÖ Ready: use run_asset_by_label('Gold', dataset_version='raw') etc.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZD8qyWKU3D",
        "outputId": "62f0853c-036b-4d2c-9692-390c2cb5f5c6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ready: use run_asset_by_label('Gold', dataset_version='raw') etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =====================================================\n",
        "# 4) RUNS  ‚Äî choose what to execute\n",
        "# =====================================================\n",
        "\n",
        "# Examples (uncomment what you need):\n",
        "\n",
        "# 4.1 Gold only (RAW)\n",
        "run_asset_by_label(\"Gold\", dataset_version=\"raw\", use_dl=True)\n",
        "\n",
        "# 4.2 Gold only (ENGINEERED)\n",
        "# run_asset_by_label(\"Gold\", dataset_version=\"engineered\", use_dl=True)\n",
        "\n",
        "# 4.3 Gold on BOTH datasets\n",
        "# run_pair_raw_and_engineered(\"Gold\", use_dl=True)\n",
        "\n",
        "# 4.4 All pairs on RAW (7 runs per pair)\n",
        "# run_all_pairs(dataset_version=\"raw\", use_dl=True)\n",
        "\n",
        "# 4.5 All pairs on ENGINEERED (another 7 runs per pair)\n",
        "# run_all_pairs(dataset_version=\"engineered\", use_dl=True)\n",
        "\n",
        "# TXT outputs appear in:\n",
        "#   /content/drive/MyDrive/gt-markets/outputs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbnCif16Khqm",
        "outputId": "78a76a6c-651f-4d42-bc2c-797b7807313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "‚ñ∂Ô∏è  Running Gold [raw]  |  Pair: GC=F\n",
            "    Dataset: merged_financial_trends_data_2025-09-07.csv\n",
            "========================================\n",
            "[Data] 2016-03-08 ‚Üí 2020-05-18 | Rows: 985\n",
            "[Baseline] Features: 40\n",
            "  BASE LR: ACC=0.503 F1=0.480 AUC=0.492\n"
          ]
        }
      ]
    }
  ]
}