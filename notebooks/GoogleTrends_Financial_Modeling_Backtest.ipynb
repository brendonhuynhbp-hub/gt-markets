{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyP+bR/zthDe4xjIRcs087xw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/update-2.1/notebooks/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "6fe57512-bf38-4386-a18f-ae501bd48bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "PROJECT_DIR: /content/drive/MyDrive/gt-markets\n",
            "DATA_DIR:    /content/drive/MyDrive/gt-markets/data/processed\n",
            "KW_DIR:      /content/drive/MyDrive/gt-markets/data/Keyword Selection\n",
            "OUT_DIR:     /content/drive/MyDrive/gt-markets/outputs\n",
            "TF on CPU\n",
            "TF version: 2.19.0\n",
            "merged_financial_trends_data_2025-09-07.csv: filling NaNs in trend cols with 0: ['entrepreneurial_trend', 'cryptocurrency_trend'] \n",
            "RAW frame: 2015-09-08 → 2025-09-05 | rows: 2609\n",
            "ENG frame: 2015-09-08 → 2025-09-05 | rows: 2609\n",
            "=== RUN CONFIG ===\n",
            "FREQ: D | DEBUG: False | RUN_ID: d_prod_20250914-055550\n",
            "OUTPUT ROOT: /content/drive/MyDrive/gt-markets/outputs/runs/d_prod_20250914-055550\n",
            "RUN_MODE: all\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# GoogleTrends Financial Modeling — End-to-End Backtest\n",
        "# - Mount Drive, paths, data load/resample\n",
        "# - Feature engineering + keyword lags\n",
        "# - Leak guard + NaN safety\n",
        "# - ML (LR/RF/XGB) + DL (MLP/LSTM/GRU) with toggles\n",
        "# - Rolling walk-forward (ML & DL) + Platt calibration\n",
        "# - Backtest, logging, figs, leaderboards\n",
        "# =========================================================\n",
        "\n",
        "# -------- Global switches --------\n",
        "DEBUG          = False       # True → quick run; False → full\n",
        "FREQ           = \"D\"        # \"D\" or \"W\"\n",
        "RUN_MODE       = \"all\"      # \"all\" or \"single\"\n",
        "PAIR_KEY       = \"gold\"     # used when RUN_MODE == \"single\"\n",
        "\n",
        "# Rolling\n",
        "USE_ROLLING_ML = True\n",
        "USE_ROLLING_DL = True\n",
        "MAX_FOLDS      = 5\n",
        "ROLL_STEP      = \"test\"     # \"test\" or integer step\n",
        "DL_WINDOW      = 30         # sequence length for DL\n",
        "\n",
        "# Calibration\n",
        "CALIBRATE_PROBS = True\n",
        "\n",
        "# Model toggles (ML)\n",
        "ENABLE_LR   = True\n",
        "ENABLE_RF   = True\n",
        "ENABLE_XGB  = True   # set False if xgboost unavailable\n",
        "\n",
        "# Model toggles (DL)\n",
        "ENABLE_MLP  = True   # MLP over flattened window\n",
        "ENABLE_LSTM = True\n",
        "ENABLE_GRU  = True\n",
        "\n",
        "# -------- Drive + paths --------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import os, warnings, random, re, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CANDIDATE_PROJECT_DIRS = [\n",
        "    Path(\"/content/drive/MyDrive/gt-markets\"),\n",
        "    Path(\"/content/drive/Shareddrives/gt-markets\"),\n",
        "]\n",
        "PROJECT_DIR = next((p for p in CANDIDATE_PROJECT_DIRS if p.exists()), None)\n",
        "assert PROJECT_DIR is not None, \"Project directory not found.\"\n",
        "\n",
        "DATA_DIR = PROJECT_DIR / \"data\" / \"processed\"\n",
        "KW_DIR   = PROJECT_DIR / \"data\" / \"Keyword Selection\"\n",
        "OUT_DIR  = PROJECT_DIR / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"\n",
        "ENG_FILE = DATA_DIR / \"merged_financial_trends_engineered_2025-09-07.csv\"\n",
        "KW_CSV   = KW_DIR  / \"combined_significant_lagged_correlations.csv\"\n",
        "assert RAW_FILE.exists() and ENG_FILE.exists() and KW_CSV.exists(), \"Missing data files.\"\n",
        "\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"DATA_DIR:   \", DATA_DIR)\n",
        "print(\"KW_DIR:     \", KW_DIR)\n",
        "print(\"OUT_DIR:    \", OUT_DIR)\n",
        "\n",
        "# -------- Repro + warnings --------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------- ML/DL stack --------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    print(f\"TF GPU devices: {len(gpus)} (mem growth on)\" if gpus else \"TF on CPU\")\n",
        "except Exception as e:\n",
        "    print(\"TF GPU setup note:\", e)\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAVE_XGB = True\n",
        "except Exception:\n",
        "    HAVE_XGB = False\n",
        "\n",
        "# -------- Assets --------\n",
        "ASSETS = [\n",
        "    {\"PAIR_ID\": \"GC=F\",     \"price_col\": \"GC=F Close\",     \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\",  \"price_col\": \"BTC-USD Close\",  \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",     \"price_col\": \"CL=F Close\",     \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"USDCNY=X\", \"price_col\": \"USDCNY=X Close\", \"label\": \"USDCNY\"},\n",
        "]\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in ASSETS}\n",
        "\n",
        "# -------- Load + trend NaN fill --------\n",
        "def _load_and_clean(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    trend_cols = [c for c in df.columns if \"trend\" in c.lower()]\n",
        "    if trend_cols:\n",
        "        bad = [c for c in trend_cols if df[c].isna().any()]\n",
        "        if bad:\n",
        "            print(f\"{path.name}: filling NaNs in trend cols with 0:\", bad[:10], \"...\" if len(bad)>10 else \"\")\n",
        "            df[bad] = df[bad].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "df_raw0 = _load_and_clean(RAW_FILE)\n",
        "df_eng0 = _load_and_clean(ENG_FILE)\n",
        "if DEBUG:\n",
        "    df_raw0 = df_raw0.tail(1000)\n",
        "    df_eng0 = df_eng0.tail(1000)\n",
        "    print(\"DEBUG mode: using last 1000 rows (RAW/ENG).\")\n",
        "\n",
        "# -------- Frequency control (D/W) --------\n",
        "def to_frequency(df_in: pd.DataFrame, freq: str = \"D\") -> pd.DataFrame:\n",
        "    if freq.upper() == \"D\": return df_in.copy()\n",
        "    assert freq.upper() == \"W\"\n",
        "    out = pd.DataFrame(index=df_in.resample(\"W\").last().index)\n",
        "    # prices as last-of-week; non-price as mean-of-week\n",
        "    price_cols = {a[\"price_col\"] for a in ASSETS}\n",
        "    for a in ASSETS:\n",
        "        if a[\"price_col\"] in df_in.columns:\n",
        "            out[a[\"price_col\"]] = df_in[a[\"price_col\"]].resample(\"W\").last()\n",
        "    for c in df_in.columns:\n",
        "        if c not in price_cols:\n",
        "            out[c] = df_in[c].resample(\"W\").mean()\n",
        "    return out\n",
        "\n",
        "df_raw = to_frequency(df_raw0, FREQ)\n",
        "df_eng = to_frequency(df_eng0, FREQ)\n",
        "print(\"RAW frame:\", df_raw.index.min().date(), \"→\", df_raw.index.max().date(), \"| rows:\", len(df_raw))\n",
        "print(\"ENG frame:\", df_eng.index.min().date(), \"→\", df_eng.index.max().date(), \"| rows:\", len(df_eng))\n",
        "\n",
        "# -------- Keywords utilities --------\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list[str]:\n",
        "    d = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\",\"Keyword\"}.issubset(d.columns)\n",
        "    aliases = {pair_id}\n",
        "    if pair_id == \"USDCNY=X\": aliases |= {\"CNY=X\"}\n",
        "    if pair_id == \"CNY=X\":    aliases |= {\"USDCNY=X\"}\n",
        "    return (d.loc[d[\"Pair\"].isin(aliases), \"Keyword\"]\n",
        "              .dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "\n",
        "def map_keywords_to_trend_cols(all_cols: pd.Index, keywords: list[str]) -> list[str]:\n",
        "    norm = lambda s: str(s).lower().strip().replace(\" \", \"_\")\n",
        "    desired = {f\"{norm(k)}_trend\" for k in keywords}\n",
        "    return [c for c in all_cols if str(c).lower() in desired]\n",
        "\n",
        "# -------- Targets + %chg lags --------\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"ret1\"] = out[price_col].pct_change()\n",
        "    out[\"y_up\"] = (out[price_col].shift(-1) > out[price_col]).astype(int)\n",
        "    return out.dropna(subset=[price_col]).dropna()\n",
        "\n",
        "def build_trend_lag_features(df_in: pd.DataFrame, sel_cols: list[str], lag_steps=(7,14,21)) -> pd.DataFrame:\n",
        "    out = {}\n",
        "    for c in sel_cols:\n",
        "        s = df_in[c].astype(float)\n",
        "        for L in lag_steps:\n",
        "            chg = s.pct_change(L).shift(1)\n",
        "            chg = chg.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(-10.0, 10.0)\n",
        "            out[f\"{c}__chg{L}\"] = chg\n",
        "    return pd.DataFrame(out, index=df_in.index)\n",
        "\n",
        "# -------- Versioned outputs --------\n",
        "from datetime import datetime\n",
        "RUN_MODE_TAG = \"debug\" if DEBUG else \"prod\"\n",
        "RUN_STAMP    = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_ID       = f\"{FREQ.lower()}_{RUN_MODE_TAG}_{RUN_STAMP}\"\n",
        "\n",
        "RUN_ROOT = OUT_DIR / \"runs\" / RUN_ID\n",
        "STAGES = {\n",
        "    \"data\":        RUN_ROOT / \"00_data\",\n",
        "    \"logs\":        RUN_ROOT / \"10_logs\",\n",
        "    \"preds_val\":   RUN_ROOT / \"20_preds\" / \"val\",\n",
        "    \"preds_test\":  RUN_ROOT / \"20_preds\" / \"test\",\n",
        "    \"backtests\":   RUN_ROOT / \"30_backtests\",\n",
        "    \"figs\":        RUN_ROOT / \"40_figs\",\n",
        "    \"leaderboard\": RUN_ROOT / \"50_leaderboards\",\n",
        "}\n",
        "for p in STAGES.values(): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(df_raw.head(1)\n",
        " .assign(_start=df_raw.index.min(), _end=df_raw.index.max())\n",
        " .to_csv(STAGES[\"data\"] / f\"dataset_snapshot_{RAW_FILE.name}.head1.csv\"))\n",
        "\n",
        "def _slug(s: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"_\", s.lower()).strip(\"_\")\n",
        "\n",
        "def _pred_name(asset_label: str, dataset_tag: str, model: str, *, window: int | None = None, split=\"test\") -> str:\n",
        "    bits = [_slug(asset_label), dataset_tag, model.lower()]\n",
        "    if window: bits.append(f\"w{window}\")\n",
        "    return \".\".join([\"_\".join(bits), split, \"csv\"])\n",
        "\n",
        "def _log_name(asset_label: str, dataset_tag: str, run_name: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{dataset_tag}_{_slug(run_name)}.{RUN_STAMP}.txt\"\n",
        "\n",
        "def _fig_name(asset_label: str, tail: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{tail}.png\"\n",
        "\n",
        "def save_txt_log(asset_label: str, dataset_tag: str, run_name: str, lines: list[str]):\n",
        "    path = STAGES[\"logs\"] / _log_name(asset_label, dataset_tag, run_name)\n",
        "    with open(path, \"w\") as f: f.write(\"\\n\".join(lines))\n",
        "    return path\n",
        "\n",
        "def save_preds_df(df_pred: pd.DataFrame, asset_label: str, dataset_tag: str, model: str, *, window: int | None, split: str):\n",
        "    stage = STAGES[\"preds_val\"] if str(split).startswith(\"val\") else STAGES[\"preds_test\"]\n",
        "    outp = stage / _pred_name(asset_label, dataset_tag, model, window=window, split=split)\n",
        "    df_pred.to_csv(outp); return outp\n",
        "\n",
        "def save_leaderboard(df_leader: pd.DataFrame, tag: str = \"metrics\"):\n",
        "    path = STAGES[\"leaderboard\"] / f\"leaderboard_{tag}.csv\"\n",
        "    df_leader.to_csv(path, index=False); return path\n",
        "\n",
        "def save_figure(fig, asset_label: str, tail: str):\n",
        "    path = STAGES[\"figs\"] / _fig_name(asset_label, tail)\n",
        "    fig.savefig(path, dpi=150, bbox_inches=\"tight\"); plt.close(fig); return path\n",
        "\n",
        "print(\"=== RUN CONFIG ===\")\n",
        "print(f\"FREQ: {FREQ} | DEBUG: {DEBUG} | RUN_ID: {RUN_ID}\")\n",
        "print(\"OUTPUT ROOT:\", RUN_ROOT)\n",
        "print(\"RUN_MODE:\", RUN_MODE)\n",
        "print()\n",
        "\n",
        "# -------- Folds (purged+embargoed) --------\n",
        "from collections import namedtuple\n",
        "Fold = namedtuple(\"Fold\", [\"fold_id\", \"tr\", \"va\", \"te\"])\n",
        "\n",
        "def time_series_walk_forward_folds(\n",
        "    n_samples: int,\n",
        "    train_frac: float = 0.70,\n",
        "    valid_frac: float = 0.15,\n",
        "    test_frac:  float = 0.15,\n",
        "    embargo: int = 5,\n",
        "    step: str | int = \"test\",\n",
        "    min_train: int = 100,\n",
        "    max_folds: int | None = None\n",
        "):\n",
        "    assert abs(train_frac + valid_frac + test_frac - 1.0) < 1e-6\n",
        "    folds = []\n",
        "    tr_w = math.floor(n_samples * train_frac)\n",
        "    va_w = math.floor(n_samples * valid_frac)\n",
        "    te_w = n_samples - tr_w - va_w\n",
        "    step_sz = te_w if step == \"test\" else max(1, int(step))\n",
        "    end = tr_w + va_w + te_w\n",
        "    fid = 0\n",
        "    while end <= n_samples:\n",
        "        tr_end = tr_w\n",
        "        va_end = tr_w + va_w\n",
        "        te_end = tr_w + va_w + te_w\n",
        "        tr = slice(0, max(tr_end - embargo, 0))\n",
        "        va = slice(min(tr_end + embargo, n_samples), max(va_end - embargo, 0))\n",
        "        te = slice(min(va_end + embargo, n_samples), min(te_end, n_samples))\n",
        "        if (tr.stop - tr.start) >= max(1, min_train) and \\\n",
        "           (va.stop - va.start) >= 5 and \\\n",
        "           (te.stop - te.start) >= 5 and \\\n",
        "           (tr.stop <= va.start) and (va.stop <= te.start):\n",
        "            folds.append(Fold(fid, tr, va, te))\n",
        "        tr_w += step_sz; va_w += step_sz; te_w += step_sz\n",
        "        end = tr_w + va_w + te_w; fid += 1\n",
        "        if max_folds is not None and len(folds) >= max_folds: break\n",
        "    return folds\n",
        "\n",
        "# -------- Calibration (Platt) --------\n",
        "def platt_calibrate_on_val(p_val: np.ndarray, y_val: np.ndarray):\n",
        "    if not CALIBRATE_PROBS or len(y_val) < 20 or len(np.unique(y_val)) < 2:\n",
        "        return None\n",
        "    try:\n",
        "        cal = LogisticRegression(max_iter=250, solver=\"lbfgs\")\n",
        "        cal.fit(p_val.reshape(-1, 1), y_val.astype(int))\n",
        "        return cal\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def apply_calibration(cal, p: np.ndarray) -> np.ndarray:\n",
        "    if cal is None: return p\n",
        "    try:\n",
        "        return cal.predict_proba(p.reshape(-1, 1))[:, 1]\n",
        "    except Exception:\n",
        "        return p\n",
        "\n",
        "# -------- Dataset assembly (per asset) --------\n",
        "def _prepare_dataset_for_asset(asset: dict, dataset_root: str, use_keywords: bool):\n",
        "    label      = asset[\"label\"]\n",
        "    price_col  = asset[\"price_col\"]\n",
        "    dataset_tag = f\"{dataset_root}_{'ext' if use_keywords else 'base'}\"\n",
        "    base_source = df_raw if dataset_root == \"raw\" else df_eng\n",
        "\n",
        "    base = make_target(base_source[[price_col]], price_col).dropna(subset=[price_col, \"y_up\"])\n",
        "\n",
        "    price_cols_all = {a[\"price_col\"] for a in ASSETS}\n",
        "    non_trend_engineered = [c for c in base_source.columns\n",
        "                            if c not in price_cols_all and \"trend\" not in c.lower()]\n",
        "\n",
        "    keywords_used = 0\n",
        "    if use_keywords:\n",
        "        kws = load_keywords_for_pair(KW_CSV, asset[\"PAIR_ID\"])\n",
        "        trend_cols = [c for c in map_keywords_to_trend_cols(base_source.columns, kws) if c in base_source.columns]\n",
        "        lag_df = build_trend_lag_features(base_source, trend_cols, lag_steps=(7,14,21))\n",
        "        ext = (base\n",
        "               .join(base_source[non_trend_engineered], how=\"left\")\n",
        "               .join(base_source[trend_cols],          how=\"left\")\n",
        "               .join(lag_df,                           how=\"left\"))\n",
        "        if trend_cols: ext[trend_cols] = ext[trend_cols].fillna(0.0)\n",
        "        lag_cols = [c for c in ext.columns if \"__chg\" in c]\n",
        "        if lag_cols:  ext[lag_cols] = ext[lag_cols].fillna(0.0)\n",
        "        keywords_used = len(trend_cols)\n",
        "    else:\n",
        "        ext = base.join(base_source[non_trend_engineered], how=\"left\")\n",
        "\n",
        "    keep_cols = [c for c in ext.columns if c not in (\"y_up\", price_col) and np.issubdtype(ext[c].dtype, np.number)]\n",
        "    feature_cols = sorted(keep_cols)\n",
        "    ext = ext[[*feature_cols, \"y_up\", price_col]].copy()\n",
        "\n",
        "    # Cross-asset leak guard\n",
        "    def _asset_aliases(a: dict) -> set[str]:\n",
        "        pid = str(a[\"PAIR_ID\"]).lower()\n",
        "        root = str(a[\"price_col\"]).split()[0].lower()\n",
        "        lab = str(a[\"label\"]).lower()\n",
        "        S = {pid, root, lab}\n",
        "        if pid == \"usdcny=x\": S |= {\"cny=x\"}\n",
        "        if pid == \"cny=x\":    S |= {\"usdcny=x\"}\n",
        "        return S\n",
        "    other_tokens = set().union(*[_asset_aliases(a) for a in ASSETS if a is not asset])\n",
        "    leaky = [c for c in feature_cols if any(tok in c.lower() for tok in other_tokens)]\n",
        "    if leaky:\n",
        "        print(\"[WARN] Cross-asset leakage detected, dropping:\", leaky[:10], \"...\" if len(leaky) > 10 else \"\")\n",
        "        feature_cols = [c for c in feature_cols if c not in leaky]\n",
        "        ext = ext[[*feature_cols, \"y_up\", price_col]].copy()\n",
        "\n",
        "    # NaN safety on features\n",
        "    if feature_cols:\n",
        "        ext[feature_cols] = ext[feature_cols].ffill().bfill().fillna(0.0)\n",
        "\n",
        "    ext = ext.dropna(subset=[\"y_up\"])\n",
        "    return label, dataset_tag, ext, feature_cols, keywords_used, price_col\n",
        "\n",
        "# -------- DL factories (fixed ranks) --------\n",
        "def make_mlp(input_shape_3d):\n",
        "    # input_shape_3d = (window, features); flatten to (window*features)\n",
        "    keras.utils.set_random_seed(SEED)\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape_3d),\n",
        "        keras.layers.Flatten(),                                  # <-- ensure 2D to Dense\n",
        "        keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(64, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ], name=\"MLP_flat\")\n",
        "\n",
        "def make_lstm(input_shape, units=64, dropout=0.20):\n",
        "    keras.utils.set_random_seed(SEED)\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),\n",
        "        keras.layers.LSTM(units, return_sequences=True),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.LSTM(units//2, return_sequences=False),     # <-- vector output\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ], name=\"LSTM_seq\")\n",
        "\n",
        "def make_gru(input_shape, units=64, dropout=0.20):\n",
        "    keras.utils.set_random_seed(SEED)\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),\n",
        "        keras.layers.GRU(units, return_sequences=True),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.GRU(units//2, return_sequences=False),      # <-- vector output\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ], name=\"GRU_seq\")\n",
        "\n",
        "def compile_binary(model, lr=1e-3):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[keras.metrics.AUC(name=\"auc\"), keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
        "    return model\n",
        "\n",
        "# -------- Sequence packing --------\n",
        "def build_sequences_from_extended(df_ext: pd.DataFrame, feature_cols: list[str], y_col=\"y_up\", window=30):\n",
        "    X = df_ext[feature_cols].values\n",
        "    y = df_ext[y_col].astype(int).values\n",
        "    idx = df_ext.index\n",
        "    xs, ys, dates = [], [], []\n",
        "    for t in range(window, len(df_ext)):\n",
        "        xs.append(X[t-window:t, :]); ys.append(y[t]); dates.append(idx[t])\n",
        "    return np.asarray(xs), np.asarray(ys), pd.DatetimeIndex(dates)\n",
        "\n",
        "def split_scale_sequences(X_seq, y_seq, train=0.70, valid=0.15):\n",
        "    n = len(X_seq); i_tr = int(n*train); i_va = int(n*(train+valid))\n",
        "    X_tr, X_va, X_te = X_seq[:i_tr], X_seq[i_tr:i_va], X_seq[i_va:]\n",
        "    y_tr, y_va, y_te = y_seq[:i_tr], y_seq[i_tr:i_va], y_seq[i_va:]\n",
        "    if len(X_tr)==0: return (X_tr,y_tr), (X_va,y_va), (X_te,y_te)\n",
        "    T,W,F = X_tr.shape\n",
        "    scaler = StandardScaler().fit(X_tr.reshape(T*W,F))\n",
        "    def _tf(x):\n",
        "        if len(x)==0: return x\n",
        "        TT,WW,FF = x.shape\n",
        "        return scaler.transform(x.reshape(TT*WW,FF)).reshape(TT,WW,FF)\n",
        "    return (_tf(X_tr),y_tr),(_tf(X_va),y_va),(_tf(X_te),y_te)\n",
        "\n",
        "# -------- DL runner (MLP/LSTM/GRU) --------\n",
        "def run_asset_dl(asset: dict,\n",
        "                 dataset_root: str = \"raw\",\n",
        "                 use_keywords: bool = True,\n",
        "                 use_rolling: bool = USE_ROLLING_DL,\n",
        "                 max_folds: int = MAX_FOLDS,\n",
        "                 roll_step: str | int = ROLL_STEP,\n",
        "                 window: int = DL_WINDOW):\n",
        "\n",
        "    label, dataset_tag, ext, feature_cols, keywords_used, price_col = _prepare_dataset_for_asset(\n",
        "        asset, dataset_root, use_keywords\n",
        "    )\n",
        "    rows_used = len(ext)\n",
        "\n",
        "    modes = []\n",
        "    if ENABLE_MLP:  modes.append(\"mlp\")\n",
        "    if ENABLE_LSTM: modes.append(\"lstm\")\n",
        "    if ENABLE_GRU:  modes.append(\"gru\")\n",
        "    if not modes:\n",
        "        print(f\"[SKIP] DL disabled for {label} [{dataset_tag}]\")\n",
        "        return\n",
        "\n",
        "    # sequences\n",
        "    Xseq, yseq, idx_seq = build_sequences_from_extended(ext, feature_cols, \"y_up\", window)\n",
        "    nseq = len(Xseq)\n",
        "    if nseq < 80:\n",
        "        print(f\"[WARN] {label} [{dataset_tag}]: not enough sequences (>=80 required).\")\n",
        "        return\n",
        "    (X_tr_raw, y_tr_raw), (X_va_raw, y_va_raw), (X_te_raw, y_te_raw) = split_scale_sequences(Xseq, yseq, 0.70, 0.15)\n",
        "    i_tr_end = int(nseq*0.70); i_va_end = int(nseq*0.85)\n",
        "\n",
        "    def slice_scaled(tr: slice, va: slice, te: slice):\n",
        "        X_tr = X_tr_raw[max(tr.start,0): min(tr.stop, i_tr_end)]\n",
        "        y_tr = y_tr_raw[max(tr.start,0): min(tr.stop, i_tr_end)]\n",
        "        va_s = max(va.start - i_tr_end, 0); va_e = max(min(va.stop - i_tr_end, len(X_va_raw)), 0)\n",
        "        X_va = X_va_raw[va_s:va_e]; y_va = y_va_raw[va_s:va_e]\n",
        "        te_s = max(te.start - i_va_end, 0); te_e = max(min(te.stop - i_va_end, len(X_te_raw)), 0)\n",
        "        X_te = X_te_raw[te_s:te_e]; y_te = y_te_raw[te_s:te_e]\n",
        "        return (X_tr, y_tr), (X_va, y_va), (X_te, y_te)\n",
        "\n",
        "    folds = time_series_walk_forward_folds(nseq, 0.70, 0.15, 0.15, embargo=5,\n",
        "                                           step=roll_step, min_train=50, max_folds=max_folds) if use_rolling \\\n",
        "            else [Fold(0, slice(0,int(nseq*0.70)), slice(int(nseq*0.70)+5,int(nseq*0.85)-5), slice(int(nseq*0.85)+5,nseq))]\n",
        "\n",
        "    rows_summary = []\n",
        "    for mode in modes:\n",
        "        per_fold = []\n",
        "        for fd in folds:\n",
        "            (X_tr, y_tr), (X_va, y_va), (X_te, y_te) = slice_scaled(fd.tr, fd.va, fd.te)\n",
        "            if len(X_tr)==0 or len(X_va)==0 or len(X_te)==0: continue\n",
        "\n",
        "            if mode == \"mlp\":\n",
        "                model = compile_binary(make_mlp(X_tr.shape[1:]), lr=1e-3)             # Flatten inside\n",
        "            elif mode == \"lstm\":\n",
        "                model = compile_binary(make_lstm(X_tr.shape[1:], units=64, dropout=0.20), lr=1e-3)\n",
        "            elif mode == \"gru\":\n",
        "                model = compile_binary(make_gru(X_tr.shape[1:], units=64, dropout=0.20), lr=1e-3)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
        "            EPOCHS = 3 if DEBUG else 50\n",
        "            BATCH  = 32 if DEBUG else 64\n",
        "            model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
        "                      epochs=EPOCHS, batch_size=BATCH, callbacks=[es], verbose=0)\n",
        "\n",
        "            p_va = model.predict(X_va, verbose=0).ravel()\n",
        "            p_te = model.predict(X_te, verbose=0).ravel()\n",
        "            cal  = platt_calibrate_on_val(p_va, y_va)\n",
        "            p_teC = apply_calibration(cal, p_te)\n",
        "            h_te  = (p_teC >= 0.5).astype(int)\n",
        "\n",
        "            auc = roc_auc_score(y_te, p_teC) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "            acc = accuracy_score(y_te, h_te)\n",
        "            f1  = f1_score(y_te, h_te) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "            per_fold.append({\"asset\": label, \"dataset\": dataset_tag, \"model\": mode.upper(),\n",
        "                             \"fold\": fd.fold_id, \"auc\": auc, \"acc\": acc, \"f1\": f1,\n",
        "                             \"va_n\": len(y_va), \"te_n\": len(y_te)})\n",
        "\n",
        "            val_idx  = idx_seq[fd.va]\n",
        "            test_idx = idx_seq[fd.te]\n",
        "            df_val  = pd.DataFrame({\"date\": val_idx,  \"y_true\": y_va, \"prob_up\": p_va}).set_index(\"date\")\n",
        "            df_test = pd.DataFrame({\"date\": test_idx, \"y_true\": y_te, \"prob_up\": p_teC}).set_index(\"date\")\n",
        "            save_preds_df(df_val,  label, dataset_tag, mode, window=DL_WINDOW, split=f\"val_f{fd.fold_id}\")\n",
        "            save_preds_df(df_test, label, dataset_tag, mode, window=DL_WINDOW, split=f\"test_f{fd.fold_id}\")\n",
        "\n",
        "        if per_fold:\n",
        "            dfm = pd.DataFrame(per_fold)\n",
        "            agg = dfm.agg({\"auc\":\"mean\",\"acc\":\"mean\",\"f1\":\"mean\"}).to_dict()\n",
        "            agg.update({\"asset\": label, \"dataset\": dataset_tag, \"model\": mode.upper(), \"folds\": len(dfm)})\n",
        "            rows_summary.append(agg)\n",
        "\n",
        "    if rows_summary:\n",
        "        roll_csv = STAGES[\"leaderboard\"] / \"rolling_dl_metrics.csv\"\n",
        "        df_roll = pd.DataFrame(rows_summary)\n",
        "        if roll_csv.exists():\n",
        "            prev = pd.read_csv(roll_csv)\n",
        "            df_roll = pd.concat([prev, df_roll], ignore_index=True)\n",
        "        df_roll.to_csv(roll_csv, index=False)\n",
        "\n",
        "    lines = [\n",
        "        f\"{label} [{dataset_tag}] — DL modes={','.join(modes)} | rolling={use_rolling} | window={DL_WINDOW}\",\n",
        "        f\"[Run] FREQ={FREQ} | DEBUG={DEBUG} | RUN_ID={RUN_ID}\",\n",
        "        f\"[Data] Rows: {rows_used} | Sequences: {len(Xseq)} | Folds: {len(folds)}\",\n",
        "        f\"[Keywords] used={keywords_used}\",\n",
        "        \"[Columns]\",\n",
        "        *[f\"    - {c}\" for c in feature_cols],\n",
        "        \"[Hyperparams]\",\n",
        "        f\"    MLP 128-64, drop=0.30 (flattened), lr=1e-3, epochs={'3' if DEBUG else '50'}, batch={'32' if DEBUG else '64'}\",\n",
        "        f\"    LSTM 64→32,  drop=0.20, lr=1e-3, epochs={'3' if DEBUG else '50'}, batch={'32' if DEBUG else '64'}\",\n",
        "        f\"    GRU  64→32,  drop=0.20, lr=1e-3, epochs={'3' if DEBUG else '50'}, batch={'32' if DEBUG else '64'}\",\n",
        "        f\"    Calibration: {'Platt on validation' if CALIBRATE_PROBS else 'OFF'}\",\n",
        "    ]\n",
        "    save_txt_log(label, dataset_tag, f\"DL_{','.join(modes)}\", lines)\n",
        "    print(f\"✅ DL finished {label} [{dataset_tag}] | rows: {rows_used} | features: {len(feature_cols)}\")\n",
        "\n",
        "# -------- ML runner (LR/RF/XGB) --------\n",
        "def run_asset_ml(asset: dict,\n",
        "                 dataset_root: str = \"raw\",\n",
        "                 use_keywords: bool = True,\n",
        "                 use_rolling: bool = USE_ROLLING_ML,\n",
        "                 max_folds: int = MAX_FOLDS,\n",
        "                 roll_step: str | int = ROLL_STEP):\n",
        "\n",
        "    label, dataset_tag, ext, feature_cols, keywords_used, price_col = _prepare_dataset_for_asset(\n",
        "        asset, dataset_root, use_keywords\n",
        "    )\n",
        "    rows_used = len(ext)\n",
        "\n",
        "    X = ext[feature_cols].values\n",
        "    y = ext[\"y_up\"].astype(int).values\n",
        "    n = len(X)\n",
        "    if n < 100:\n",
        "        print(f\"[WARN] {label} [{dataset_tag}]: not enough rows (>=100 required).\")\n",
        "        return\n",
        "\n",
        "    folds = time_series_walk_forward_folds(n, 0.70, 0.15, 0.15, embargo=5,\n",
        "                                           step=roll_step, min_train=80, max_folds=max_folds) if use_rolling else \\\n",
        "            [Fold(0, slice(0,int(n*0.70)-5), slice(int(n*0.70)+5, int(n*0.85)-5), slice(int(n*0.85)+5, n))]\n",
        "\n",
        "    fitters = []\n",
        "\n",
        "    if ENABLE_LR:\n",
        "        def fit_lr(Xtr, ytr):\n",
        "            pipe = Pipeline(steps=[\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"scl\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
        "            ]).fit(Xtr, ytr)\n",
        "            return (\"LR\", pipe)\n",
        "        fitters.append(fit_lr)\n",
        "\n",
        "    if ENABLE_RF:\n",
        "        def fit_rf(Xtr, ytr):\n",
        "            pipe = Pipeline(steps=[\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"clf\", RandomForestClassifier(\n",
        "                    n_estimators=400 if not DEBUG else 100,\n",
        "                    max_depth=None, n_jobs=-1, random_state=SEED,\n",
        "                    class_weight=\"balanced_subsample\"\n",
        "                ))\n",
        "            ]).fit(Xtr, ytr)\n",
        "            return (\"RF\", pipe)\n",
        "        fitters.append(fit_rf)\n",
        "\n",
        "    if ENABLE_XGB and HAVE_XGB:\n",
        "        def fit_xgb(Xtr, ytr):\n",
        "            pipe = Pipeline(steps=[\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"clf\", XGBClassifier(\n",
        "                    n_estimators=600 if not DEBUG else 200,\n",
        "                    max_depth=4, learning_rate=0.08,\n",
        "                    subsample=0.8, colsample_bytree=0.8,\n",
        "                    min_child_weight=2, reg_lambda=1.0,\n",
        "                    tree_method=\"hist\", eval_metric=\"auc\",\n",
        "                    random_state=SEED, n_jobs=-1\n",
        "                ))\n",
        "            ]).fit(Xtr, ytr)\n",
        "            return (\"XGB\", pipe)\n",
        "        fitters.append(fit_xgb)\n",
        "\n",
        "    rows_summary = []\n",
        "    for fit_fn in fitters:\n",
        "        per_fold = []\n",
        "        for fd in folds:\n",
        "            tr, va, te = fd.tr, fd.va, fd.te\n",
        "            Xtr, ytr = X[tr], y[tr]\n",
        "            Xva, yva = X[va], y[va]\n",
        "            Xte, yte = X[te], y[te]\n",
        "            if len(Xtr)==0 or len(Xva)==0 or len(Xte)==0: continue\n",
        "\n",
        "            name, model = fit_fn(Xtr, ytr)\n",
        "\n",
        "            p_va = model.predict_proba(Xva)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(Xva)\n",
        "            p_te = model.predict_proba(Xte)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(Xte)\n",
        "\n",
        "            cal   = platt_calibrate_on_val(p_va, yva)\n",
        "            p_teC = apply_calibration(cal, p_te)\n",
        "            h_te  = (p_teC >= 0.5).astype(int)\n",
        "\n",
        "            auc = roc_auc_score(yte, p_teC) if len(np.unique(yte))>1 else float(\"nan\")\n",
        "            acc = accuracy_score(yte, h_te)\n",
        "            f1  = f1_score(yte, h_te) if len(np.unique(yte))>1 else float(\"nan\")\n",
        "            per_fold.append({\"asset\": label, \"dataset\": dataset_tag, \"model\": name,\n",
        "                             \"fold\": fd.fold_id, \"auc\": auc, \"acc\": acc, \"f1\": f1,\n",
        "                             \"va_n\": len(yva), \"te_n\": len(yte)})\n",
        "\n",
        "            va_idx = ext.index[va]; te_idx = ext.index[te]\n",
        "            df_val  = pd.DataFrame({\"date\": va_idx, \"y_true\": yva, \"prob_up\": p_va}).set_index(\"date\")\n",
        "            df_test = pd.DataFrame({\"date\": te_idx, \"y_true\": yte, \"prob_up\": p_teC}).set_index(\"date\")\n",
        "            save_preds_df(df_val,  label, dataset_tag, name.lower(), window=None, split=f\"val_f{fd.fold_id}\")\n",
        "            save_preds_df(df_test, label, dataset_tag, name.lower(), window=None, split=f\"test_f{fd.fold_id}\")\n",
        "\n",
        "        if per_fold:\n",
        "            dfm = pd.DataFrame(per_fold)\n",
        "            agg = dfm.agg({\"auc\":\"mean\",\"acc\":\"mean\",\"f1\":\"mean\"}).to_dict()\n",
        "            agg.update({\"asset\": label, \"dataset\": dataset_tag, \"model\": dfm[\"model\"].iloc[0], \"folds\": len(dfm)})\n",
        "            rows_summary.append(agg)\n",
        "\n",
        "    if rows_summary:\n",
        "        roll_csv = STAGES[\"leaderboard\"] / \"rolling_ml_metrics.csv\"\n",
        "        df_roll = pd.DataFrame(rows_summary)\n",
        "        if roll_csv.exists():\n",
        "            prev = pd.read_csv(roll_csv)\n",
        "            df_roll = pd.concat([prev, df_roll], ignore_index=True)\n",
        "        df_roll.to_csv(roll_csv, index=False)\n",
        "\n",
        "    lines = [\n",
        "        f\"{label} [{dataset_tag}] — ML (rolling={use_rolling})\",\n",
        "        f\"[Run] FREQ={FREQ} | DEBUG={DEBUG} | RUN_ID={RUN_ID}\",\n",
        "        f\"[Data] Rows: {rows_used} | Folds: {len(folds)} | MaxFolds: {max_folds}\",\n",
        "        f\"[Keywords] used={keywords_used}\",\n",
        "        \"[Columns]\",\n",
        "        *[f\"    - {c}\" for c in feature_cols],\n",
        "        \"[Hyperparams]\",\n",
        "        \"    LR: StandardScaler + median imputer + lbfgs, class_weight=balanced\",\n",
        "        f\"    RF: median imputer + n_estimators={400 if not DEBUG else 100}, class_weight=balanced_subsample\",\n",
        "        (\"    XGB: median imputer + n_estimators={}; depth=4; lr=0.08; subsample=0.8; colsample=0.8; min_child_weight=2\"\n",
        "         .format(600 if not DEBUG else 200)) if (ENABLE_XGB and HAVE_XGB) else \"    XGB: not available\",\n",
        "        f\"    Calibration: {'Platt on validation' if CALIBRATE_PROBS else 'OFF'}\",\n",
        "    ]\n",
        "    save_txt_log(label, dataset_tag, \"ML\", lines)\n",
        "    print(f\"✅ ML finished {label} [{dataset_tag}] | rows: {rows_used} | features: {len(feature_cols)}\")\n",
        "\n",
        "# -------- Backtest + figs --------\n",
        "def equity_curve_from_probs(df_probs: pd.DataFrame, price_series: pd.Series, entry_col=\"prob_up\", thresh=0.5):\n",
        "    df = df_probs.copy().join(price_series, how=\"left\")\n",
        "    ret = price_series.pct_change().shift(-1)\n",
        "    signal = (df[entry_col] >= thresh).astype(int)\n",
        "    eq = (1 + (signal * ret).fillna(0.0)).cumprod()\n",
        "    return eq\n",
        "\n",
        "def summarize_backtests():\n",
        "    rows = []\n",
        "    for p in sorted((STAGES[\"preds_test\"]).glob(\"*.csv\")):\n",
        "        parts = p.name.split(\".\")\n",
        "        if len(parts) < 3 or not parts[1].startswith(\"test\"): continue\n",
        "\n",
        "        stem = parts[0]                   # <asset>_<dataset>_<model>[_wXX]\n",
        "        tokens = stem.split(\"_\")\n",
        "        asset_slug = tokens[0]\n",
        "        model = tokens[-1].split(\"_w\")[0]\n",
        "        dataset = \"_\".join(tokens[1:-1]) if len(tokens) > 2 else \"unknown\"\n",
        "\n",
        "        d = pd.read_csv(p)\n",
        "        if \"date\" not in d.columns: d.rename(columns={d.columns[0]: \"date\"}, inplace=True)\n",
        "        d[\"date\"] = pd.to_datetime(d[\"date\"]); d.set_index(\"date\", inplace=True)\n",
        "        if not {\"prob_up\",\"y_true\"}.issubset(d.columns): continue\n",
        "\n",
        "        label = next((a for a in ASSETS if asset_slug == _slug(a[\"label\"])), None)\n",
        "        if label is None: continue\n",
        "        base_source = df_raw if dataset.startswith(\"raw\") else df_eng\n",
        "        price = base_source[label[\"price_col\"]].reindex(d.index).ffill()\n",
        "\n",
        "        eq = equity_curve_from_probs(d, price, \"prob_up\", 0.5)\n",
        "        rows.append({\"asset\": label[\"label\"], \"dataset\": dataset, \"model\": model.upper(),\n",
        "                     \"final_equity\": float(eq.iloc[-1]) if len(eq) else np.nan,\n",
        "                     \"obs\": int(len(eq))})\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(6,3))\n",
        "        eq.plot(ax=ax); ax.set_title(f\"{label['label']} — {dataset} — {model.upper()} equity\")\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        save_figure(fig, label[\"label\"].lower(), f\"{model.lower()}_{dataset}_equity\")\n",
        "\n",
        "    if rows:\n",
        "        dfb = pd.DataFrame(rows).sort_values([\"asset\",\"dataset\",\"final_equity\"],\n",
        "                                             ascending=[True, True, False])\n",
        "        path = STAGES[\"leaderboard\"] / \"backtest_summary.csv\"\n",
        "        dfb.to_csv(path, index=False)\n",
        "        print(\"Backtest summary saved:\", path)\n",
        "        with pd.option_context('display.max_rows', None, 'display.width', 120):\n",
        "            print(dfb.head(50))\n",
        "    else:\n",
        "        print(\"No backtests found under test predictions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Driver: iterate dataset roots × keyword usage × assets\n",
        "# =========================================================\n",
        "grid = [\n",
        "    (\"raw\", False),  # raw_base\n",
        "    (\"raw\", True),   # raw_ext\n",
        "    (\"eng\", False),  # eng_base\n",
        "    (\"eng\", True),   # eng_ext\n",
        "]\n",
        "\n",
        "assets_to_run = [asset_by_label[PAIR_KEY]] if RUN_MODE == \"single\" else ASSETS\n",
        "\n",
        "for dataset_root, use_kw in grid:\n",
        "    tag = f\"{dataset_root}_{'ext' if use_kw else 'base'}\"\n",
        "    print(f\"--- {tag} — ML + DL ---\")\n",
        "    for asset in assets_to_run:\n",
        "        run_asset_ml(asset,\n",
        "                     dataset_root=dataset_root,\n",
        "                     use_keywords=use_kw,\n",
        "                     use_rolling=USE_ROLLING_ML,\n",
        "                     max_folds=MAX_FOLDS,\n",
        "                     roll_step=ROLL_STEP)\n",
        "        run_asset_dl(asset,\n",
        "                     dataset_root=dataset_root,\n",
        "                     use_keywords=use_kw,\n",
        "                     use_rolling=USE_ROLLING_DL,\n",
        "                     max_folds=MAX_FOLDS,\n",
        "                     roll_step=ROLL_STEP,\n",
        "                     window=DL_WINDOW)\n",
        "\n",
        "# Leaderboards + figs\n",
        "summarize_backtests()\n",
        "\n",
        "print(\"\\n=== ARTIFACTS ===\")\n",
        "for k, v in STAGES.items():\n",
        "    try:\n",
        "        n = len(list(v.glob(\"*\"))) if v.exists() else 0\n",
        "        print(f\"{k:>12}: {v}  ({n} files)\")\n",
        "    except Exception:\n",
        "        print(f\"{k:>12}: {v}\")\n"
      ],
      "metadata": {
        "id": "eaTi5-cs-B3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d05031-cb78-484a-b636-e19ac15b0a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- raw_base — ML + DL ---\n",
            "[WARN] Cross-asset leakage detected, dropping: ['BTC-USD 14-day RSI', 'BTC-USD 20-day simple moving average', 'BTC-USD 30-day Rolling Volatility', 'BTC-USD High', 'BTC-USD Log Returns', 'BTC-USD Low', 'BTC-USD Open', 'CL=F 14-day RSI', 'CL=F 20-day simple moving average', 'CL=F 30-day Rolling Volatility'] ...\n",
            "✅ ML finished Gold [raw_base] | rows: 2608 | features: 16\n",
            "[WARN] Cross-asset leakage detected, dropping: ['BTC-USD 14-day RSI', 'BTC-USD 20-day simple moving average', 'BTC-USD 30-day Rolling Volatility', 'BTC-USD High', 'BTC-USD Log Returns', 'BTC-USD Low', 'BTC-USD Open', 'CL=F 14-day RSI', 'CL=F 20-day simple moving average', 'CL=F 30-day Rolling Volatility'] ...\n"
          ]
        }
      ]
    }
  ]
}