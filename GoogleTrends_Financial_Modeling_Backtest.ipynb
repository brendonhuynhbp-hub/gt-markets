{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "GoogleTrends_Financial_Modeling_Backtest.ipynb",
      "authorship_tag": "ABX9TyMXPKO0MYL4TC7paIBI9m4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendonhuynhbp-hub/gt-markets/blob/main/GoogleTrends_Financial_Modeling_Backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup: Mount Drive + Paths"
      ],
      "metadata": {
        "id": "AbwJ76zT6Jn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRpMJvOO1q2r",
        "outputId": "710e6ddb-293d-41b1-8b05-259811bd1f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "PROJECT_DIR: /content/drive/MyDrive/gt-markets\n",
            "DATA_DIR:    /content/drive/MyDrive/gt-markets/data/processed\n",
            "KW_DIR:      /content/drive/MyDrive/gt-markets/data/Keyword Selection\n",
            "OUT_DIR:     /content/drive/MyDrive/gt-markets/outputs\n",
            "TF on CPU\n",
            "TF version: 2.19.0\n",
            "merged_financial_trends_data_2025-09-07.csv: filling NaNs in trend cols with 0: ['entrepreneurial_trend', 'cryptocurrency_trend'] \n",
            "RAW frame: 2015-09-13 → 2025-09-07 | rows: 522\n",
            "ENG frame: 2015-09-13 → 2025-09-07 | rows: 522\n",
            "RUN_ID: w_prod_20250914-013201\n",
            "        data: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/00_data\n",
            "        logs: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/10_logs\n",
            "   preds_val: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/20_preds/val\n",
            "  preds_test: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/20_preds/test\n",
            "   backtests: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/30_backtests\n",
            "        figs: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs\n",
            " leaderboard: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/50_leaderboards\n",
            "=== RUN CONFIG ===\n",
            "FREQ: W | DEBUG: False | RUN_ID: w_prod_20250914-013201\n",
            "OUTPUT ROOT: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201\n",
            "RUN_MODE: all\n",
            "\n",
            "--- RAW_BASE — ML + DL ---\n",
            "✅ ML finished Gold [raw_base] | rows: 521 | features: 15\n",
            "✅ DL finished Gold [raw_base] | rows: 521 | features: 15\n",
            "✅ ML finished BTC [raw_base] | rows: 521 | features: 15\n",
            "✅ DL finished BTC [raw_base] | rows: 521 | features: 15\n",
            "✅ ML finished Oil [raw_base] | rows: 521 | features: 15\n",
            "✅ DL finished Oil [raw_base] | rows: 521 | features: 15\n",
            "✅ ML finished USDCNY [raw_base] | rows: 521 | features: 15\n",
            "✅ DL finished USDCNY [raw_base] | rows: 521 | features: 15\n",
            "\n",
            "--- RAW_EXT — ML + DL ---\n",
            "✅ ML finished Gold [raw_ext] | rows: 521 | features: 75\n",
            "✅ DL finished Gold [raw_ext] | rows: 521 | features: 75\n",
            "✅ ML finished BTC [raw_ext] | rows: 521 | features: 103\n",
            "✅ DL finished BTC [raw_ext] | rows: 521 | features: 103\n",
            "[WARN] Cross-asset leakage detected, dropping: ['gold_price_trend', 'gold_trend', 'gold_price_trend__chg7', 'gold_price_trend__chg14', 'gold_price_trend__chg21', 'gold_trend__chg7', 'gold_trend__chg14', 'gold_trend__chg21'] \n",
            "✅ ML finished Oil [raw_ext] | rows: 521 | features: 103\n",
            "[WARN] Cross-asset leakage detected, dropping: ['gold_price_trend', 'gold_trend', 'gold_price_trend__chg7', 'gold_price_trend__chg14', 'gold_price_trend__chg21', 'gold_trend__chg7', 'gold_trend__chg14', 'gold_trend__chg21'] \n",
            "✅ DL finished Oil [raw_ext] | rows: 521 | features: 103\n",
            "✅ ML finished USDCNY [raw_ext] | rows: 521 | features: 127\n",
            "✅ DL finished USDCNY [raw_ext] | rows: 521 | features: 127\n",
            "\n",
            "--- ENG_BASE — ML + DL ---\n",
            "✅ ML finished Gold [eng_base] | rows: 521 | features: 36\n",
            "✅ DL finished Gold [eng_base] | rows: 521 | features: 36\n",
            "✅ ML finished BTC [eng_base] | rows: 521 | features: 36\n",
            "✅ DL finished BTC [eng_base] | rows: 521 | features: 36\n",
            "✅ ML finished Oil [eng_base] | rows: 521 | features: 36\n",
            "✅ DL finished Oil [eng_base] | rows: 521 | features: 36\n",
            "✅ ML finished USDCNY [eng_base] | rows: 521 | features: 36\n",
            "✅ DL finished USDCNY [eng_base] | rows: 521 | features: 36\n",
            "\n",
            "--- ENG_EXT — ML + DL ---\n",
            "✅ ML finished Gold [eng_ext] | rows: 521 | features: 96\n",
            "✅ DL finished Gold [eng_ext] | rows: 521 | features: 96\n",
            "✅ ML finished BTC [eng_ext] | rows: 521 | features: 124\n",
            "✅ DL finished BTC [eng_ext] | rows: 521 | features: 124\n",
            "[WARN] Cross-asset leakage detected, dropping: ['gold_price_trend', 'gold_trend', 'gold_price_trend__chg7', 'gold_price_trend__chg14', 'gold_price_trend__chg21', 'gold_trend__chg7', 'gold_trend__chg14', 'gold_trend__chg21'] \n",
            "✅ ML finished Oil [eng_ext] | rows: 521 | features: 124\n",
            "[WARN] Cross-asset leakage detected, dropping: ['gold_price_trend', 'gold_trend', 'gold_price_trend__chg7', 'gold_price_trend__chg14', 'gold_price_trend__chg21', 'gold_trend__chg7', 'gold_trend__chg14', 'gold_trend__chg21'] \n",
            "✅ DL finished Oil [eng_ext] | rows: 521 | features: 124\n",
            "✅ ML finished USDCNY [eng_ext] | rows: 521 | features: 148\n",
            "✅ DL finished USDCNY [eng_ext] | rows: 521 | features: 148\n",
            "Leaderboard saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/50_leaderboards/leaderboard_metrics.csv\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/btc_rf_eng_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/btc_mlp_eng_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/btc_mlp_raw_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/btc_lstm_raw_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/gold_lr_eng_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/gold_mlp_eng_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/gold_lr_raw_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/gold_lstm_raw_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/oil_lr_eng_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/oil_lr_eng_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/oil_lstm_raw_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/oil_lstm_raw_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/usdcny_lstm_eng_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/usdcny_xgb_eng_ext_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/usdcny_lr_raw_base_equity.png\n",
            "Figure saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs/usdcny_mlp_raw_ext_equity.png\n",
            "Backtest summary saved: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/50_leaderboards/backtest_summary.csv\n",
            "     asset   dataset model  final_equity  obs\n",
            "1      BTC   eng_ext   MLP      1.641511   74\n",
            "2      BTC  raw_base   MLP      1.268500   74\n",
            "3      BTC   raw_ext  LSTM      1.129485   69\n",
            "0      BTC  eng_base    RF      0.912481   74\n",
            "7     Gold   raw_ext  LSTM      1.416852   69\n",
            "5     Gold   eng_ext   MLP      1.214748   74\n",
            "4     Gold  eng_base    LR      1.155970   74\n",
            "6     Gold  raw_base    LR      1.007203   74\n",
            "9      Oil   eng_ext    LR      1.311487   74\n",
            "8      Oil  eng_base    LR      0.968081   74\n",
            "11     Oil   raw_ext  LSTM      0.905930   69\n",
            "10     Oil  raw_base  LSTM      0.772409   69\n",
            "13  USDCNY   eng_ext   XGB      1.030042   74\n",
            "14  USDCNY  raw_base    LR      1.013551   74\n",
            "15  USDCNY   raw_ext   MLP      1.011898   74\n",
            "12  USDCNY  eng_base  LSTM      1.010171   69\n",
            "\n",
            "=== ARTIFACTS ===\n",
            "        data: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/00_data  (1 files)\n",
            "        logs: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/10_logs  (32 files)\n",
            "   preds_val: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/20_preds/val  (80 files)\n",
            "  preds_test: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/20_preds/test  (80 files)\n",
            "   backtests: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/30_backtests  (16 files)\n",
            "        figs: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/40_figs  (16 files)\n",
            " leaderboard: /content/drive/MyDrive/gt-markets/outputs/runs/w_prod_20250914-013201/50_leaderboards  (2 files)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================================================\n",
        "# Google Trends + Financial Modeling — Backtest Pipeline\n",
        "# (Per-asset engineered feature filtering + leak guard + column logging)\n",
        "# =========================================================\n",
        "\n",
        "# -------------------------------\n",
        "# Global switches\n",
        "# -------------------------------\n",
        "DEBUG    = False           # True → faster sanity run; False → full training\n",
        "FREQ     = \"W\"            # \"D\" (daily) or \"W\" (weekly)\n",
        "RUN_MODE = \"all\"          # \"all\" or \"single\"\n",
        "PAIR_KEY = \"gold\"         # when RUN_MODE == \"single\": \"gold\"|\"btc\"|\"oil\"|\"usdcny\"\n",
        "\n",
        "# -------------------------------\n",
        "# Drive + core paths\n",
        "# -------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "import os, warnings, random, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CANDIDATE_PROJECT_DIRS = [\n",
        "    Path(\"/content/drive/MyDrive/gt-markets\"),\n",
        "    Path(\"/content/drive/Shareddrives/gt-markets\"),\n",
        "]\n",
        "PROJECT_DIR = next((p for p in CANDIDATE_PROJECT_DIRS if p.exists()), None)\n",
        "assert PROJECT_DIR is not None, \"Project directory not found in Drive.\"\n",
        "\n",
        "DATA_DIR = PROJECT_DIR / \"data\" / \"processed\"\n",
        "KW_DIR   = PROJECT_DIR / \"data\" / \"Keyword Selection\"   # exact folder name\n",
        "OUT_DIR  = PROJECT_DIR / \"outputs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW_FILE = DATA_DIR / \"merged_financial_trends_data_2025-09-07.csv\"\n",
        "ENG_FILE = DATA_DIR / \"merged_financial_trends_engineered_2025-09-07.csv\"\n",
        "KW_CSV   = KW_DIR  / \"combined_significant_lagged_correlations.csv\"\n",
        "\n",
        "assert RAW_FILE.exists(), f\"Missing dataset: {RAW_FILE}\"\n",
        "assert ENG_FILE.exists(), f\"Missing engineered dataset: {ENG_FILE}\"\n",
        "assert KW_CSV.exists(),   f\"Missing keyword file: {KW_CSV}\"\n",
        "\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"DATA_DIR:   \", DATA_DIR)\n",
        "print(\"KW_DIR:     \", KW_DIR)\n",
        "print(\"OUT_DIR:    \", OUT_DIR)\n",
        "\n",
        "# -------------------------------\n",
        "# Reproducibility + warnings\n",
        "# -------------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -------------------------------\n",
        "# ML/DL stack\n",
        "# -------------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    print(f\"TF GPU devices: {len(gpus)} (mem growth on)\" if gpus else \"TF on CPU\")\n",
        "except Exception as e:\n",
        "    print(\"TF GPU setup note:\", e)\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# -------------------------------\n",
        "# Asset registry\n",
        "# -------------------------------\n",
        "ASSETS = [\n",
        "    {\"PAIR_ID\": \"GC=F\",      \"price_col\": \"GC=F Close\",      \"label\": \"Gold\"},\n",
        "    {\"PAIR_ID\": \"BTC-USD\",   \"price_col\": \"BTC-USD Close\",   \"label\": \"BTC\"},\n",
        "    {\"PAIR_ID\": \"CL=F\",      \"price_col\": \"CL=F Close\",      \"label\": \"Oil\"},\n",
        "    {\"PAIR_ID\": \"USDCNY=X\",  \"price_col\": \"USDCNY=X Close\",  \"label\": \"USDCNY\"},\n",
        "]\n",
        "asset_by_label = {a[\"label\"].lower(): a for a in ASSETS}\n",
        "\n",
        "# -------------------------------\n",
        "# Load + clean data (fill GT NaNs with 0)\n",
        "# -------------------------------\n",
        "def _load_and_clean(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    trend_cols = [c for c in df.columns if \"trend\" in c.lower()]\n",
        "    if trend_cols:\n",
        "        bad = [c for c in trend_cols if df[c].isna().any()]\n",
        "        if bad:\n",
        "            print(f\"{path.name}: filling NaNs in trend cols with 0:\", bad[:10], \"...\" if len(bad)>10 else \"\")\n",
        "            df[bad] = df[bad].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "df_raw0 = _load_and_clean(RAW_FILE)\n",
        "df_eng0 = _load_and_clean(ENG_FILE)\n",
        "if DEBUG:\n",
        "    # Keep a small tail for quick runs\n",
        "    df_raw0 = df_raw0.tail(1000)\n",
        "    df_eng0 = df_eng0.tail(1000)\n",
        "    print(\"DEBUG mode: using last 1000 rows (RAW/ENG).\")\n",
        "\n",
        "# -------------------------------\n",
        "# Frequency control\n",
        "# -------------------------------\n",
        "def to_frequency(df_in: pd.DataFrame, freq: str = \"D\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Weekly re-sample uses last-of-period for prices (avoid look-ahead),\n",
        "    and mean-of-period for all non-price columns (trends/indicators).\n",
        "    \"\"\"\n",
        "    if freq.upper() == \"D\":\n",
        "        return df_in.copy()\n",
        "    assert freq.upper() == \"W\", \"Supported: 'D' or 'W'.\"\n",
        "    out = pd.DataFrame(index=df_in.resample(\"W\").last().index)\n",
        "    # prices as last-of-week\n",
        "    for a in ASSETS:\n",
        "        if a[\"price_col\"] in df_in.columns:\n",
        "            out[a[\"price_col\"]] = df_in[a[\"price_col\"]].resample(\"W\").last()\n",
        "    # other columns as weekly mean\n",
        "    price_cols = {a[\"price_col\"] for a in ASSETS}\n",
        "    for c in df_in.columns:\n",
        "        if c not in price_cols:\n",
        "            out[c] = df_in[c].resample(\"W\").mean()\n",
        "    return out\n",
        "\n",
        "df_raw = to_frequency(df_raw0, FREQ)\n",
        "df_eng = to_frequency(df_eng0, FREQ)\n",
        "print(\"RAW frame:\", df_raw.index.min().date(), \"→\", df_raw.index.max().date(), \"| rows:\", len(df_raw))\n",
        "print(\"ENG frame:\", df_eng.index.min().date(), \"→\", df_eng.index.max().date(), \"| rows:\", len(df_eng))\n",
        "\n",
        "# -------------------------------\n",
        "# Keyword utilities\n",
        "# -------------------------------\n",
        "def load_keywords_for_pair(csv_path: Path, pair_id: str) -> list[str]:\n",
        "    d = pd.read_csv(csv_path)\n",
        "    assert {\"Pair\",\"Keyword\"}.issubset(d.columns), \"Keyword CSV must have: Pair, Keyword.\"\n",
        "    aliases = {pair_id}\n",
        "    if pair_id == \"USDCNY=X\": aliases |= {\"CNY=X\"}\n",
        "    if pair_id == \"CNY=X\":    aliases |= {\"USDCNY=X\"}\n",
        "    kws = (d.loc[d[\"Pair\"].isin(aliases), \"Keyword\"]\n",
        "             .dropna().astype(str).str.strip().str.lower().unique().tolist())\n",
        "    return kws\n",
        "\n",
        "def map_keywords_to_trend_cols(all_cols: pd.Index, keywords: list[str]) -> list[str]:\n",
        "    norm = lambda s: str(s).lower().strip().replace(\" \", \"_\")\n",
        "    desired = {f\"{norm(k)}_trend\" for k in keywords}\n",
        "    return [c for c in all_cols if str(c).lower() in desired]\n",
        "\n",
        "# -------------------------------\n",
        "# Targets + safe lag features\n",
        "# -------------------------------\n",
        "def make_target(df: pd.DataFrame, price_col: str) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"ret1\"] = out[price_col].pct_change()\n",
        "    out[\"y_up\"] = (out[price_col].shift(-1) > out[price_col]).astype(int)\n",
        "    return out.dropna(subset=[price_col]).dropna()\n",
        "\n",
        "def build_trend_lag_features(df_in: pd.DataFrame, sel_cols: list[str], lag_steps=(7,14,21)) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Safe %change features over given lags, shifted by +1 step (causality),\n",
        "    with ±inf→NaN→0 fill and clipping for numeric stability.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for c in sel_cols:\n",
        "        s = df_in[c].astype(float)\n",
        "        for L in lag_steps:\n",
        "            chg = s.pct_change(L).shift(1)\n",
        "            chg = chg.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "            chg = chg.clip(lower=-10.0, upper=10.0)\n",
        "            out[f\"{c}__chg{L}\"] = chg\n",
        "    return pd.DataFrame(out, index=df_in.index)\n",
        "\n",
        "# -------------------------------\n",
        "# Output structure (versioned)\n",
        "# -------------------------------\n",
        "from datetime import datetime\n",
        "RUN_MODE_TAG = \"debug\" if DEBUG else \"prod\"\n",
        "RUN_STAMP    = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_ID       = f\"{FREQ.lower()}_{RUN_MODE_TAG}_{RUN_STAMP}\"\n",
        "\n",
        "RUN_ROOT = OUT_DIR / \"runs\" / RUN_ID\n",
        "STAGES = {\n",
        "    \"data\":        RUN_ROOT / \"00_data\",\n",
        "    \"logs\":        RUN_ROOT / \"10_logs\",\n",
        "    \"preds_val\":   RUN_ROOT / \"20_preds\" / \"val\",\n",
        "    \"preds_test\":  RUN_ROOT / \"20_preds\" / \"test\",\n",
        "    \"backtests\":   RUN_ROOT / \"30_backtests\",\n",
        "    \"figs\":        RUN_ROOT / \"40_figs\",\n",
        "    \"leaderboard\": RUN_ROOT / \"50_leaderboards\",\n",
        "}\n",
        "for p in STAGES.values(): p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(df_raw.head(1)\n",
        " .assign(_start=df_raw.index.min(), _end=df_raw.index.max())\n",
        " .to_csv(STAGES[\"data\"] / f\"dataset_snapshot_{RAW_FILE.name}.head1.csv\"))\n",
        "\n",
        "def _slug(s: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"_\", s.lower()).strip(\"_\")\n",
        "\n",
        "def _pred_name(asset_label: str, dataset_tag: str, model: str, *, window: int | None = None, split=\"test\") -> str:\n",
        "    bits = [_slug(asset_label), dataset_tag, model.lower()]\n",
        "    if window: bits.append(f\"w{window}\")\n",
        "    return \".\".join([\"_\".join(bits), split, \"csv\"])\n",
        "\n",
        "def _log_name(asset_label: str, dataset_tag: str, run_name: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{dataset_tag}_{_slug(run_name)}.{RUN_STAMP}.txt\"\n",
        "\n",
        "def _fig_name(asset_label: str, tail: str) -> str:\n",
        "    return f\"{_slug(asset_label)}_{tail}.png\"\n",
        "\n",
        "def save_txt_log(asset_label: str, dataset_tag: str, run_name: str, lines: list[str]):\n",
        "    path = STAGES[\"logs\"] / _log_name(asset_label, dataset_tag, run_name)\n",
        "    with open(path, \"w\") as f: f.write(\"\\n\".join(lines))\n",
        "    return path\n",
        "\n",
        "def save_preds_df(df_pred: pd.DataFrame, asset_label: str, dataset_tag: str, model: str, *, window: int | None, split: str):\n",
        "    stage = STAGES[\"preds_val\"] if split == \"val\" else STAGES[\"preds_test\"]\n",
        "    outp = stage / _pred_name(asset_label, dataset_tag, model, window=window, split=split)\n",
        "    df_pred.to_csv(outp); return outp\n",
        "\n",
        "def save_leaderboard(df_leader: pd.DataFrame, tag: str = \"metrics\"):\n",
        "    path = STAGES[\"leaderboard\"] / f\"leaderboard_{tag}.csv\"\n",
        "    df_leader.to_csv(path, index=False); return path\n",
        "\n",
        "def save_figure(fig, asset_label: str, tail: str):\n",
        "    path = STAGES[\"figs\"] / _fig_name(asset_label, tail)\n",
        "    fig.savefig(path, dpi=150, bbox_inches=\"tight\"); plt.close(fig); return path\n",
        "\n",
        "print(\"RUN_ID:\", RUN_ID)\n",
        "for name, path in STAGES.items(): print(f\"{name:>12}: {path}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Models & sequence utilities\n",
        "# -------------------------------\n",
        "def make_purged_splits(n: int, train=0.70, valid=0.15, embargo: int = 5):\n",
        "    i_tr = int(n * train); i_va = int(n * (train + valid))\n",
        "    tr = slice(0, i_tr)\n",
        "    va = slice(min(i_tr + embargo, i_va), i_va)\n",
        "    te = slice(min(i_va + embargo, n), n)\n",
        "    return tr, va, te\n",
        "\n",
        "def make_mlp(input_dim: int):\n",
        "    tf.keras.utils.set_random_seed(SEED)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(input_dim,)),\n",
        "        keras.layers.Dense(128, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(64, activation=\"relu\"),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.30),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[keras.metrics.AUC(name=\"auc\"), keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def make_lstm(input_shape, units=64, dropout=0.2):\n",
        "    tf.keras.utils.set_random_seed(SEED)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=input_shape),\n",
        "        keras.layers.LSTM(units, return_sequences=True),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.LSTM(units//2),\n",
        "        keras.layers.Dropout(dropout),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(1e-3),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[keras.metrics.AUC(name=\"auc\"), keras.metrics.BinaryAccuracy(name=\"acc\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_sequences_from_extended(df_ext: pd.DataFrame, feature_cols: list[str], y_col=\"y_up\", window=30):\n",
        "    X = df_ext[feature_cols].values\n",
        "    y = df_ext[y_col].astype(int).values\n",
        "    idx = df_ext.index\n",
        "    xs, ys, dates = [], [], []\n",
        "    for t in range(window, len(df_ext)):\n",
        "        xs.append(X[t-window:t, :]); ys.append(y[t]); dates.append(idx[t])\n",
        "    return np.asarray(xs), np.asarray(ys), pd.DatetimeIndex(dates)\n",
        "\n",
        "def split_scale_sequences(X_seq, y_seq, train=0.70, valid=0.15):\n",
        "    n = len(X_seq); i_tr = int(n*train); i_va = int(n*(train+valid))\n",
        "    X_tr, X_va, X_te = X_seq[:i_tr], X_seq[i_tr:i_va], X_seq[i_va:]\n",
        "    y_tr, y_va, y_te = y_seq[:i_tr], y_seq[i_tr:i_va], y_seq[i_va:]\n",
        "    if len(X_tr)==0: return (X_tr,y_tr), (X_va,y_va), (X_te,y_te)\n",
        "    T,W,F = X_tr.shape\n",
        "    scaler = StandardScaler().fit(X_tr.reshape(T*W,F))\n",
        "    def _tf(x):\n",
        "        if len(x)==0: return x\n",
        "        TT,WW,FF = x.shape\n",
        "        return scaler.transform(x.reshape(TT*WW,FF)).reshape(TT,WW,FF)\n",
        "    return (_tf(X_tr),y_tr),(_tf(X_va),y_va),(_tf(X_te),y_te)\n",
        "\n",
        "# -------------------------------\n",
        "# Shared feature sanitiser\n",
        "# -------------------------------\n",
        "def sanitize_features(df_ext: pd.DataFrame, price_col: str):\n",
        "    \"\"\"\n",
        "    Make features ML/DL-safe:\n",
        "      - Identify feature columns (exclude price, ret1, y_up)\n",
        "      - Replace ±inf→NaN, then fill NaN with 0.0\n",
        "      - Clip to [-10, 10]\n",
        "      - Drop rows missing price or target\n",
        "    Returns: cleaned frame, feature_cols list.\n",
        "    \"\"\"\n",
        "    exclude = {price_col, \"ret1\", \"y_up\"}\n",
        "    feature_cols = [c for c in df_ext.columns if c not in exclude]\n",
        "    if feature_cols:\n",
        "        df_ext[feature_cols] = (\n",
        "            df_ext[feature_cols]\n",
        "            .replace([np.inf, -np.inf], np.nan)\n",
        "            .fillna(0.0)\n",
        "            .clip(lower=-10.0, upper=10.0)\n",
        "        )\n",
        "    df_ext = df_ext.dropna(subset=[price_col, \"y_up\"]).copy()\n",
        "    return df_ext, feature_cols\n",
        "\n",
        "# -------------------------------\n",
        "# Data source selector\n",
        "# -------------------------------\n",
        "def get_base_source(dataset_version: str) -> pd.DataFrame:\n",
        "    root = dataset_version.split(\"_\",1)[0].lower()\n",
        "    if root == \"raw\": return df_raw\n",
        "    if root == \"eng\": return df_eng\n",
        "    raise ValueError(f\"Unknown dataset_version: {dataset_version}\")\n",
        "\n",
        "# =========================================================\n",
        "# NEW: per-asset engineered feature selection + leak guard\n",
        "# =========================================================\n",
        "def _asset_aliases(asset: dict) -> set[str]:\n",
        "    \"\"\"\n",
        "    Tokens that can appear in column names for THIS asset:\n",
        "      - PAIR_ID (e.g., \"GC=F\", \"BTC-USD\", \"USDCNY=X\")\n",
        "      - root from price_col (e.g., \"GC=F\")\n",
        "      - human label (e.g., \"gold\")\n",
        "    Handles USDCNY<->CNY aliasing.\n",
        "    \"\"\"\n",
        "    pid = str(asset[\"PAIR_ID\"]).lower()\n",
        "    root = str(asset[\"price_col\"]).split()[0].lower()\n",
        "    label = str(asset[\"label\"]).lower()\n",
        "    aliases = {pid, root, label}\n",
        "    if pid == \"usdcny=x\":\n",
        "        aliases |= {\"cny=x\"}\n",
        "    if pid == \"cny=x\":\n",
        "        aliases |= {\"usdcny=x\"}\n",
        "    return aliases\n",
        "\n",
        "def _other_assets_tokens(current: dict) -> set[str]:\n",
        "    toks = set()\n",
        "    for a in ASSETS:\n",
        "        if a is current:\n",
        "            continue\n",
        "        toks |= _asset_aliases(a)\n",
        "    return toks\n",
        "\n",
        "def select_engineered_for_asset(all_cols: list[str], asset: dict) -> list[str]:\n",
        "    \"\"\"\n",
        "    Keep engineered (non-trend) columns for THIS asset only:\n",
        "      - Exclude all price columns\n",
        "      - Exclude all trend columns (handled separately)\n",
        "      - Exclude columns that contain tokens of OTHER assets\n",
        "      - Allow columns that contain CURRENT asset tokens or are 'generic'\n",
        "    \"\"\"\n",
        "    price_cols_all = {a[\"price_col\"] for a in ASSETS}\n",
        "    other_tokens = _other_assets_tokens(asset)\n",
        "\n",
        "    selected = []\n",
        "    for c in all_cols:\n",
        "        lc = str(c).lower()\n",
        "        if c in price_cols_all:\n",
        "            continue\n",
        "        if \"trend\" in lc:\n",
        "            continue\n",
        "        if any(tok in lc for tok in other_tokens):\n",
        "            continue\n",
        "        selected.append(c)\n",
        "    return selected\n",
        "\n",
        "# -------------------------------\n",
        "# DL runner (MLP + LSTM) with safe selector + leak guard + column log\n",
        "# -------------------------------\n",
        "def run_asset_dl(asset: dict, dataset_version: str = \"raw\", use_keywords: bool = True, use_dl_mode: str = \"both\"):\n",
        "    label       = asset[\"label\"]\n",
        "    price_col   = asset[\"price_col\"]\n",
        "    dataset_tag = f\"{dataset_version}_{'ext' if use_keywords else 'base'}\"\n",
        "    label_tag   = f\"{label} [{dataset_tag}]\"\n",
        "\n",
        "    base_source = get_base_source(dataset_version)\n",
        "\n",
        "    # Base target\n",
        "    base = make_target(base_source[[price_col]], price_col).dropna(subset=[price_col, \"y_up\"])\n",
        "\n",
        "    # Engineered (non-trend) features -> filtered per-asset\n",
        "    non_trend_engineered = select_engineered_for_asset(list(base_source.columns), asset)\n",
        "\n",
        "    keywords_used = 0\n",
        "    if use_keywords:\n",
        "        kws = load_keywords_for_pair(KW_CSV, asset[\"PAIR_ID\"])\n",
        "        trend_cols = [c for c in map_keywords_to_trend_cols(base_source.columns, kws) if c in base_source.columns]\n",
        "        lag_df = build_trend_lag_features(base_source, trend_cols, lag_steps=(7,14,21))\n",
        "        ext = (\n",
        "            base\n",
        "            .join(base_source[non_trend_engineered], how=\"left\")\n",
        "            .join(base_source[trend_cols],          how=\"left\")\n",
        "            .join(lag_df,                           how=\"left\")\n",
        "        )\n",
        "        if trend_cols: ext[trend_cols] = ext[trend_cols].fillna(0.0)\n",
        "        lag_cols = [c for c in ext.columns if \"__chg\" in c]\n",
        "        if lag_cols:  ext[lag_cols]  = ext[lag_cols].fillna(0.0)\n",
        "        keywords_used = len(trend_cols)\n",
        "    else:\n",
        "        ext = base.join(base_source[non_trend_engineered], how=\"left\")\n",
        "\n",
        "    # Final sanitisation + feature list\n",
        "    ext, feature_cols = sanitize_features(ext, price_col)\n",
        "\n",
        "    # Leak guard: drop any columns that still contain other-asset tokens\n",
        "    other_tokens = _other_assets_tokens(asset)\n",
        "    leaky = [c for c in feature_cols if any(tok in c.lower() for tok in other_tokens)]\n",
        "    if leaky:\n",
        "        print(\"[WARN] Cross-asset leakage detected, dropping:\", leaky[:10], \"...\" if len(leaky) > 10 else \"\")\n",
        "        keep = [c for c in feature_cols if c not in leaky]\n",
        "        ext = ext[[*keep, \"y_up\", price_col]].copy()\n",
        "        feature_cols = keep\n",
        "\n",
        "    rows_used = len(ext)\n",
        "    results = {}\n",
        "    modes = [use_dl_mode] if use_dl_mode in {\"mlp\",\"lstm\"} else [\"mlp\",\"lstm\"]\n",
        "\n",
        "    for mode in modes:\n",
        "        if mode == \"mlp\":\n",
        "            X = ext[feature_cols].values\n",
        "            y = ext[\"y_up\"].astype(int).values\n",
        "            tr, va, te = make_purged_splits(len(ext), train=0.70, valid=0.15, embargo=5)\n",
        "            X_tr, X_va, X_te = X[tr], X[va], X[te]\n",
        "            y_tr, y_va, y_te = y[tr], y[va], y[te]\n",
        "\n",
        "            scaler = StandardScaler().fit(X_tr)\n",
        "            X_tr, X_va, X_te = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "            MAX_EPOCHS = 3 if DEBUG else 30\n",
        "            BATCH      = 32 if DEBUG else 64\n",
        "            es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=5, restore_best_weights=True)\n",
        "\n",
        "            mlp = make_mlp(X_tr.shape[1])\n",
        "            mlp.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=MAX_EPOCHS, batch_size=BATCH, callbacks=[es], verbose=0)\n",
        "\n",
        "            p_va = mlp.predict(X_va, verbose=0).ravel(); h_va = (p_va >= 0.5).astype(int)\n",
        "            p_te = mlp.predict(X_te, verbose=0).ravel(); h_te = (p_te >= 0.5).astype(int)\n",
        "\n",
        "            m = {\"acc\": accuracy_score(y_te, h_te) if len(y_te) else float(\"nan\"),\n",
        "                 \"f1\":  f1_score(y_te, h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                 \"auc\": roc_auc_score(y_te, p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\")}\n",
        "            results[\"MLP\"] = (None, m)\n",
        "\n",
        "            df_val  = pd.DataFrame({\"date\": ext.index[va], \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\")\n",
        "            df_test = pd.DataFrame({\"date\": ext.index[te], \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\")\n",
        "            save_preds_df(df_val,  label, dataset_tag, \"mlp\", window=None, split=\"val\")\n",
        "            save_preds_df(df_test, label, dataset_tag, \"mlp\", window=None, split=\"test\")\n",
        "\n",
        "        elif mode == \"lstm\":\n",
        "            WINDOW     = 30\n",
        "            MAX_EPOCHS = 3 if DEBUG else 50\n",
        "            BATCH      = 32 if DEBUG else 64\n",
        "\n",
        "            Xseq, yseq, idx = build_sequences_from_extended(ext, feature_cols, \"y_up\", window=WINDOW)\n",
        "            nseq = len(Xseq)\n",
        "            if nseq < 50:\n",
        "                print(f\"[WARN] Not enough samples for {label_tag} (LSTM).\")\n",
        "            else:\n",
        "                tr, va, te = make_purged_splits(nseq, train=0.70, valid=0.15, embargo=5)\n",
        "                (X_tr_raw,y_tr_raw), (X_va_raw,y_va_raw), (X_te_raw,y_te_raw) = split_scale_sequences(Xseq, yseq, train=0.70, valid=0.15)\n",
        "                i_tr_end = int(nseq*0.70); i_va_end = int(nseq*0.85)\n",
        "                X_tr, y_tr = X_tr_raw[tr], y_tr_raw[tr]\n",
        "                X_va, y_va = X_va_raw[va.start - i_tr_end: va.stop - i_tr_end], y_va_raw[va.start - i_tr_end: va.stop - i_tr_end]\n",
        "                X_te, y_te = X_te_raw[te.start - i_va_end:],                y_te_raw[te.start - i_va_end:]\n",
        "\n",
        "                if len(X_va)==0 or len(X_te)==0:\n",
        "                    print(f\"[WARN] Not enough samples after purge for {label_tag} (LSTM).\")\n",
        "                else:\n",
        "                    es = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
        "                    lstm = make_lstm(input_shape=X_tr.shape[1:])\n",
        "                    lstm.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=MAX_EPOCHS, batch_size=BATCH, callbacks=[es], verbose=0)\n",
        "\n",
        "                    p_va = lstm.predict(X_va, verbose=0).ravel(); h_va = (p_va >= 0.5).astype(int)\n",
        "                    p_te = lstm.predict(X_te, verbose=0).ravel(); h_te = (p_te >= 0.5).astype(int)\n",
        "\n",
        "                    m = {\"acc\": accuracy_score(y_te, h_te) if len(y_te) else float(\"nan\"),\n",
        "                         \"f1\":  f1_score(y_te, h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                         \"auc\": roc_auc_score(y_te, p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\")}\n",
        "                    results[\"LSTM\"] = (None, m)\n",
        "\n",
        "                    val_idx  = idx[va]; test_idx = idx[te]\n",
        "                    df_val  = pd.DataFrame({\"date\": val_idx,  \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\")\n",
        "                    df_test = pd.DataFrame({\"date\": test_idx, \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\")\n",
        "                    save_preds_df(df_val,  label, dataset_tag, \"lstm\", window=WINDOW, split=\"val\")\n",
        "                    save_preds_df(df_test, label, dataset_tag, \"lstm\", window=WINDOW, split=\"test\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # LOGGING with columns list\n",
        "    # -------------------------------\n",
        "    if results:\n",
        "        lines = [\n",
        "    f\"{label} [{dataset_tag}] — DL {','.join(results.keys())} (use_keywords={use_keywords})\",\n",
        "    f\"[Run] FREQ={FREQ} | DEBUG={DEBUG} | RUN_ID={RUN_ID}\",\n",
        "    f\"[Data] Rows used: {rows_used} | Features: {len(feature_cols)}\",\n",
        "    f\"[Extended] Keywords used: {keywords_used}\",\n",
        "    \"[Columns]\"\n",
        "] + [f\"    - {c}\" for c in feature_cols]\n",
        "        for name, (_x, met) in results.items():\n",
        "            lines.append(f\"{name}: ACC={met.get('acc', float('nan')):.3f} F1={met.get('f1', float('nan')):.3f} AUC={met.get('auc', float('nan')):.3f}\")\n",
        "        save_txt_log(label, dataset_tag, f\"DL_{','.join(results.keys())}\", lines)\n",
        "\n",
        "    print(f\"✅ DL finished {label_tag} | rows: {rows_used} | features: {len(feature_cols)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# ML runner (LR / RF / XGB) with safe selector + leak guard + column log\n",
        "# -------------------------------\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    _HAS_XGB = True\n",
        "except Exception:\n",
        "    print(\"[WARN] XGBoost not available; skipping XGB.\")\n",
        "    _HAS_XGB = False\n",
        "\n",
        "def run_asset_ml(asset: dict, dataset_version: str = \"raw\", use_keywords: bool = True):\n",
        "    label       = asset[\"label\"]\n",
        "    price_col   = asset[\"price_col\"]\n",
        "    dataset_tag = f\"{dataset_version}_{'ext' if use_keywords else 'base'}\"\n",
        "    label_tag   = f\"{label} [{dataset_tag}]\"\n",
        "\n",
        "    base_source = get_base_source(dataset_version)\n",
        "    base = make_target(base_source[[price_col]], price_col).dropna(subset=[price_col, \"y_up\"])\n",
        "\n",
        "    # Engineered (non-trend) features -> filtered per-asset\n",
        "    non_trend_engineered = select_engineered_for_asset(list(base_source.columns), asset)\n",
        "\n",
        "    keywords_used = 0\n",
        "    if use_keywords:\n",
        "        kws = load_keywords_for_pair(KW_CSV, asset[\"PAIR_ID\"])\n",
        "        trend_cols = [c for c in map_keywords_to_trend_cols(base_source.columns, kws) if c in base_source.columns]\n",
        "        lag_df = build_trend_lag_features(base_source, trend_cols, lag_steps=(7,14,21))\n",
        "        ext = (\n",
        "            base\n",
        "            .join(base_source[non_trend_engineered], how=\"left\")\n",
        "            .join(base_source[trend_cols],          how=\"left\")\n",
        "            .join(lag_df,                           how=\"left\")\n",
        "        )\n",
        "        if trend_cols: ext[trend_cols] = ext[trend_cols].fillna(0.0)\n",
        "        lag_cols = [c for c in ext.columns if \"__chg\" in c]\n",
        "        if lag_cols:  ext[lag_cols]  = ext[lag_cols].fillna(0.0)\n",
        "        keywords_used = len(trend_cols)\n",
        "    else:\n",
        "        ext = base.join(base_source[non_trend_engineered], how=\"left\")\n",
        "\n",
        "    # Final sanitisation + feature list\n",
        "    ext, feature_cols = sanitize_features(ext, price_col)\n",
        "\n",
        "    # Leak guard: drop any columns that still contain other-asset tokens\n",
        "    other_tokens = _other_assets_tokens(asset)\n",
        "    leaky = [c for c in feature_cols if any(tok in c.lower() for tok in other_tokens)]\n",
        "    if leaky:\n",
        "        print(\"[WARN] Cross-asset leakage detected, dropping:\", leaky[:10], \"...\" if len(leaky) > 10 else \"\")\n",
        "        keep = [c for c in feature_cols if c not in leaky]\n",
        "        ext = ext[[*keep, \"y_up\", price_col]].copy()\n",
        "        feature_cols = keep\n",
        "\n",
        "    rows_used = len(ext)\n",
        "\n",
        "    # Split + scale\n",
        "    X = ext[feature_cols].values\n",
        "    y = ext[\"y_up\"].astype(int).values\n",
        "    tr, va, te = make_purged_splits(len(ext), train=0.70, valid=0.15, embargo=5)\n",
        "    X_tr, X_va, X_te = X[tr], X[va], X[te]\n",
        "    y_tr, y_va, y_te = y[tr], y[va], y[te]\n",
        "    scaler = StandardScaler().fit(X_tr)\n",
        "    X_tr, X_va, X_te = scaler.transform(X_tr), scaler.transform(X_va), scaler.transform(X_te)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "    lr.fit(X_tr, y_tr)\n",
        "    p_va = lr.predict_proba(X_va)[:,1]; h_va = (p_va >= 0.5).astype(int)\n",
        "    p_te = lr.predict_proba(X_te)[:,1]; h_te = (p_te >= 0.5).astype(int)\n",
        "    m = {\"acc\": accuracy_score(y_te,h_te) if len(y_te) else float(\"nan\"),\n",
        "         \"f1\":  f1_score(y_te,h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "         \"auc\": roc_auc_score(y_te,p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\")}\n",
        "    results[\"LR\"] = (None, m)\n",
        "    save_preds_df(pd.DataFrame({\"date\": ext.index[va], \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\"),\n",
        "                  label, dataset_tag, \"lr\", window=None, split=\"val\")\n",
        "    save_preds_df(pd.DataFrame({\"date\": ext.index[te], \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\"),\n",
        "                  label, dataset_tag, \"lr\", window=None, split=\"test\")\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100 if not DEBUG else 50,\n",
        "        max_depth=None, n_jobs=-1, random_state=SEED,\n",
        "        class_weight=\"balanced_subsample\"\n",
        "    )\n",
        "    rf.fit(X_tr, y_tr)\n",
        "    p_va = rf.predict_proba(X_va)[:,1]; h_va = (p_va >= 0.5).astype(int)\n",
        "    p_te = rf.predict_proba(X_te)[:,1]; h_te = (p_te >= 0.5).astype(int)\n",
        "    m = {\"acc\": accuracy_score(y_te,h_te) if len(y_te) else float(\"nan\"),\n",
        "         \"f1\":  f1_score(y_te,h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "         \"auc\": roc_auc_score(y_te,p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\")}  # noqa\n",
        "    results[\"RF\"] = (None, m)\n",
        "    save_preds_df(pd.DataFrame({\"date\": ext.index[va], \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\"),\n",
        "                  label, dataset_tag, \"rf\", window=None, split=\"val\")\n",
        "    save_preds_df(pd.DataFrame({\"date\": ext.index[te], \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\"),\n",
        "                  label, dataset_tag, \"rf\", window=None, split=\"test\")\n",
        "\n",
        "    # XGBoost (optional)\n",
        "    try:\n",
        "        if _HAS_XGB:\n",
        "            xgb = XGBClassifier(\n",
        "                n_estimators=400 if not DEBUG else 150,\n",
        "                max_depth=4, learning_rate=0.05,\n",
        "                subsample=0.8, colsample_bytree=0.8,\n",
        "                reg_lambda=1.0, random_state=SEED,\n",
        "                tree_method=\"hist\", eval_metric=\"auc\"\n",
        "            )\n",
        "            xgb.fit(X_tr, y_tr)\n",
        "            p_va = xgb.predict_proba(X_va)[:,1]; h_va = (p_va >= 0.5).astype(int)\n",
        "            p_te = xgb.predict_proba(X_te)[:,1]; h_te = (p_te >= 0.5).astype(int)\n",
        "            m = {\"acc\": accuracy_score(y_te,h_te) if len(y_te) else float(\"nan\"),\n",
        "                 \"f1\":  f1_score(y_te,h_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\"),\n",
        "                 \"auc\": roc_auc_score(y_te,p_te) if len(y_te) and len(np.unique(y_te))>1 else float(\"nan\")}\n",
        "            results[\"XGB\"] = (None, m)\n",
        "            save_preds_df(pd.DataFrame({\"date\": ext.index[va], \"y_true\": y_va, \"y_pred\": h_va, \"prob_up\": p_va}).set_index(\"date\"),\n",
        "                          label, dataset_tag, \"xgb\", window=None, split=\"val\")\n",
        "            save_preds_df(pd.DataFrame({\"date\": ext.index[te], \"y_true\": y_te, \"y_pred\": h_te, \"prob_up\": p_te}).set_index(\"date\"),\n",
        "                          label, dataset_tag, \"xgb\", window=None, split=\"test\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] XGB failed:\", e)\n",
        "\n",
        "    # -------------------------------\n",
        "    # LOGGING with columns list\n",
        "    # -------------------------------\n",
        "    if results:\n",
        "        lines = [\n",
        "    f\"{label} [{dataset_tag}] — ML {','.join(results.keys())} (use_keywords={use_keywords})\",\n",
        "    f\"[Run] FREQ={FREQ} | DEBUG={DEBUG} | RUN_ID={RUN_ID}\",\n",
        "    f\"[Data] Rows used: {rows_used} | Features: {len(feature_cols)}\",\n",
        "    f\"[Extended] Keywords used: {keywords_used}\",\n",
        "    \"[Columns]\"\n",
        "] + [f\"    - {c}\" for c in feature_cols]\n",
        "        for name, (_x, met) in results.items():\n",
        "            lines.append(f\"{name}: ACC={met.get('acc', float('nan')):.3f} F1={met.get('f1', float('nan')):.3f} AUC={met.get('auc', float('nan')):.3f}\")\n",
        "        save_txt_log(label, dataset_tag, f\"ML_{','.join(results.keys())}\", lines)\n",
        "\n",
        "    print(f\"✅ ML finished {label_tag} | rows: {rows_used} | features: {len(feature_cols)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Leaderboard + Backtest\n",
        "# -------------------------------\n",
        "def build_leaderboard_from_logs(run_root: Path = OUT_DIR / \"runs\" / RUN_ID):\n",
        "    pat_hdr   = re.compile(r\"^(?P<label>.+?) \\[(?P<dataset>.+?)\\] — (?P<run>.+)$\")\n",
        "    pat_model = re.compile(r\"^(?P<model>LR|RF|XGB|MLP|LSTM): ACC=(?P<acc>[\\d\\.]+) F1=(?P<f1>[\\d\\.NaN]+) AUC=(?P<auc>[\\d\\.NaN]+)\")\n",
        "    rows = []\n",
        "    for fp in sorted((run_root / \"10_logs\").glob(\"*.txt\")):\n",
        "        label = dataset = run = None\n",
        "        for ln in open(fp, \"r\"):\n",
        "            ln = ln.strip()\n",
        "            m1 = pat_hdr.match(ln)\n",
        "            if m1:\n",
        "                label, dataset, run = m1.group(\"label\"), m1.group(\"dataset\"), m1.group(\"run\"); continue\n",
        "            m2 = pat_model.match(ln)\n",
        "            if m2 and label and run:\n",
        "                rows.append({\n",
        "                    \"file\": fp.name,\n",
        "                    \"asset\": label,\n",
        "                    \"dataset\": dataset,\n",
        "                    \"run_type\": run,\n",
        "                    \"model\": m2.group(\"model\"),\n",
        "                    \"ACC\": float(m2.group(\"acc\")),\n",
        "                    \"F1\":  float(\"nan\") if m2.group(\"f1\")==\"NaN\" else float(m2.group(\"f1\")),\n",
        "                    \"AUC\": float(\"nan\") if m2.group(\"auc\")==\"NaN\" else float(m2.group(\"auc\")),\n",
        "                })\n",
        "    df_leader = (pd.DataFrame(rows)\n",
        "                 .sort_values([\"asset\",\"dataset\",\"AUC\",\"ACC\"], ascending=[True,True,False,False]))\n",
        "    path = save_leaderboard(df_leader, tag=\"metrics\")\n",
        "    print(\"Leaderboard saved:\", path)\n",
        "    return df_leader\n",
        "\n",
        "def _infer_rule_from_index(idx: pd.DatetimeIndex) -> str:\n",
        "    rule = pd.infer_freq(idx)\n",
        "    if rule: return rule\n",
        "    if len(idx) > 3:\n",
        "        gaps = (idx[1:] - idx[:-1]).days\n",
        "        if np.median(gaps) > 3:\n",
        "            return \"W\"\n",
        "    return \"D\"\n",
        "\n",
        "def backtest_pred_file(pred_csv_path: Path, price_csv_path: Path, asset_label: str,\n",
        "                       up_thr=0.55, down_thr=0.45, fee_bps=0.0005) -> pd.DataFrame:\n",
        "    preds = pd.read_csv(pred_csv_path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
        "    if preds.empty: return preds\n",
        "\n",
        "    PRICE_COL_MAP = {a[\"label\"]: a[\"price_col\"] for a in ASSETS}\n",
        "    price_col = PRICE_COL_MAP[asset_label]\n",
        "    px = pd.read_csv(price_csv_path, parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    assert price_col in px.columns, f\"{price_col} not in price file\"\n",
        "    px = px[[price_col]].rename(columns={price_col: \"Close\"})\n",
        "\n",
        "    rule = _infer_rule_from_index(preds.index)\n",
        "    px_aligned = px.asfreq(\"D\") if rule == \"D\" else px.resample(rule).last()\n",
        "    px_aligned[\"ret1\"] = px_aligned[\"Close\"].pct_change()\n",
        "\n",
        "    dfb = preds.join(px_aligned, how=\"inner\").dropna(subset=[\"prob_up\",\"ret1\"])\n",
        "    if dfb.empty or len(dfb) < 3:\n",
        "        return pd.DataFrame(index=pd.DatetimeIndex([], name=\"date\"))\n",
        "\n",
        "    dfb[\"pos\"] = np.where(dfb[\"prob_up\"] >= up_thr, 1, np.where(dfb[\"prob_up\"] <= down_thr, -1, 0))\n",
        "    dfb[\"pos_shift\"] = dfb[\"pos\"].shift(1).fillna(0)\n",
        "    dfb[\"turnover\"] = (dfb[\"pos\"] != dfb[\"pos_shift\"]).astype(int)\n",
        "    dfb[\"pnl\"] = dfb[\"pos_shift\"] * dfb[\"ret1\"] - dfb[\"turnover\"] * fee_bps\n",
        "    dfb[\"cum_pnl\"] = (1 + dfb[\"pnl\"]).cumprod()\n",
        "    return dfb\n",
        "\n",
        "def plot_and_save_equity(df_bt: pd.DataFrame, asset_label: str, model_tag: str):\n",
        "    fig = plt.figure()\n",
        "    df_bt[\"cum_pnl\"].plot()\n",
        "    plt.title(f\"{asset_label} – {model_tag}\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Equity\")\n",
        "    path = save_figure(fig, asset_label, f\"{_slug(model_tag)}_equity\")\n",
        "    print(\"Figure saved:\", path)\n",
        "    return path\n",
        "\n",
        "def backtest_best_from_leaderboard(df_leader: pd.DataFrame, *, only_model=(\"LR\",\"RF\",\"XGB\",\"MLP\",\"LSTM\")):\n",
        "    best = (df_leader[df_leader[\"model\"].isin(only_model)]\n",
        "            .sort_values([\"asset\",\"dataset\",\"AUC\",\"ACC\"], ascending=[True,True,False,False])\n",
        "            .groupby([\"asset\",\"dataset\"]).head(1))\n",
        "\n",
        "    summaries = []\n",
        "    for _, row in best.iterrows():\n",
        "        asset, dataset, model = row[\"asset\"], row[\"dataset\"], row[\"model\"]\n",
        "        guess = _pred_name(asset, dataset, model, window=30 if model==\"LSTM\" else None, split=\"test\")\n",
        "        pred_path = STAGES[\"preds_test\"] / guess\n",
        "        if not pred_path.exists():\n",
        "            print(f\"Skip (no prediction file): {pred_path.name}\")\n",
        "            continue\n",
        "\n",
        "        bt = backtest_pred_file(pred_path, RAW_FILE, asset, up_thr=0.55, down_thr=0.45, fee_bps=0.0005)\n",
        "        if bt.empty or \"cum_pnl\" not in bt or bt[\"cum_pnl\"].empty:\n",
        "            print(f\"Skip (no overlapping data after alignment): {asset} | {pred_path.name}\")\n",
        "            continue\n",
        "\n",
        "        model_tag = f\"{model} ({dataset})\"\n",
        "        (STAGES[\"backtests\"] / f\"{_slug(asset)}_{_slug(model_tag)}_backtest.csv\").parent.mkdir(parents=True, exist_ok=True)\n",
        "        bt.to_csv(STAGES[\"backtests\"] / f\"{_slug(asset)}_{_slug(model_tag)}_backtest.csv\")\n",
        "        plot_and_save_equity(bt, asset, model_tag)\n",
        "\n",
        "        summaries.append({\n",
        "            \"asset\": asset,\n",
        "            \"dataset\": dataset,\n",
        "            \"model\": model,\n",
        "            \"final_equity\": float(bt[\"cum_pnl\"].iloc[-1]),\n",
        "            \"obs\": int(bt[\"cum_pnl\"].shape[0])\n",
        "        })\n",
        "\n",
        "    if summaries:\n",
        "        df_sum = pd.DataFrame(summaries).sort_values([\"asset\",\"final_equity\"], ascending=[True,False])\n",
        "        path = STAGES[\"leaderboard\"] / \"backtest_summary.csv\"\n",
        "        df_sum.to_csv(path, index=False)\n",
        "        print(\"Backtest summary saved:\", path)\n",
        "        print(df_sum)\n",
        "    else:\n",
        "        print(\"No backtest summaries produced (nothing overlapped or files missing).\")\n",
        "\n",
        "# =========================================================\n",
        "# RUNS — choose mode via RUN_MODE (\"all\" or \"single\")\n",
        "# =========================================================\n",
        "print(\"=== RUN CONFIG ===\")\n",
        "print(\"FREQ:\", FREQ, \"| DEBUG:\", DEBUG, \"| RUN_ID:\", RUN_ID)\n",
        "print(\"OUTPUT ROOT:\", RUN_ROOT)\n",
        "print(\"RUN_MODE:\", RUN_MODE)\n",
        "\n",
        "if RUN_MODE == \"all\":\n",
        "    for dataset_version in [\"raw\", \"eng\"]:\n",
        "        for use_kw in [False, True]:  # base first, then extended\n",
        "            tag = f\"{dataset_version}_{'base' if not use_kw else 'ext'}\"\n",
        "            print(f\"\\n--- {tag.upper()} — ML + DL ---\")\n",
        "            for _, asset in asset_by_label.items():\n",
        "                # Classical ML baselines\n",
        "                run_asset_ml(asset, dataset_version=dataset_version, use_keywords=use_kw)\n",
        "                # Deep learning (tabular MLP + sequence LSTM)\n",
        "                run_asset_dl(asset, dataset_version=dataset_version, use_keywords=use_kw, use_dl_mode=\"both\")\n",
        "\n",
        "    # Build leaderboard from all logs saved in this run\n",
        "    df_leader = build_leaderboard_from_logs(RUN_ROOT)\n",
        "    # Backtest the best model per asset × dataset flavour (includes ML + DL)\n",
        "    backtest_best_from_leaderboard(df_leader, only_model=(\"LR\",\"RF\",\"XGB\",\"MLP\",\"LSTM\"))\n",
        "\n",
        "elif RUN_MODE == \"single\":\n",
        "    asset = asset_by_label[PAIR_KEY]\n",
        "    print(\"ASSET:\", asset[\"label\"])\n",
        "    for dataset_version in [\"raw\", \"eng\"]:\n",
        "        for use_kw in [False, True]:  # base first, then extended\n",
        "            tag = f\"{dataset_version}_{'base' if not use_kw else 'ext'}\"\n",
        "            print(f\"\\n--- {asset['label']} [{tag.upper()}] — ML + DL ---\")\n",
        "            # Classical ML\n",
        "            run_asset_ml(asset, dataset_version=dataset_version, use_keywords=use_kw)\n",
        "            # Deep Learning\n",
        "            run_asset_dl(asset, dataset_version=dataset_version, use_keywords=use_kw, use_dl_mode=\"both\")\n",
        "\n",
        "    df_leader = build_leaderboard_from_logs(RUN_ROOT)\n",
        "    backtest_best_from_leaderboard(df_leader, only_model=(\"LR\",\"RF\",\"XGB\",\"MLP\",\"LSTM\"))\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"RUN_MODE must be 'all' or 'single'.\")\n",
        "\n",
        "print(\"\\n=== ARTIFACTS ===\")\n",
        "for name, path in STAGES.items():\n",
        "    num = len(list(path.glob('*')))\n",
        "    print(f\"{name:>12}: {path}  ({num} files)\")"
      ]
    }
  ]
}